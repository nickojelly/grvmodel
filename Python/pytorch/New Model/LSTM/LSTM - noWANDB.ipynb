{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch\n",
    "with torch.profiler.profile() as profiler:\n",
    "        pass\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import wandb\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "from operator import itemgetter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogInput:\n",
    "    def __init__(self, dogid, raceid,stats, dog,dog_box, lstmCellh,lstmCellc) -> None:\n",
    "        self.dogid= dogid\n",
    "        self.raceid = raceid\n",
    "        self.stats = stats.to('cuda:0')\n",
    "        self.dog = dog\n",
    "        self.lstmCellh = lstmCellh.float().to('cuda:0')\n",
    "        self.lstmCellc = lstmCellc.float().to('cuda:0')\n",
    "        self.visited = 0\n",
    "        \n",
    "        \n",
    "    def lstm_i(self, lstmInput):\n",
    "        (self.lstmCellh,self.lstmCellc) = lstmInput\n",
    "        # self.lstmCellh=self.lstmCellh.to(device)\n",
    "        # self.lstmCellc=self.lstmCellc.to(device)\n",
    "        self.visited = self.visited + 1\n",
    "        # if self.visited>1:\n",
    "        #     print(\"FOUND LEAK\")\n",
    "        #     sasdfasd\n",
    "\n",
    "    def nextrace(self, raceid):\n",
    "        self.nextrace_id = raceid\n",
    "\n",
    "    def prevrace(self, raceid):\n",
    "        self.prevrace_id = raceid\n",
    "\n",
    "    def lstm_o(self, lstm_o):\n",
    "        # print(lstm_o[0]._version)\n",
    "        (lh,lc) = lstm_o\n",
    "        if self.nextrace_id==-1:\n",
    "            pass\n",
    "        else:\n",
    "            # self.dog.races[self.nextrace_id].lstm_i((lh.detach(), lc.detach()))  #DETACH\n",
    "            self.dog.races[self.nextrace_id].lstm_i((lh.detach(),lc.detach()))\n",
    "\n",
    "    def detach_state(self):\n",
    "        self.lstmCellh = self.lstmCellh.detach()\n",
    "        self.lstmCellc = self.lstmCellc.detach()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog:\n",
    "    def __init__(self, dogid, hidden_size, layers) -> None:\n",
    "        self.dogid = dogid\n",
    "        # self.raceids = raceids #possible dictionary of race id keys dog stat outs\n",
    "        self.lstmcell = 0\n",
    "        self.layers = layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.l_debug = None\n",
    "        self.races = {}\n",
    "\n",
    "    def add_races(self, raceid, racedate, stats,nextraceid, prevraceid, box):\n",
    "        self.races[raceid] = DogInput(self.dogid, raceid, stats, self, box, torch.randn(self.hidden_size),torch.randn(self.hidden_size)) #this is the change\n",
    "        self.races[raceid].nextrace(nextraceid)\n",
    "        self.races[raceid].prevrace(prevraceid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Race:\n",
    "    def __init__(self, raceid,trackOHE, dist, classes):\n",
    "        self.raceid = raceid\n",
    "        self.race_dist = dist.to('cuda:0')\n",
    "        self.race_track = trackOHE.to('cuda:0')\n",
    "        self.classes =  classes.to('cuda:0')\n",
    "\n",
    "    def add_dogs(self, dogs_list:DogInput):\n",
    "        self.dog1 = dogs_list[0]\n",
    "        self.dog2 = dogs_list[1]\n",
    "        self.dog3 = dogs_list[2]\n",
    "        self.dog4 = dogs_list[3]\n",
    "        self.dog5 = dogs_list[4]\n",
    "        self.dog6 = dogs_list[5]\n",
    "        self.dog7 = dogs_list[6]\n",
    "        self.dog8 = dogs_list[7]\n",
    "        self.dogs = dogs_list\n",
    "\n",
    "    def nn_input(self):\n",
    "        input = torch.cat([x.stats for x in self.dogs], dim = 0)\n",
    "        full_input = torch.cat((self.race_dist,self.race_track, input), dim=0).to(device='cuda:0')\n",
    "        self.full_input = full_input\n",
    "        return full_input\n",
    "\n",
    "    def lstm_input(self):\n",
    "        l_input = [(x.lstmCellh,x.lstmCellc) for x in self.dogs]\n",
    "        return l_input\n",
    "\n",
    "    def lstm_detach(self):\n",
    "        [x.detach_state for x in self.dogs]\n",
    "\n",
    "    def list_dogs(self):\n",
    "        dogs_l = [x for x in self.dogs]\n",
    "        return dogs_l\n",
    "\n",
    "    def pass_lstm_output(self, lstm_h, lstm_c):\n",
    "        for i,dog in enumerate(self.dogs):\n",
    "            \n",
    "            lh = lstm_h[i]\n",
    "            lc = lstm_c[i]\n",
    "            # lh,lc = lh.detach(), lc.detach()\n",
    "            lh,lc = lh.detach(), lc.detach()\n",
    "            dog.lstm_o((lh,lc))\n",
    "            dog.dog.l_debug = (lh,lc)\n",
    "        # zipped_lstm = zip(self.dogs,lstms)\n",
    "        # [x.lstm_o(y) for x,y in zipped_lstm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Races:\n",
    "    def __init__(self, hidden_size, layers, batch_size = 100) -> None:\n",
    "        self.racesDict = {}\n",
    "        self.dogsDict = {}\n",
    "        self.raceIDs = []\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = layers\n",
    "        self.getter = operator.itemgetter(*range(batch_size))\n",
    "\n",
    "    def add_race(self,raceid:str, trackOHE, dist, classes):\n",
    "        self.racesDict[raceid] = Race(raceid, trackOHE, dist, classes)\n",
    "        self.raceIDs.append(raceid)\n",
    "\n",
    "    def add_dog(self,dogid):\n",
    "        if dogid not in self.dogsDict.keys():\n",
    "            self.dogsDict[dogid] = Dog(dogid, self.hidden_size, self.layers)\n",
    "        else:\n",
    "            self.dogsDict[dogid] = self.dogsDict[dogid]\n",
    "\n",
    "    def get_race_input(self, idx) -> Race:\n",
    "        raceidx = operator.itemgetter(*idx)\n",
    "        #raceidx  = self.getter(idx)\n",
    "        race_batch_id = raceidx(self.raceIDs)\n",
    "\n",
    "        #race_getter = operator.itemgetter(*raceidx)\n",
    "\n",
    "        races = [self.racesDict[x] for x in race_batch_id] #Returns list of class Race\n",
    "        \n",
    "\n",
    "        # raceidx = self.raceIDs[idx]\n",
    "        #input = torch.cat([x.stats for x in races.dogs.values()], dim = 0)\n",
    "        #full_input = torch.cat((self.racesDict[raceidx].race_dist,self.racesDict[raceidx].race_track, input), dim=0 )\n",
    "        # dogs = [x for x in self.racesDict[raceidx].dogs]\n",
    "        \n",
    "        return races #self.racesDict[raceidx]\n",
    "\n",
    "    def get_race_classes(self, idx):\n",
    "        raceidx = self.raceIDs[idx]\n",
    "        classes = [x for x in self.raceDict[raceidx].classes]\n",
    "        return classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.lstm2 = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.lstm3 = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.lstm4 = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.lstm5 = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.lstm6 = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.lstm7 = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.lstm8 = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size * 8, 64)\n",
    "        self.fc3 = nn.Linear(64, 8)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    # x represents our data\n",
    "    def forward(self, race: Race):\n",
    "        #x = race.nn_input().float().to('cuda:0')\n",
    "        x = torch.stack([r.full_input.float() for r in race])\n",
    "\n",
    "        #creates list of LSTM data \n",
    "        lstm_ins = [list(i) for i in zip(*[r.lstm_input() for r in race])]\n",
    "\n",
    "        # creates list of tensors for lstm Cells\n",
    "        hCell = [torch.stack([x[0] for x in y]) for y in lstm_ins]\n",
    "        cCell = [torch.stack([x[1] for x in y]) for y in lstm_ins]\n",
    "\n",
    "        (h1, c1) = self.lstm1(x, (hCell[0], cCell[0]))\n",
    "        (h2, c2) = self.lstm2(x, (hCell[1], cCell[1]))\n",
    "        (h3, c3) = self.lstm3(x, (hCell[2], cCell[2]))\n",
    "        (h4, c4) = self.lstm4(x, (hCell[3], cCell[3]))\n",
    "        (h5, c5) = self.lstm5(x, (hCell[4], cCell[4]))\n",
    "        (h6, c6) = self.lstm6(x, (hCell[5], cCell[5]))\n",
    "        (h7, c7) = self.lstm7(x, (hCell[6], cCell[6]))\n",
    "        (h8, c8) = self.lstm8(x, (hCell[7], cCell[7]))\n",
    "\n",
    "        lstm_list = [\n",
    "            (h1, c1),\n",
    "            (h2, c2),\n",
    "            (h3, c3),\n",
    "            (h4, c4),\n",
    "            (h5, c5),\n",
    "            (h6, c6),\n",
    "            (h7, c7),\n",
    "            (h8, c8)\n",
    "        ]\n",
    "\n",
    "        hCello = [i for i in zip(*[x[0] for x in lstm_list])]\n",
    "        cCello = [i for i in zip(*[x[1] for x in lstm_list])]\n",
    "\n",
    "        for i,r in enumerate(race):\n",
    "            r.pass_lstm_output(hCello[i],cCello[i])\n",
    "            #r.lstm_detach()\n",
    "        xhh = torch.cat((h1,h2, h3, h4, h5, h6, h7, h8), dim=1)\n",
    "        xh = self.fc2(xhh)\n",
    "        xf = self.fc3(xh)\n",
    "\n",
    "        output = F.softmax(xf, dim=0)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data, hidden_size):\n",
    "\n",
    "    #Load in pickeled dataframe\n",
    "    resultsdf = pickle.load(data)\n",
    "    dog_stats_df = pd.DataFrame(resultsdf)\n",
    "    dog_stats_df = dog_stats_df.fillna(-1).drop_duplicates(subset=['dogid', 'raceid'])\n",
    "    dog_stats_df['stats_cuda'] = dog_stats_df.apply(lambda x: torch.tensor(x['stats']), axis =1)\n",
    "    dog_stats_df['box'] = dog_stats_df['stats'].apply(lambda x: x[0])\n",
    "\n",
    "    #Created RaceDB\n",
    "    raceDB = Races(hidden_size, 1)\n",
    "\n",
    "    #Fill in dog portion:\n",
    "\n",
    "    dog_stats_group = dog_stats_df.sort_values(['date']).groupby([\"dogid\"])\n",
    "\n",
    "    for i,j in tqdm(dog_stats_group):\n",
    "        j[\"next_race\"] = j[\"raceid\"].shift(-1).fillna(-1)\n",
    "        j[\"prev_race\"] = j[\"raceid\"].shift(1).fillna(-1)\n",
    "        raceDB.add_dog(i)\n",
    "        j.apply(lambda x: raceDB.dogsDict[i].add_races(x['raceid'], x['date'], torch.Tensor(x['stats']),x['next_race'], x['prev_race'], x['box']), axis=1)\n",
    "\n",
    "    #Fill in races portion\n",
    "    softmin = nn.Softmin()\n",
    "    races_group = dog_stats_df.groupby(['raceid'])\n",
    "\n",
    "    null_dog = Dog(\"nullDog\", raceDB.hidden_size, raceDB.layers)\n",
    "    null_dog_i = DogInput(\"nullDog\", \"-1\", torch.zeros(16), null_dog,0, torch.zeros(raceDB.hidden_size), torch.zeros(raceDB.hidden_size))\n",
    "    null_dog_i.nextrace(-1)\n",
    "    null_dog_i.prevrace(-1)\n",
    "\n",
    "    null_dog_list = [null_dog] * 8\n",
    "    #TO FIX LATER PROPER BOX PLACEMENT #FIXED\n",
    "\n",
    "    races_group = dog_stats_df.groupby(['raceid'])\n",
    "    for i,j in tqdm(races_group):\n",
    "    #Track info tensors\n",
    "        dist = torch.tensor([j.dist.iloc[0]]) \n",
    "        trackOHE = torch.tensor(j.trackOHE.iloc[0])\n",
    "        #margins\n",
    "        empty_dog_list = [null_dog_i]*8\n",
    "        empty_margin_list = [100]*8\n",
    "        boxes_list = [x for x in j['box']]\n",
    "        margin_list = [x for x in j[\"place\"]]\n",
    "        dog_list = [raceDB.dogsDict[x].races[i] for x in j[\"dogid\"]]\n",
    "\n",
    "        #adjustedMargin = [margin_list[x-1] for x in boxes_list]\n",
    "        for n,x in enumerate(boxes_list):\n",
    "            empty_margin_list[x-1] = margin_list[n]\n",
    "            empty_dog_list[x-1] = dog_list[n]\n",
    "        adjustedMargin = softmin(torch.tensor(empty_margin_list)).to('cuda:0')\n",
    "        #adjusted_dog_list = [dog_list[x-1] for x in boxes_list]\n",
    "        \n",
    "        raceDB.add_race(i,trackOHE,dist, adjustedMargin)\n",
    "        \n",
    "        \n",
    "        # List of Dog Input??\n",
    "        raceDB.racesDict[i].add_dogs(empty_dog_list)\n",
    "        raceDB.racesDict[i].nn_input()\n",
    "\n",
    "\n",
    "    return raceDB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "def validate_model(model,raceDB,criterion, batch_size, example_ct):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    list_t = [] \n",
    "    last = 0\n",
    "    loss_val = 0 \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in trange(60000,70000,batch_size):   \n",
    "            races_idx = range(last,last+batch_size)\n",
    "            last = i\n",
    "            race = raceDB.get_race_input(races_idx)\n",
    "            X = race\n",
    "            y = torch.stack([x.classes for x in race])\n",
    "            output = model(X)\n",
    "            #print(y)\n",
    "\n",
    "            _, actual = torch.max(y.data, 1)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            #print(predicted)\n",
    "            #print(actual)\n",
    "            correct += (predicted == actual).sum().item()\n",
    "            total +=10\n",
    "\n",
    "\n",
    "\n",
    "            loss = criterion(output, y)\n",
    "            #optimizer.zero_grad()\n",
    "            #newnet.zero_grad()\n",
    "            #loss.backward(retain_graph=True)  \n",
    "            #optimizer.step()\n",
    "            #if i %5000 == 0:\n",
    "            #    print(loss)\n",
    "        # optimizer.step() \n",
    "        #print(loss)\n",
    "    print(f\"accuray: {correct/total}\")\n",
    "    #wandb.log({\"accuracy\": correct/total, \"loss\": loss}, step=example_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(loss, example_ct, epoch):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, raceDB, criterion, optimizer, config=None):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    last = 0\n",
    "    batch_size = 1000\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "    excep_ct = 0\n",
    "    for epoch in trange(100): \n",
    "        for i in trange(batch_size,60000,batch_size):\n",
    "            batch_ct += 1   \n",
    "            races_idx = range(last,last+batch_size)\n",
    "            last = i\n",
    "            race = raceDB.get_race_input(races_idx)\n",
    "            X = race\n",
    "\n",
    "            y = torch.stack([x.classes for x in race])\n",
    "            output = model(X)\n",
    "            example_ct +=  batch_size\n",
    "            batch_ct += 1\n",
    "            # print(\"new batch \\n\\n\")\n",
    "            # for r in race:\n",
    "            #     print(f\"{r.raceid=}\")\n",
    "            #     for d in r.dogs:\n",
    "            #         print(f\"{d.dogid=}\\nlstmc = {d.lstmCellc}\\nlstmh = {d.lstmCellh}\")\n",
    "\n",
    "            loss = criterion(output, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            # try:\n",
    "            #     loss.backward(retain_graph=True)  \n",
    "            # except Exception as e:\n",
    "            #     print(\"broke\")\n",
    "\n",
    "                        \n",
    "            #     excep_ct +=1\n",
    "            #     break\n",
    "            # #loss.backward()\n",
    "            optimizer.step()\n",
    "            if ((batch_ct + 1) % 25) == 0:\n",
    "                pass\n",
    "                #train_log(loss, example_ct, epoch)\n",
    "\n",
    "            [r.lstm_detach for r in race]\n",
    "\n",
    "        print(loss)\n",
    "        validate_model(model,raceDB,criterion, 8, example_ct)\n",
    "    #print(excep_ct)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(my_dataset,config=None,prev_model=None):\n",
    "    dataset = my_dataset\n",
    "    # tell wandb to get started\n",
    "    #config = wandb.config\n",
    "    #pprint.pprint(config)\n",
    "    #pprint.pprint(config.epochs)\n",
    "    print(config)\n",
    "\n",
    "    model = Net(144,config[\"hidden_size\"])\n",
    "    # criterion = nn.HuberLoss()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    #criterion = nn.NLLLoss()\n",
    "    #optimizer = optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    # make the model, data, and optimization problem\n",
    "    #model, train_loader, test_loader, criterion, optimizer = make(config, dataset)\n",
    "\n",
    "\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model = model.to(device)\n",
    "    #optimizer = optimizer.to(device)\n",
    "    print(model)\n",
    "\n",
    "    # and use them to train the model\n",
    "    train(model, dataset, criterion, optimizer, config)\n",
    "\n",
    "    # and test its final performance\n",
    "    #test(model, test_loader)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25187/25187 [03:24<00:00, 122.96it/s]\n",
      "  0%|          | 0/72073 [00:00<?, ?it/s]C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_22392\\1524283141.py:51: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
      "  adjustedMargin = softmin(torch.tensor(empty_margin_list)).to('cuda:0')\n",
      "100%|██████████| 72073/72073 [00:48<00:00, 1491.65it/s]\n"
     ]
    }
   ],
   "source": [
    "os.getcwd()\n",
    "os.chdir(r\"C:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\DATA\")\n",
    "dog_stats_file = open( 'dog_stats_df.npy', 'rb')\n",
    "hidden_size = 3\n",
    "raceDB = build_dataset(dog_stats_file, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_config_static = {'hidden_size':hidden_size,'batch_size': 360, 'dropout': 0.3, 'epochs': 100, 'f1_layer_size': 256, 'f2_layer_size': 64 , 'learning_rate': 0.00001, 'loss': 'L1', 'l1_beta':0.1,  'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3,4,5])\n",
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_size': 3, 'batch_size': 360, 'dropout': 0.3, 'epochs': 100, 'f1_layer_size': 256, 'f2_layer_size': 64, 'learning_rate': 1e-05, 'loss': 'L1', 'l1_beta': 0.1, 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "Net(\n",
      "  (lstm1): LSTMCell(144, 3)\n",
      "  (lstm2): LSTMCell(144, 3)\n",
      "  (lstm3): LSTMCell(144, 3)\n",
      "  (lstm4): LSTMCell(144, 3)\n",
      "  (lstm5): LSTMCell(144, 3)\n",
      "  (lstm6): LSTMCell(144, 3)\n",
      "  (lstm7): LSTMCell(144, 3)\n",
      "  (lstm8): LSTMCell(144, 3)\n",
      "  (fc2): Linear(in_features=24, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=8, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [05:18<00:00,  5.39s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6935, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:04<00:00, 287.70it/s]\n",
      "  1%|          | 1/100 [05:22<8:52:05, 322.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuray: 0.0952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 14/59 [01:20<04:18,  5.75s/it]\n",
      "  1%|          | 1/100 [06:43<11:05:00, 403.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\LSTM\\LSTM - noWANDB.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m model_pipeline(raceDB,config\u001b[39m=\u001b[39;49mwandb_config_static)\n",
      "\u001b[1;32mc:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\LSTM\\LSTM - noWANDB.ipynb Cell 18\u001b[0m in \u001b[0;36mmodel_pipeline\u001b[1;34m(my_dataset, config, prev_model)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# and use them to train the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m train(model, dataset, criterion, optimizer, config)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# and test its final performance\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m#test(model, test_loader)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "\u001b[1;32mc:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\LSTM\\LSTM - noWANDB.ipynb Cell 18\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, raceDB, criterion, optimizer, config)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m X \u001b[39m=\u001b[39m race\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([x\u001b[39m.\u001b[39mclasses \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m race])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m output \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m example_ct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m  batch_size\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m batch_ct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\LSTM\\LSTM - noWANDB.ipynb Cell 18\u001b[0m in \u001b[0;36mNet.forward\u001b[1;34m(self, race)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m cCello \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[x[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m lstm_list])]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mfor\u001b[39;00m i,r \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(race):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     r\u001b[39m.\u001b[39;49mpass_lstm_output(hCello[i],cCello[i])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     r\u001b[39m.\u001b[39mlstm_detach()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m xhh \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((h1,h2, h3, h4, h5, h6, h7, h8), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\LSTM\\LSTM - noWANDB.ipynb Cell 18\u001b[0m in \u001b[0;36mRace.pass_lstm_output\u001b[1;34m(self, lstm_h, lstm_c)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m lc \u001b[39m=\u001b[39m lstm_c[i]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# lh,lc = lh.detach(), lc.detach()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m lh,lc \u001b[39m=\u001b[39m lh\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mclone(), lc\u001b[39m.\u001b[39;49mclone()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m dog\u001b[39m.\u001b[39mlstm_o((lh,lc))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/LSTM%20-%20noWANDB.ipynb#X23sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m dog\u001b[39m.\u001b[39mdog\u001b[39m.\u001b[39ml_debug \u001b[39m=\u001b[39m (lh,lc)\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\pytorch\\lib\\traceback.py:197\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     f \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39m_getframe()\u001b[39m.\u001b[39mf_back\n\u001b[1;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m format_list(extract_stack(f, limit\u001b[39m=\u001b[39;49mlimit))\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\pytorch\\lib\\traceback.py:211\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     f \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39m_getframe()\u001b[39m.\u001b[39mf_back\n\u001b[1;32m--> 211\u001b[0m stack \u001b[39m=\u001b[39m StackSummary\u001b[39m.\u001b[39;49mextract(walk_stack(f), limit\u001b[39m=\u001b[39;49mlimit)\n\u001b[0;32m    212\u001b[0m stack\u001b[39m.\u001b[39mreverse()\n\u001b[0;32m    213\u001b[0m \u001b[39mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\pytorch\\lib\\traceback.py:362\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    359\u001b[0m     result\u001b[39m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    360\u001b[0m         filename, lineno, name, lookup_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39mlocals\u001b[39m\u001b[39m=\u001b[39mf_locals))\n\u001b[0;32m    361\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m fnames:\n\u001b[1;32m--> 362\u001b[0m     linecache\u001b[39m.\u001b[39;49mcheckcache(filename)\n\u001b[0;32m    363\u001b[0m \u001b[39m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[39mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\compilerop.py:193\u001b[0m, in \u001b[0;36mcheck_linecache_ipython\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m\"\"\"Call linecache.checkcache() safely protecting our cached values.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[39m# First call the original checkcache as intended\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m linecache\u001b[39m.\u001b[39;49m_checkcache_ori(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    194\u001b[0m \u001b[39m# Then, update back the cache with our data, so that tracebacks related\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# to our compiled codes can be produced.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m linecache\u001b[39m.\u001b[39mcache\u001b[39m.\u001b[39mupdate(linecache\u001b[39m.\u001b[39m_ipython_cache)\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\pytorch\\lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[39mcontinue\u001b[39;00m   \u001b[39m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(fullname)\n\u001b[0;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[39m.\u001b[39mpop(filename, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = model_pipeline(raceDB,config=wandb_config_static)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43115443075634c02a7c247a87b0dd9d74842892e56d473b9e19f544f3149aff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
