{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .jp-OutputArea-child:has(.jp-OutputArea-prompt:empty) {\n",
       "              padding: 0 !important;\n",
       "        }\n",
       "    </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "# See https://github.com/bstriner/keras-tqdm/issues/21#issuecomment-443019223\n",
    "display(HTML(\"\"\"\n",
    "    <style>\n",
    "        .jp-OutputArea-child:has(.jp-OutputArea-prompt:empty) {\n",
    "              padding: 0 !important;\n",
    "        }\n",
    "    </style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import wandb\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "from operator import itemgetter\n",
    "import operator\n",
    "from random import randint\n",
    "from rnn_classes import Dog, DogInput, Race, Races, GRUNet\n",
    "from raceDB import build_dataset, build_pred_dataset\n",
    "import importlib\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_saver(model, optimizer, epoch, loss, hidden_state_dict, model_name = None):\n",
    "    \n",
    "    pathtofolder = \"C:/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New Model\"\n",
    "    # model_name = wandb.run.name\n",
    "    if not model_name:\n",
    "        model_name = \"test NZ GRU saver\"\n",
    "    isExist = os.path.exists(\n",
    "        f\"C:/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New Model/savedmodel/{model_name}/\"\n",
    "    )\n",
    "    if isExist:\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optim\": optimizer.state_dict(),\n",
    "                \"loss\": loss,\n",
    "                \"db\":hidden_state_dict,\n",
    "            },\n",
    "            f\"C:/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New Model/savedmodel/{model_name}/{model_name}_{epoch}.pt\",\n",
    "        )\n",
    "    else:\n",
    "        print(\"created path\")\n",
    "        os.makedirs(\n",
    "            f\"C:/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New Model/savedmodel/{model_name}/\"\n",
    "        )\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optim\": optimizer.state_dict(),\n",
    "                \"loss\": loss,\n",
    "                \"db\":hidden_state_dict,\n",
    "            },\n",
    "            f\"C:/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New Model/savedmodel/{model_name}/{model_name}_{epoch}.pt\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_CLE(x,y):\n",
    "    loss_t = -torch.log(torch.exp(x)/torch.sum(torch.exp(x), dim=-1, keepdim=True))*y\n",
    "    return loss_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "def validate_model(model:GRUNet,raceDB,criterion, batch_size, example_ct, epoch_loss, batch_ct):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    sft_max = nn.Softmax(dim=-1)\n",
    "    batch_size=10\n",
    "    len_test = len(raceDB.test_race_ids)-batch_size\n",
    "    list_t = [] \n",
    "    last = 0\n",
    "    loss_val = 0 \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    preds = []\n",
    "    grades = []\n",
    "    tracks = []\n",
    "    pred_confs = []\n",
    "    bfsps = []\n",
    "    start_prices = []\n",
    "    loss_l = []\n",
    "    loss_t = []\n",
    "    margins_l = []\n",
    "    preds_l = []\n",
    "    pred_sftmax = []\n",
    "    raw_margins = []\n",
    "    raw_places = []\n",
    "    margins_prob = []\n",
    "\n",
    "    prices_list = []\n",
    "    raw_prices = []\n",
    "\n",
    "    price_dict = {}\n",
    "    price_dict['prices'] = []\n",
    "    price_dict['imp_prob'] = []\n",
    "    price_dict['pred_prob'] = []\n",
    "    price_dict['pred_price'] = []\n",
    "    price_dict['margin'] = []\n",
    "    price_dict['onehot_win'] = []\n",
    "    price_dict['raceID'] = []\n",
    "    price_dict['dogID'] = []\n",
    "    race_ids = []\n",
    "    with torch.no_grad():\n",
    "        for i in trange(0,len_test,batch_size, leave=False):\n",
    "            races_idx = range(last,last+batch_size)\n",
    "            last = i+batch_size\n",
    "            race = raceDB.get_test_input(races_idx)\n",
    "            #tracks.extend([r.track for r in race])\n",
    "\n",
    "            X = race\n",
    "            y = torch.stack([x.classes for x in race])\n",
    "            output = model(X)\n",
    "            race_ids.extend([x.raceid for x in race])\n",
    "            #print(y)\n",
    "\n",
    "            _, actual = torch.max(y.data, 1)\n",
    "            onehot_win = F.one_hot(actual, num_classes=8)\n",
    "            conf, predicted = torch.max(output.data, 1)\n",
    "            correct += (predicted == actual).sum().item()\n",
    "\n",
    "            softmax_preds = sft_max(output)\n",
    "\n",
    "            \n",
    "            total += batch_size\n",
    "            actuals.extend(actual.tolist())\n",
    "            preds.extend(predicted.tolist())\n",
    "            pred_confs.extend(conf.tolist())\n",
    "            tracks.extend([r.track_name for r in race])\n",
    "            grades.extend([r.grade for r in race])\n",
    "            for i,dog_idx in enumerate(actual.tolist()):\n",
    "                bfsps.append(race[i].dogs[dog_idx].bfsp)\n",
    "                #start_prices.append(race[i].dogs[dog_idx].sp)\n",
    "\n",
    "            \n",
    "            loss = criterion(output, y).detach()\n",
    "            loss_tensor = validation_CLE(output,y)\n",
    "            loss_t.append(loss_tensor.tolist())\n",
    "\n",
    "            loss_l.append(loss.tolist())\n",
    "            preds_l.append(output.tolist())\n",
    "            pred_sftmax.append(softmax_preds.tolist())\n",
    "            margins_l.append(y.tolist())\n",
    "            margins_prob.append(y.tolist())\n",
    "            raw_margins.append([x.raw_margins for x in race])\n",
    "            raw_places.append([x.raw_places for x in race])\n",
    "            loss_val += loss\n",
    "\n",
    "            price_dict['prices'].extend([x.prices for x in race])\n",
    "            price_dict['imp_prob'].extend([x.implied_prob for x in race])\n",
    "            price_dict['pred_prob'].extend(softmax_preds.tolist())\n",
    "            #print([(1/(x+(-7**10))).tolist() for x in torch.exp(output)])\n",
    "            price_dict['pred_price'].extend([(1/(x)).tolist() for x in softmax_preds])\n",
    "            price_dict['margin'].extend([x.raw_margins for x in race])\n",
    "            price_dict['onehot_win'].extend(onehot_win.tolist())\n",
    "            price_dict['raceID'].extend([[x.raceid]*8 for x in race])\n",
    "            price_dict['dogID'].extend([x.list_dog_ids() for x in race])\n",
    "\n",
    "            \n",
    "\n",
    "        loss_list = []\n",
    "\n",
    "        #print(\"start loss calc\")\n",
    "        for i,l in enumerate(loss_l):\n",
    "            for j in range(0,7):\n",
    "                loss_list.append([preds_l[i][j],margins_l[i][j],loss_t[i][j],l[j],pred_sftmax[i][j],margins_prob[i][j], raw_margins[i][j], raw_places[i][j]])\n",
    "\n",
    "    loss_df = pd.DataFrame(loss_list, columns=['Preds','Margins','Indiviual Losses','Losses','softmaxPreds','Softmax Margins','Raw margins', 'Raw places'])\n",
    "    loss_table = wandb.Table(dataframe=loss_df)\n",
    "\n",
    "\n",
    "    logdf = pd.DataFrame(data = {\"actuals\":actuals, \"preds\":preds,\"conf\":pred_confs, \"grade\":grades, \"track\":tracks, \"bfsps\":bfsps})#, \"sp\":start_prices })\n",
    "    \n",
    "    logdf['correct'] = logdf.apply(lambda x: 1 if x['actuals']==x['preds'] else 0, axis=1)\n",
    "    logdf['profit'] = logdf.apply(lambda x: 0 if x['bfsps']<1 else x['bfsps']-1  if x['correct'] else -1, axis=1)\n",
    "    logdf.to_csv('logDFtest.csv')\n",
    "\n",
    "    table = wandb.Table(dataframe=logdf)\n",
    "\n",
    "    logdf['count'] = 1\n",
    "    logdf['eligible_ct'] = logdf.apply(lambda x: 1 if x['bfsps'] > 1 else 0, axis = 1)\n",
    "\n",
    "    track_df = logdf.groupby('track', as_index=False).sum().reset_index()\n",
    "    track_table = wandb.Table(dataframe=track_df)\n",
    "    \n",
    "    prices_df = pd.DataFrame(price_dict)\n",
    "\n",
    "\n",
    "    prices_df['sum_price'] = prices_df.apply(lambda x: sum(x['prices']), axis = 1)\n",
    "    prices_df = prices_df[prices_df['sum_price']>0]\n",
    "\n",
    "    prices_flat = [item for sublist in prices_df['prices'].tolist() for item in sublist]\n",
    "    pred_prices = [item for sublist in prices_df['pred_price'].tolist() for item in sublist]\n",
    "    onehot_win  = [item for sublist in prices_df['onehot_win'].tolist() for item in sublist]\n",
    "    flat_margins = [item for sublist in prices_df['margin'].tolist() for item in sublist]\n",
    "    flat_dogs = [item for sublist in prices_df['dogID'].tolist() for item in sublist]\n",
    "    flat_races = [item for sublist in prices_df['raceID'].tolist() for item in sublist]\n",
    "    all_price_df = pd.DataFrame(data={'flat_dogs':flat_dogs,'flat_races':flat_races,'prices':prices_flat, 'pred_price':pred_prices, 'onehot_win':onehot_win,'split_margin':flat_margins})\n",
    "    all_price_df = all_price_df[all_price_df['prices']>1]\n",
    "    all_price_df['imp_prob'] =  all_price_df.apply(lambda x: 1/x['prices'], axis = 1)\n",
    "    all_price_df['pred_prob'] =  all_price_df.apply(lambda x: 1/x['pred_price'], axis = 1)\n",
    "    all_price_df['bet amount'] = all_price_df.apply(lambda x: (x['pred_prob']-x['imp_prob'])*100 if (x['pred_prob']>x['imp_prob'])&(1>x['imp_prob']>0) else 0, axis = 1)\n",
    "    all_price_df['profit'] = all_price_df.apply(lambda x: x['bet amount']*(x['prices']-1)*0.95 if x['onehot_win'] else -1*x['bet amount'], axis = 1)\n",
    "    all_price_df['flat_profit'] = all_price_df.apply(lambda x: 1*(x['prices']-1)*0.95 if (x['onehot_win'] and x['bet amount']) else -1, axis = 1)\n",
    "    all_price_df['colour'] = all_price_df.apply(lambda x: \"profitz\" if x['profit']>0 else (\"loss\" if x['profit']<0 else (\"no bet - win\" if x['onehot_win'] else \"no bet\")), axis=1)\n",
    "\n",
    "    all_price_table = wandb.Table(dataframe=all_price_df)\n",
    "\n",
    "\n",
    "    price_table = wandb.Table(dataframe=prices_df)\n",
    "\n",
    "    try:\n",
    "        wandb.log({\"table_key\": table,\"loss_table\": loss_table,\"track_df\": track_table,\"price_table\":price_table, 'allprice_df':all_price_df })\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    #print(f\"accuray: {correct/total}\")\n",
    "    wandb.log({\"accuracy\": correct/total, \"loss_val\": torch.mean(loss_val)/len_test, \"correct\": correct, 'profit': logdf[logdf['actuals']==logdf['preds']]['profit'].sum(), 'multibet profit':all_price_df['profit'].sum(), 'multibet outlay':all_price_df['bet amount'].sum() })\n",
    "\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "def validate_modelv2(model:GRUNet,raceDB:Races):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    sft_max = nn.Softmax(dim=-1)\n",
    "    batch_size=10\n",
    "    len_test = len(raceDB.raceIDs)-batch_size\n",
    "    list_t = [] \n",
    "    last = 0\n",
    "    loss_val = 0 \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    preds = []\n",
    "    grades = []\n",
    "    tracks = []\n",
    "    pred_confs = []\n",
    "    bfsps = []\n",
    "    start_prices = []\n",
    "    loss_l = []\n",
    "    loss_t = []\n",
    "    margins_l = []\n",
    "    preds_l = []\n",
    "    pred_sftmax = []\n",
    "    raw_margins = []\n",
    "    raw_places = []\n",
    "    margins_prob = []\n",
    "\n",
    "    prices_list = []\n",
    "    raw_prices = []\n",
    "\n",
    "    price_dict = {}\n",
    "    price_dict['prices'] = []\n",
    "    price_dict['imp_prob'] = []\n",
    "    price_dict['pred_prob'] = []\n",
    "    price_dict['pred_price'] = []\n",
    "    price_dict['margin'] = []\n",
    "    price_dict['onehot_win'] = []\n",
    "    with torch.no_grad():\n",
    "        for i in trange(0,len_test,batch_size, leave=False):\n",
    "            races_idx = range(last,last+batch_size)\n",
    "            last = i\n",
    "            race = raceDB.get_race_input(races_idx)\n",
    "            #tracks.extend([r.track for r in race])\n",
    "            X = race\n",
    "            y = torch.stack([x.classes for x in race])\n",
    "            output = model(X)\n",
    "            #print(y)\n",
    "\n",
    "            _, actual = torch.max(y.data, 1)\n",
    "            onehot_win = F.one_hot(actual, num_classes=8)\n",
    "            conf, predicted = torch.max(output.data, 1)\n",
    "            correct += (predicted == actual).sum().item()\n",
    "\n",
    "            softmax_preds = sft_max(output)\n",
    "\n",
    "            \n",
    "            total += batch_size\n",
    "            actuals.extend(actual.tolist())\n",
    "            preds.extend(predicted.tolist())\n",
    "            pred_confs.extend(conf.tolist())\n",
    "            tracks.extend([r.track_name for r in race])\n",
    "            grades.extend([r.grade for r in race])\n",
    "            for i,dog_idx in enumerate(actual.tolist()):\n",
    "                bfsps.append(race[i].dogs[dog_idx].bfsp)\n",
    "                #start_prices.append(race[i].dogs[dog_idx].sp)\n",
    "\n",
    "\n",
    "            price_dict['prices'].extend([x.prices for x in race])\n",
    "            price_dict['imp_prob'].extend([x.implied_prob for x in race])\n",
    "            price_dict['pred_prob'].extend(softmax_preds.tolist())\n",
    "            #print([(1/(x+(-7**10))).tolist() for x in torch.exp(output)])\n",
    "            price_dict['pred_price'].extend([(1/(x)).tolist() for x in softmax_preds])\n",
    "            price_dict['margin'].extend([x.raw_margins for x in race])\n",
    "            price_dict['onehot_win'].extend(onehot_win.tolist())\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    logdf = pd.DataFrame(data = {\"actuals\":actuals, \"preds\":preds,\"conf\":pred_confs, \"grade\":grades, \"track\":tracks, \"bfsps\":bfsps})#, \"sp\":start_prices })\n",
    "    \n",
    "    logdf['correct'] = logdf.apply(lambda x: 1 if x['actuals']==x['preds'] else 0, axis=1)\n",
    "    logdf['profit'] = logdf.apply(lambda x: 0 if x['bfsps']<1 else x['bfsps']-1  if x['correct'] else -1, axis=1)\n",
    "    logdf.to_csv('logDFtest.csv')\n",
    "\n",
    "    table = wandb.Table(dataframe=logdf)\n",
    "\n",
    "    logdf['count'] = 1\n",
    "    logdf['eligible_ct'] = logdf.apply(lambda x: 1 if x['bfsps'] > 1 else 0, axis = 1)\n",
    "\n",
    "    track_df = logdf.groupby('track', as_index=False).sum().reset_index()\n",
    "    \n",
    "    prices_df = pd.DataFrame(price_dict)\n",
    "\n",
    "\n",
    "    prices_df['sum_price'] = prices_df.apply(lambda x: sum(x['prices']), axis = 1)\n",
    "    prices_df = prices_df[prices_df['sum_price']>0]\n",
    "\n",
    "    prices_flat = [item for sublist in prices_df['prices'].tolist() for item in sublist]\n",
    "    pred_prices = [item for sublist in prices_df['pred_price'].tolist() for item in sublist]\n",
    "    onehot_win  = [item for sublist in prices_df['onehot_win'].tolist() for item in sublist]\n",
    "    flat_margins = [item for sublist in prices_df['margin'].tolist() for item in sublist]\n",
    "    all_price_df = pd.DataFrame(data={'prices':prices_flat, 'pred_price':pred_prices, 'onehot_win':onehot_win,'split_margin':flat_margins})\n",
    "    all_price_df = all_price_df[all_price_df['prices']>1]\n",
    "    all_price_df['imp_prob'] =  all_price_df.apply(lambda x: 1/x['prices'], axis = 1)\n",
    "    all_price_df['pred_prob'] =  all_price_df.apply(lambda x: 1/x['pred_price'], axis = 1)\n",
    "    all_price_df['bet amount'] = all_price_df.apply(lambda x: (x['pred_prob']-x['imp_prob'])*100 if (x['pred_prob']>x['imp_prob'])&(1>x['imp_prob']>0) else 0, axis = 1)\n",
    "    all_price_df['profit'] = all_price_df.apply(lambda x: x['bet amount']*(x['prices']-1)*0.95 if x['onehot_win'] else -1*x['bet amount'], axis = 1)\n",
    "    all_price_df['flat_profit'] = all_price_df.apply(lambda x: 1*(x['prices']-1)*0.95 if (x['onehot_win'] and x['bet amount']) else -1, axis = 1)\n",
    "    all_price_df['colour'] = all_price_df.apply(lambda x: \"profitz\" if x['profit']>0 else (\"loss\" if x['profit']<0 else (\"no bet - win\" if x['onehot_win'] else \"no bet\")), axis=1)\n",
    "\n",
    "\n",
    "    #print(f\"accuray: {correct/total}\")\n",
    "    stats = {\"accuracy\": correct/total,  \"correct\": correct, 'profit': logdf[logdf['actuals']==logdf['preds']]['profit'].sum(), 'multibet profit':all_price_df['profit'].sum(), 'multibet outlay':all_price_df['bet amount'].sum() }\n",
    "    print(stats)\n",
    "    return all_price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(loss, example_ct, epoch):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, raceDB:Races, criterion, optimizer,scheduler, config=None):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    last = 0\n",
    "    batch_size = 25\n",
    "    len_train = len(raceDB.train_race_ids)-batch_size\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "    m = nn.LogSoftmax(dim=1)\n",
    "    epoch_loss=0\n",
    "    batch_before_backwards = 10\n",
    "    saved_batches = []\n",
    "    #losses = torch.tesnor()\n",
    "    for epoch in trange(2000):\n",
    "        model.train()\n",
    "        batch_ct = 0\n",
    "        setup_loss = 1\n",
    "        batch_size = randint(500,2000)\n",
    "        len_train = len(raceDB.train_race_ids)-batch_size\n",
    "        for i in trange(0,len_train,batch_size, leave=False):\n",
    "            last = i\n",
    "            #print(f\"{i=}\\n{batch_ct=}, {setup_loss=}, {batch_ct+1%10==0=}\")\n",
    "            if ((batch_ct+1)%batch_before_backwards==0):\n",
    "                if setup_loss:\n",
    "                    print(\"hit here\")\n",
    "                    continue\n",
    "                else:\n",
    "                    epoch_loss = torch.mean(epoch_loss)\n",
    "                    optimizer.zero_grad()\n",
    "                    epoch_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    model.zero_grad()\n",
    "                    raceDB.detach_all_hidden_states()\n",
    "                    setup_loss = 1\n",
    "                    wandb.log({\"batch_loss\": epoch_loss.item(), \"batch_before_backwards\": batch_before_backwards}, step = example_ct)\n",
    "                    batch_before_backwards = randint(5,20)\n",
    "                    model(saved_batches)\n",
    "                    saved_batches = []\n",
    "\n",
    "            batch_ct += 1\n",
    "\n",
    "            races_idx = range(last,last+batch_size)\n",
    "            race = raceDB.get_train_input(races_idx)\n",
    "            X = race\n",
    "            saved_batches.extend(X)\n",
    "            y = torch.stack([x.classes for x in race])\n",
    "            w = torch.stack([x.win_weight for x in race])\n",
    "            _, actual = torch.max(y.data, 1)\n",
    "            output = model(X)\n",
    "\n",
    "            \n",
    "            example_ct +=  batch_size\n",
    "\n",
    "            loss = criterion(output, y)*w\n",
    "            if setup_loss:\n",
    "                epoch_loss = loss\n",
    "                setup_loss=0\n",
    "\n",
    "            if ((batch_ct + 1) % 25) == 0:                    \n",
    "                    train_log(torch.mean(loss), example_ct, epoch)\n",
    "\n",
    "            #epoch_loss = torch.stack([epoch_loss, loss])\n",
    "            epoch_loss = epoch_loss + loss\n",
    "            \n",
    "\n",
    "        #print(\"finished epoch\")\n",
    "        setup_loss = 1\n",
    "\n",
    "        wandb.log({\"epoch_loss\": torch.mean(epoch_loss)}, step = example_ct)\n",
    "        acc = validate_model(model,raceDB,criterion, 8, example_ct, epoch_loss, batch_ct)\n",
    "        epoch_loss = 0  \n",
    "        #print(acc)\n",
    "        scheduler.step(acc)\n",
    "    #print(losses)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\DATA\")\n",
    "dog_stats_file = pd.read_pickle('new gru input 2023-01.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_stats_df = dog_stats_file\n",
    "dog_stats_df['track_name'] = dog_stats_df[dog_stats_df['track_name'].notna()]['track_name']\n",
    "dog_stats_df['track_name'] = dog_stats_df['track_name'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1105508, 15)\n",
      "(1105508, 15)\n",
      "num_features_per_dog=16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabe51cf026a4ddcaac34c9720cd118f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39367 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a2d02bd1544b52a27a3f5bd04f8ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of races = 155094, number of unique dogs = 39367\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.getcwd()\n",
    "os.chdir(r\"C:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\DATA\")\n",
    "#dog_stats_file = open( 'new gru input 2023-01.npy', 'rb')\n",
    "hidden_size = 64\n",
    "raceDB = build_dataset('new gru input 2023-01.npy', hidden_size ,track_filer=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples 151326, Test examples 3768\n"
     ]
    }
   ],
   "source": [
    "date = datetime.datetime.strptime(\"2022-12-15\", \"%Y-%m-%d\")\n",
    "raceDB.create_test_split_date(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# dog_stats_file = open( r'C:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\DATA\\prediction validation 2023-01.npy', 'rb')\n",
    "# hidden_size = 64\n",
    "# predDB = build_dataset(dog_stats_file, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #os.chdir(r\"C:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\DATA\")\n",
    "# dog_stats_file = open( r'C:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\DATA\\forms\\prediction_inputs_gru.npy', 'rb')\n",
    "# hidden_size = 64\n",
    "# predDB = build_pred_dataset(dog_stats_file, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raceDB.create_hidden_states_dict()\n",
    "# predDB.fill_hidden_states_from_dict(hidden_dict=raceDB.hidden_states_dict)\n",
    "# # len(raceDB.hidden_states_dict)\n",
    "# # predDB.to_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "def predict_model(model,predDB):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    list_t = [] \n",
    "    last = 0\n",
    "    loss_val = 0 \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        races_idx = range(0,len(predDB.raceIDs)-1)\n",
    "        race = predDB.get_race_input(races_idx)\n",
    "        X = race\n",
    "        # for i,r in enumerate(race):\n",
    "        #     print(r.raceid, r.track_name)\n",
    "        #     #print(i,r.lstm_input())\n",
    "\n",
    "        output = model(X)\n",
    "        \n",
    "        print(output)\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        for i,r in enumerate(race):\n",
    "            print(r.raceid, r.track_name, r.dogs[predicted[i].item()])\n",
    "\n",
    "        print(predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure(optimizer, criterion, outs, classes):\n",
    "    optimizer.zero_grad()\n",
    "    loss = nn.functional.mse_loss(outs, classes)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "def model_pipeline(my_dataset=raceDB,config=None,prev_model=None, sweep=True, model_state_dict=None):\n",
    "    if my_dataset:\n",
    "      dataset = my_dataset    \n",
    "    else:\n",
    "      dataset = raceDB\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"GRU - FastTrack - NZ Testing\", config=config):\n",
    "      #  access all HPs through wandb.config, so logging matches execution!\n",
    "      wandb.define_metric(\"loss\", summary=\"min\")\n",
    "      wandb.define_metric(\"test_accuracy\", summary=\"max\")\n",
    "      wandb.define_metric(\"bfprofit\", summary=\"max\")\n",
    "      config = wandb.config\n",
    "      pprint.pprint(config)\n",
    "      pprint.pprint(config.epochs)\n",
    "      print(config)\n",
    "\n",
    "      model = GRUNet(203,config['hidden_size'])\n",
    "      if model_state_dict:\n",
    "        model.load_state_dict(model_state_dict)\n",
    "      #criterion = nn.KLDivLoss(reduction='none', log_target=True)\n",
    "      criterion = nn.CrossEntropyLoss(reduction='none', label_smoothing=0.001)\n",
    "      optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "      scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max',threshold=0.001, patience=50, verbose=True, factor=0.5)\n",
    "      model = model.to(device)\n",
    "      # optimizer = optimizer.to(device)\n",
    "      print(model)\n",
    "\n",
    "      # and use them to train the model\n",
    "      try:\n",
    "        train(model, dataset, criterion, optimizer, scheduler, config)\n",
    "      except KeyboardInterrupt:\n",
    "        print(\"finished Early\")\n",
    "      dataset.create_hidden_states_dict()\n",
    "      # if sweep:\n",
    "    #   raceDB.reset_all_lstm_states\n",
    "    \n",
    "\n",
    "\n",
    "    # and test its final performance\n",
    "    #test(model, test_loader)\n",
    "\n",
    "    return (model,dataset, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raceDB.raceIDs)\n",
    "wandb_config_static = {'hidden_size':hidden_size,'batch_size': 500, 'dropout': 0.3, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64 , 'learning_rate': 0.0001, 'loss': 'L1', 'l1_beta':0.1,  'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raceDB.detach_all_hidden_states()\n",
    "# raceDB.create_test_split()\n",
    "# raceDB.reset_all_lstm_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = r\"C:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\savedmodel\\Good profit NZ before update.pt\\Good profit NZ before update.pt_450.pt\"\n",
    "# saved_dict = torch.load(save_path)\n",
    "# # model_state_dict = saved_dict['model_state_dict']\n",
    "# raceDB.fill_hidden_states_from_dict(saved_dict['db'])\n",
    "# raceDB.to_cuda()\n",
    "# # predDB.fill_hidden_states_from_dict(hidden_dict=raceDB.hidden_states_dict)\n",
    "# # predDB.to_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model,dataset, optimizer) = model_pipeline(raceDB,config=wandb_config_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop\n",
    "model = GRUNet(203,64).to(device)\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6886b981d21406fb3cd633198e9408b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.22962962962962963, 'correct': 62, 'profit': 155.8730624273856, 'multibet profit': 610.4800603151755, 'multibet outlay': 5630.432833115364}\n"
     ]
    }
   ],
   "source": [
    "all_price_df = validate_modelv2(model, predDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss            810\n",
       "no bet          380\n",
       "no bet - win    135\n",
       "profitz          58\n",
       "Name: colour, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_price_df.colour.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pause' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\LSTM\\GRU - WANDB - priced - Fasttrack v5.5 NZ.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/GRU%20-%20WANDB%20-%20priced%20-%20Fasttrack%20v5.5%20NZ.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pause\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New%20Model/LSTM/GRU%20-%20WANDB%20-%20priced%20-%20Fasttrack%20v5.5%20NZ.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m (model,dataset, optimizer) \u001b[39m=\u001b[39m model_pipeline(raceDB,config\u001b[39m=\u001b[39mwandb_config_static, prev_model\u001b[39m=\u001b[39mmodel)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pause' is not defined"
     ]
    }
   ],
   "source": [
    "pause\n",
    "(model,dataset, optimizer) = model_pipeline(raceDB,config=wandb_config_static, prev_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1383, 10)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# \n",
    "# \n",
    "# all_price_df[all_price_df['onehot_win']==1].sort_values('prices', ascending=False).shape\n",
    "all_price_df.sort_values('prices', ascending=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.create_hidden_states_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created path\n"
     ]
    }
   ],
   "source": [
    "model_saver(model, optimizer, 450, 0.1, dataset.hidden_states_dict, model_name=\"All races long run 1k epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\"method\": \"random\"}\n",
    "\n",
    "metric = {\"name\": \"profit\", \"goal\": \"maximize\"}\n",
    "\n",
    "sweep_config[\"metric\"] = metric\n",
    "\n",
    "\n",
    "parameters_dict = {\n",
    "    \"optimizer\": {\"value\": \"adamW\"},\n",
    "    \"f1_layer_size\": {\"values\": [256]},\n",
    "    \"f2_layer_size\": {\"values\": [64]},\n",
    "    \"dropout\": {\"values\": [0.3]},\n",
    "    \"len_data\": {\"value\": 70000},\n",
    "    \"hidden_size\": {\"value\":16}\n",
    "}\n",
    "\n",
    "sweep_config[\"parameters\"] = parameters_dict\n",
    "\n",
    "parameters_dict.update(\n",
    "    {\n",
    "        \"epochs\": {\"values\": [250]},\n",
    "        \"validation_split\": {\"value\": 0.1},\n",
    "        \"loss\": {\n",
    "            \"values\": [ \"L1\"],\n",
    "            # \"values\": [\"Huber\", \"MSE\", \"L1\", \"BCE\", \"Custom\", \"KL\"]\n",
    "            # 'value': 'l1_custom'\n",
    "        },\n",
    "        \"num_layers\": {\"values\": [2]},\n",
    "    }\n",
    ")\n",
    "\n",
    "parameters_dict.update(\n",
    "    {\n",
    "        \"learning_rate\": {\n",
    "            # a flat distribution between 0 and 0.1\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.00011,\n",
    "            \"max\": 0.001,\n",
    "        },\n",
    "        \"l1_beta\": {\"value\": 0.1\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            'values': [500,1000,5000]\n",
    "            # \"values\": [32, 64, 128, 360, 720]\n",
    "            # 'values':[4,8,16,32,64,128,360]\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)\n",
    "\n",
    "\n",
    "sweep_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"LSTM_sweeps\")\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "wandb.agent(sweep_id, function=model_pipeline, count=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43115443075634c02a7c247a87b0dd9d74842892e56d473b9e19f544f3149aff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
