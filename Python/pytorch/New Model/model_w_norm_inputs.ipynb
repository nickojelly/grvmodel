{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import wandb\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "with torch.profiler.profile() as profiler:\n",
    "        pass\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnickojelly\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nick\\.conda\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluded =  0\n"
     ]
    }
   ],
   "source": [
    "REBUILD_DATA = True\n",
    "\n",
    "\n",
    "class GRV:\n",
    "    # class to store training data\n",
    "\n",
    "    # reading data from pickle\n",
    "    file = open(r\"DATA/total_list_71k_splitmargins_weight.npy\", \"rb\")\n",
    "    data = pickle.load(file)\n",
    "    # seperate out classes from inputs\n",
    "    raceIDs, inputs, classes, prices, win_price, margins, betfairSP = zip(*data)\n",
    "    # removing nan from inputs and convert to float\n",
    "    inputs_df = pd.DataFrame(inputs)\n",
    "    inputs_df.fillna(value=-1, inplace=True)\n",
    "    inputs = inputs_df.values.tolist()\n",
    "    inputs = [[float(i) for i in j] for j in inputs]\n",
    "\n",
    "    # data\n",
    "    training_data = []\n",
    "\n",
    "    def make_training_data(self):\n",
    "        excluded = 0\n",
    "        for i in range(len(self.inputs)):\n",
    "            if len(self.classes[i]) == 8:\n",
    "                self.training_data.append(\n",
    "                    [\n",
    "                        np.array(self.inputs[i]),\n",
    "                        np.array(self.classes[i]),\n",
    "                        np.array(self.prices[i]),\n",
    "                        np.array(self.margins[i]),\n",
    "                        np.array(self.betfairSP[i]),\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                adjustedList = self.classes[i] + ([8] * (8 - len(self.classes[i])))\n",
    "                adjustedListP = self.prices[i] + ([0] * (8 - len(self.prices[i])))\n",
    "                adjustedListM = self.margins[i] + ([100] * (8 - len(self.margins[i])))\n",
    "                adjustedListSP = self.betfairSP[i] + ([0] * (8 - len(self.betfairSP[i]))) # USED TO BE MARGINS DONT KNOW WHY\n",
    "                self.training_data.append(\n",
    "                    [\n",
    "                        np.array(self.inputs[i]),\n",
    "                        np.array(adjustedList),\n",
    "                        np.array(adjustedListP),\n",
    "                        np.array(adjustedListM),\n",
    "                        np.array(adjustedListSP)\n",
    "                    ]\n",
    "                )\n",
    "                if len(adjustedList) != 8:\n",
    "                    print(adjustedList)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print(\"excluded = \", excluded)\n",
    "\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    grv = GRV()\n",
    "    grv.make_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_2904\\383741329.py:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  Y_w = torch.tensor([i for i in Y_w])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71768\n"
     ]
    }
   ],
   "source": [
    "softmin = nn.Softmin(dim=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "# dataset setup\n",
    "training_data = grv.training_data\n",
    "\n",
    "X = torch.Tensor(np.array([i[0] for i in training_data]))\n",
    "Y = torch.Tensor(np.array([i[1] for i in training_data]))\n",
    "P = torch.Tensor(np.array([i[2] for i in training_data]))\n",
    "Y_m = softmin(torch.Tensor(np.array([i[3] for i in training_data]))).float()\n",
    "bfSP = torch.Tensor(np.array([i[4] for i in training_data]))\n",
    "len_dataset = len(X)\n",
    "# Generate winner only class\n",
    "Y_w = []\n",
    "for i in Y:\n",
    "    n = np.zeros(8)\n",
    "    index = torch.argmin(i)\n",
    "    n[index] = float(1)\n",
    "    Y_w.append(n)\n",
    "\n",
    "Y_w = torch.tensor([i for i in Y_w])\n",
    "\n",
    "scaler.fit(X)\n",
    "Xn = scaler.transform(X)\n",
    "Xn = torch.tensor(Xn).float()\n",
    "\n",
    "with open(\"normalizer.npy\", \"wb\") as fp:   #Pickling\n",
    "    \n",
    "    pickle.dump(scaler, fp)\n",
    "\n",
    "# Xn = nn.functional.normalize(X, dim = 1)\n",
    "X = Xn.to(device)\n",
    "Y_w = Y_w.to(device)\n",
    "\n",
    "\n",
    "Y_m = Y_m.to(device)\n",
    "P = P.to(device)\n",
    "bfSP = bfSP.to('cuda')\n",
    "bfSP = torch.nan_to_num(bfSP, nan=0)\n",
    "my_dataset = TensorDataset(X, Y_m, P, bfSP)\n",
    "my_dataloader = DataLoader(my_dataset)\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1902e-08, 5.4522e-01, 1.6185e-07,  ..., 3.9502e-04, 2.0311e-02,\n",
      "         7.0031e-05],\n",
      "        [1.9163e-06, 9.9489e-01, 4.2758e-07,  ..., 4.1068e-03, 9.8058e-07,\n",
      "         9.0723e-04],\n",
      "        [1.0918e-02, 2.2546e-04, 3.8218e-07,  ..., 6.6391e-04, 2.8960e-08,\n",
      "         2.3285e-01],\n",
      "        ...,\n",
      "        [7.3866e-04, 1.2708e-04, 8.1634e-04,  ..., 9.2249e-01, 3.5032e-44,\n",
      "         3.5032e-44],\n",
      "        [1.4338e-03, 2.6000e-05, 8.8923e-01,  ..., 4.7621e-07, 4.4387e-03,\n",
      "         3.3631e-44],\n",
      "        [7.4100e-05, 9.9990e-01, 1.0864e-05,  ..., 1.0028e-05, 3.7835e-44,\n",
      "         3.7835e-44]], device='cuda:0')\n",
      "tensor([0.9614, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000], device='cuda:0')\n",
      "tensor([0.9614, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "print(Y_m)\n",
    "print(X[:, 119])\n",
    "print(Xn[:, 119])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to pass in dataset to get_data (created above)\n",
    "def make_loader(dataset, config, train=True):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(config[\"validation_split\"] * dataset_size))\n",
    "    random_seed = 42\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    if train:\n",
    "        dataset_sampler = SubsetRandomSampler(train_indices)\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "            num_workers=0,\n",
    "            sampler=dataset_sampler\n",
    "        )\n",
    "    else:\n",
    "        dataset_sampler = SubsetRandomSampler(val_indices)\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "            num_workers=0,\n",
    "            sampler=dataset_sampler,\n",
    "        )\n",
    "\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(f1_layer_size, f2_layer_size, dropout, num_layers=2):\n",
    "\n",
    "    if num_layers == 2:\n",
    "        network = nn.Sequential(  # fully-connected, dual hidden layer\n",
    "            nn.Linear(128, f1_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(f1_layer_size, f2_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(f2_layer_size, 8),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        network = nn.Sequential(  # fully-connected, dual hidden layer\n",
    "            nn.Linear(128, f1_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(f1_layer_size, f2_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(f2_layer_size, f2_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(f2_layer_size, 8),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_MSE(output, target):\n",
    "    sorts = torch.argsort(target)\n",
    "    out = sorts.narrow(1,0,3)\n",
    "    ohe = torch.nn.functional.one_hot(out, num_classes=8).sum(1)\n",
    "    out_first3 = ohe*output\n",
    "    target_ohe = ohe*target\n",
    "    loss = torch.sum((((target_ohe+1)*abs(out_first3-target_ohe)+1)**2))\n",
    "    # loss = torch.sum(((out_first3-target_ohe))**2)\n",
    "    return loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,3,5,6]])\n",
    "y =torch.tensor([[6,2,3,4,5]]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_order_MSE(output, target):\n",
    "    sorts = torch.argsort(target)\n",
    "    print(sorts)\n",
    "    out = sorts.narrow(1,0,3)\n",
    "    ohe = torch.nn.functional.one_hot(out, num_classes=8).sum(1)\n",
    "    out_first3 = ohe*output\n",
    "    target_ohe = ohe*target\n",
    "    loss = torch.mean(((abs(out_first3-target_ohe)*10)**2))\n",
    "    # loss = torch.sum(((out_first3-target_ohe))**2)\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_customiser(x,y,loss_func=nn.HuberLoss(reduction='none',delta=0.1)):\n",
    "    sorts_t = torch.argsort(y)\n",
    "    out = sorts_t.narrow(1,0,3)\n",
    "    ohe = torch.nn.functional.one_hot(out, num_classes=8).sum(1)\n",
    "    loss = loss_func(x,y)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_l1(x=None,y=None,beta=0.1):\n",
    "    loss_func=nn.SmoothL1Loss(reduction='none',beta=beta)\n",
    "    sorts_t = torch.argsort(y)\n",
    "    sorts_o = torch.argsort(x)\n",
    "    diff_place = abs(sorts_t-sorts_o)\n",
    "    out = sorts_t.narrow(1,0,3)\n",
    "    ohe = torch.nn.functional.one_hot(out, num_classes=8).sum(1)\n",
    "    loss = loss_func(x,y)*ohe\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_l1_place(x=None,y=None,beta=0.1):\n",
    "    loss_func=nn.SmoothL1Loss(reduction='none',beta=beta)\n",
    "    sorts_t = torch.argsort(y)\n",
    "    sorts_o = torch.argsort(x)\n",
    "    diff_place = abs(sorts_t-sorts_o)\n",
    "    out = sorts_t.narrow(1,0,3)\n",
    "    ohe = torch.nn.functional.one_hot(out, num_classes=8).sum(1)\n",
    "    loss = loss_func(x,y)*diff_place\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_l1_place_1(x=None,y=None,beta=0.1):\n",
    "    loss_func=nn.SmoothL1Loss(reduction='none',beta=beta)\n",
    "    sorts_t = torch.argsort(y)\n",
    "    weight_adj = 1/(sorts_t+0.5) # WOOOH MAGIC NUMBER BAD :(\n",
    "    out = sorts_t.narrow(1,0,3)\n",
    "    ohe = torch.nn.functional.one_hot(out, num_classes=8).sum(1)\n",
    "    loss = loss_func(x,y)*weight_adj\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config, dataset):\n",
    "    # Make the data\n",
    "\n",
    "    train_loader = make_loader(dataset, config, train=True)\n",
    "    test_loader = make_loader(dataset, config, train=False)\n",
    "    # Make the model\n",
    "    # model = Net().to(device)\n",
    "    model = build_network(\n",
    "        config[\"f1_layer_size\"], config[\"f2_layer_size\"], config[\"dropout\"], config[\"num_layers\"]\n",
    "    )\n",
    "\n",
    "    loss_functions = {\n",
    "        \"Huber\":nn.HuberLoss(),\n",
    "        \"Huber_custom\":loss_customiser,\n",
    "        \"l1_custom\":custom_l1_place,\n",
    "        \"l1_custom_1stplace\":custom_l1_place_1,\n",
    "        \"MSE\":nn.MSELoss(),\n",
    "        \"L1\":nn.SmoothL1Loss(reduction='mean', beta=config[\"l1_beta\"]),\n",
    "        \"BCE\":nn.CrossEntropyLoss(),\n",
    "        \"Custom\":custom_MSE,\n",
    "        \"KL\":nn.KLDivLoss(reduction='batchmean'),\n",
    "        \"NLL\":nn.NLLLoss()\n",
    "    }\n",
    "    # Make the loss and optimizer\n",
    "    #  criterion = nn.NLLLoss()\n",
    "    loss_f = loss_functions[config['loss']]\n",
    "    criterion = loss_f\n",
    "    optimizer = config[\"optimizer\"]\n",
    "\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(), lr=config[\"learning_rate\"], momentum=0.9\n",
    "        )\n",
    "    if optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    print(optimizer)\n",
    "    if optimizer == \"adamW\":\n",
    "        print(\"HERE\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    return model, train_loader, test_loader, criterion, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, batch_ct):\n",
    "    model.eval()\n",
    "    classL, predL, maxL, correctL, priceP, priceR, bfPriceR, pred_odds, model_outputs = [], [], [], [], [], [], [], [], []\n",
    "    # Run the model on some test examples\n",
    "    with torch.no_grad():\n",
    "        correct, total, max_sum, max_w_sum, profit, bfprofit, bfnotavail, bf_available,bf_only_bets, bf_only_profit, test_val,bf_sp_prices = 0, 0, 0, 0, 0,0,0,0,0,0 ,0,0\n",
    "        alt_bet_1, alt_bet_2 = 0,0\n",
    "        value_pick_correct, value_pick_profit = 0, 0\n",
    "        num_bets = 0\n",
    "        for images, labels, prices, bfspPrices in test_loader:\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            #converts prices from tensor to list\n",
    "            prices = prices[0,].tolist()\n",
    "            bfspPrices = bfspPrices[0,].tolist()\n",
    "\n",
    "            #gets the prediction and its confidence and the real class\n",
    "            max, predicted = torch.max(outputs.data, 1)\n",
    "            _, real = torch.max(labels.data, 1)\n",
    "\n",
    "            #converts prediction from tensor to item\n",
    "            prediction = predicted.item()\n",
    "            real_item = real.item()\n",
    "\n",
    "            #appends prediction and likelyhood to lists\n",
    "            predL.append(prediction)\n",
    "            maxL.append(max.item())\n",
    "\n",
    "        \n",
    "            total += labels.size(0)\n",
    "            correct += prediction == real_item\n",
    "\n",
    "            correctL.append(int(prediction == real_item))\n",
    "            classL.append(real_item)\n",
    "\n",
    "            priceR.append(prices[real_item])\n",
    "            priceP.append(prices[prediction])\n",
    "            bfPriceR.append(bfspPrices[real_item])\n",
    "            # print(outputs.data.flatten().tolist())\n",
    "\n",
    "            predicted_odds = [\n",
    "                1 / ((x + 10**-7)) for x in outputs.data.flatten().tolist()\n",
    "            ]\n",
    "\n",
    "            pred_odds.append(predicted_odds)\n",
    "            model_outputs.append(outputs.data.flatten().tolist())\n",
    "\n",
    "            if prices[real_item] > (predicted_odds[real_item]):\n",
    "                value_pick_correct += 1\n",
    "                value_pick_profit += prices[real_item]\n",
    "                \n",
    "\n",
    "            bets = [x > (y) for x, y in zip(prices, predicted_odds)]\n",
    "            num_bets += sum(bets)\n",
    "\n",
    "            value_pick_profit += -sum(bets)\n",
    "\n",
    "            delta_pred = prices[prediction]-predicted_odds[prediction]\n",
    "            alt_bet_1 -= 1+10*delta_pred\n",
    "            alt_bet_2 -= 5\n",
    "\n",
    "\n",
    "            if bfspPrices[prediction]:\n",
    "                bf_only_bets += 1\n",
    "                bf_only_profit -= 1\n",
    "                bf_sp_prices += bfspPrices[prediction]\n",
    "                test_val += 1\n",
    "\n",
    "            if prediction == real_item:\n",
    "                max_sum += max\n",
    "                profit += prices[real_item]\n",
    "                if bfspPrices[real_item]:\n",
    "                    bfprofit += bfspPrices[real_item]\n",
    "                    bf_only_profit += bfspPrices[real_item]\n",
    "                    alt_bet_1 += (1+10*delta_pred)*bfspPrices[real_item]\n",
    "                    alt_bet_2 += 5*bfspPrices[real_item]\n",
    "                    bf_available += 1\n",
    "                    test_val -= 1\n",
    "                else:\n",
    "                    bfprofit += prices[real_item]*1.12\n",
    "                    bfnotavail += 1\n",
    "                    alt_bet_1 += (1+10*delta_pred)*prices[real_item]*1.12\n",
    "                    alt_bet_2 += 5*prices[real_item]*1.12\n",
    "            else:\n",
    "                max_w_sum += max\n",
    "\n",
    "            profit += -1\n",
    "            bfprofit += -1\n",
    "\n",
    "            # print(f\"{correct=}\")\n",
    "\n",
    "        # print(f\"Accuracy of the model on the {total} \" +\n",
    "        #       f\"test images: {100 * correct / total}%\" +\n",
    "        #       f\"profit: {profit}\"+\n",
    "        #       f\"profit: {value_pick_profit}\")\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"test_accuracy\": correct / total,\n",
    "                \"correct_conf\": max_sum / correct,\n",
    "                \"incorrect_conf\": (max_w_sum) / (total - correct),\n",
    "                \"profit\": profit,\n",
    "                \"bfprofit\": bfprofit,\n",
    "                \"bfnotavail\": bfnotavail,\n",
    "                \"value_pick_roi\": value_pick_profit / num_bets,\n",
    "                \"value_pick_correct\": value_pick_correct,\n",
    "                \"num_bets_per\": num_bets / total,\n",
    "                \"bf_avail\": bf_available,\n",
    "                \"bfOnlyProfit\": bf_only_profit,\n",
    "                \"bfOnlyBets\": bf_only_bets,\n",
    "                \"test_val\": test_val,\n",
    "                \"alt_bet_1\":alt_bet_1,\n",
    "                \"alt_bet_2\":alt_bet_2\n",
    "            }\n",
    "        )\n",
    "\n",
    "        logdf = pd.DataFrame(\n",
    "            data={\n",
    "                \"class\": classL,\n",
    "                \"pred\": predL,\n",
    "                \"max\": maxL,\n",
    "                \"correct\": correctL,\n",
    "                \"priceR\": priceR,\n",
    "                \"priceP\": priceP,\n",
    "                \"bets\": sum(bets),\n",
    "                \"pred_odds\": pred_odds,\n",
    "                \"model_outputs\": model_outputs,\n",
    "                \"bfodds\" : bfPriceR\n",
    "            }\n",
    "        )\n",
    "        # table = wandb.Table(dataframe=logdf)\n",
    "        # wandb.log({\"table_key\": table})\n",
    "        # classCounts = logdf[\"class\"].value_counts()\n",
    "        # predCounts = logdf[\"pred\"].value_counts()\n",
    "        # boxplot = logdf.boxplot(column=['priceR'],by='correct')\n",
    "        # print(classCounts, predCounts)\n",
    "        # boxplot\n",
    "        # plt.savefig(\"boxplot.png\")\n",
    "        # wandb.log({\"boxplot\":boxplot})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(loss, example_ct, epoch):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
    "    #print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_saver(model, optimizer, epoch, loss):\n",
    "    \n",
    "    pathtofolder = \"C:/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New Model\"\n",
    "    model_name = wandb.run.name\n",
    "    isExist = os.path.exists(\n",
    "        f\"C:/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New Model/savedmodel/{model_name}/\"\n",
    "    )\n",
    "    if isExist:\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": loss,\n",
    "            },\n",
    "            f\"C:/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New Model/savedmodel/{model_name}/{model_name}_{epoch}.pt\",\n",
    "        )\n",
    "    else:\n",
    "        print(\"created path\")\n",
    "        os.makedirs(\n",
    "            f\"C:/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New Model/savedmodel/{model_name}/\"\n",
    "        )\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": loss,\n",
    "            },\n",
    "            f\"C:/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New Model/savedmodel/{model_name}/{model_name}_{epoch}.pt\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader,test_loader, criterion, optimizer, config):\n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Run training and track with wandb\n",
    "    total_batches = len(loader) * config.epochs\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "\n",
    "    raw_inputs = True\n",
    "    if config['loss'] == \"KL\":\n",
    "\n",
    "        for epoch in tqdm(range(config.epochs)):\n",
    "            for _, (images, labels, _ , _) in enumerate(loader):\n",
    "\n",
    "                loss = train_batch_lsftmax(images, labels, model, optimizer, criterion, btch_count=batch_ct, raw_inputs=True)\n",
    "                example_ct +=  len(images)\n",
    "                batch_ct += 1\n",
    "\n",
    "                # Report metrics every 25th batch\n",
    "                if ((batch_ct + 1) % 250) == 0:\n",
    "                    train_log(loss, example_ct, epoch)\n",
    "                    \n",
    "\n",
    "            if epoch %10 ==0:\n",
    "                test(model,test_loader, epoch)\n",
    "                model_saver(model,optimizer,epoch,loss)\n",
    "\n",
    "    else:\n",
    "        for epoch in tqdm(range(config.epochs)):\n",
    "            for _, (images, labels, _ , _) in enumerate(loader):\n",
    "\n",
    "                loss = train_batch(images, labels, model, optimizer, criterion, btch_count=batch_ct, raw_inputs=True)\n",
    "                example_ct +=  len(images)\n",
    "                batch_ct += 1\n",
    "\n",
    "                # Report metrics every 25th batch\n",
    "                if ((batch_ct + 1) % 250) == 0:\n",
    "                    train_log(loss, example_ct, epoch)\n",
    "\n",
    "            if epoch %10 ==0:\n",
    "                test(model,test_loader, epoch)\n",
    "                model_saver(model, optimizer, epoch, loss)\n",
    "\n",
    "def train_batch(images, labels, model, optimizer, criterion, btch_count=0, raw_inputs=True, beta=0.1):\n",
    "    images, labels = images, labels\n",
    "    \n",
    "\n",
    "    # Forward pass ➡\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels.float())\n",
    "    \n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_batch_lsftmax(images, labels, model, optimizer, criterion, btch_count=0, raw_inputs=True):\n",
    "    images, labels = images, labels\n",
    "    \n",
    "\n",
    "    # Forward pass ➡\n",
    "    outputs = model(images)\n",
    "    loss = criterion(F.log_softmax(outputs), labels.float())\n",
    "    \n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(config=None,prev_model=None):\n",
    "    dataset = my_dataset\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"new customs 71.7K\", config=config):\n",
    "      # access all HPs through wandb.config, so logging matches execution!\n",
    "      wandb.define_metric(\"loss\", summary=\"min\")\n",
    "      wandb.define_metric(\"test_accuracy\", summary=\"max\")\n",
    "      wandb.define_metric(\"bfprofit\", summary=\"max\")\n",
    "      config = wandb.config\n",
    "      pprint.pprint(config)\n",
    "      pprint.pprint(config.epochs)\n",
    "      print(config)\n",
    "\n",
    "      # make the model, data, and optimization problem\n",
    "      model, train_loader, test_loader, criterion, optimizer = make(config, dataset)\n",
    "\n",
    "      prev_model = r'C:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\good models\\vague-sweep-3_490.pt'\n",
    "      if prev_model:\n",
    "        checkpoint = torch.load(prev_model, map_location=\"cuda:0\")\n",
    "        print(\"here\")\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "      model = model.to(device)\n",
    "      #optimizer = optimizer.to(device)\n",
    "      print(model)\n",
    "\n",
    "      # and use them to train the model\n",
    "      train(model, train_loader,test_loader, criterion, optimizer, config)\n",
    "\n",
    "      # and test its final performance\n",
    "      #test(model, test_loader)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'profit'},\n",
      " 'parameters': {'batch_size': {'values': [8, 1000]},\n",
      "                'dropout': {'values': [0.3, 0.4, 0.5]},\n",
      "                'epochs': {'values': [1000]},\n",
      "                'f1_layer_size': {'values': [256]},\n",
      "                'f2_layer_size': {'values': [64]},\n",
      "                'l1_beta': {'distribution': 'uniform', 'max': 1, 'min': 0},\n",
      "                'learning_rate': {'distribution': 'uniform',\n",
      "                                  'max': 0.001,\n",
      "                                  'min': 0.0001},\n",
      "                'len_data': {'value': 71768},\n",
      "                'loss': {'values': ['L1']},\n",
      "                'num_layers': {'values': [2]},\n",
      "                'optimizer': {'value': 'adamW'},\n",
      "                'validation_split': {'value': 0.1}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'method': 'random',\n",
       " 'metric': {'name': 'profit', 'goal': 'maximize'},\n",
       " 'parameters': {'optimizer': {'value': 'adamW'},\n",
       "  'f1_layer_size': {'values': [256]},\n",
       "  'f2_layer_size': {'values': [64]},\n",
       "  'dropout': {'values': [0.3, 0.4, 0.5]},\n",
       "  'len_data': {'value': 71768},\n",
       "  'epochs': {'values': [1000]},\n",
       "  'validation_split': {'value': 0.1},\n",
       "  'loss': {'values': ['L1']},\n",
       "  'num_layers': {'values': [2]},\n",
       "  'learning_rate': {'distribution': 'uniform', 'min': 0.0001, 'max': 0.001},\n",
       "  'l1_beta': {'distribution': 'uniform', 'min': 0, 'max': 1},\n",
       "  'batch_size': {'values': [8, 1000]}}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sweep_config = {\"method\": \"random\"}\n",
    "\n",
    "metric = {\"name\": \"profit\", \"goal\": \"maximize\"}\n",
    "\n",
    "sweep_config[\"metric\"] = metric\n",
    "\n",
    "\n",
    "parameters_dict = {\n",
    "    \"optimizer\": {\"value\": \"adamW\"},\n",
    "    \"f1_layer_size\": {\"values\": [256]},\n",
    "    \"f2_layer_size\": {\"values\": [64]},\n",
    "    \"dropout\": {\"values\": [0.3, 0.4, 0.5]},\n",
    "    \"len_data\": {\"value\": len_dataset},\n",
    "}\n",
    "\n",
    "sweep_config[\"parameters\"] = parameters_dict\n",
    "\n",
    "parameters_dict.update(\n",
    "    {\n",
    "        \"epochs\": {\"values\": [1000]},\n",
    "        \"validation_split\": {\"value\": 0.1},\n",
    "        \"loss\": {\n",
    "            \"values\": [ \"L1\"],\n",
    "            # \"values\": [\"Huber\", \"MSE\", \"L1\", \"BCE\", \"Custom\", \"KL\"]\n",
    "            # 'value': 'l1_custom'\n",
    "        },\n",
    "        \"num_layers\": {\"values\": [2]},\n",
    "    }\n",
    ")\n",
    "\n",
    "parameters_dict.update(\n",
    "    {\n",
    "        \"learning_rate\": {\n",
    "            # a flat distribution between 0 and 0.1\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.0001,\n",
    "            \"max\": 0.001,\n",
    "        },\n",
    "        \"l1_beta\": {\n",
    "            # a flat distribution between 0 and 0.1\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0,\n",
    "            \"max\": 1,\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            'values': [8,1000]\n",
    "            # \"values\": [32, 64, 128, 360, 720]\n",
    "            # 'values':[4,8,16,32,64,128,360]\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)\n",
    "\n",
    "\n",
    "sweep_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\wandb\\run-20220930_203508-6lrmaoq8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nickojelly/new%20customs%2071.7K/runs/6lrmaoq8\" target=\"_blank\">wise-dust-23</a></strong> to <a href=\"https://wandb.ai/nickojelly/new%20customs%2071.7K\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64, 'dropout': 0.3, 'epochs': 5000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'learning_rate': 7.5e-05, 'loss': 'L1', 'l1_beta': 0.1, 'len_data': 71768, 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "5000\n",
      "{'batch_size': 64, 'dropout': 0.3, 'epochs': 5000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'learning_rate': 7.5e-05, 'loss': 'L1', 'l1_beta': 0.1, 'len_data': 71768, 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "adamW\n",
      "HERE\n",
      "here\n",
      "Sequential(\n",
      "  (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.3, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.3, inplace=False)\n",
      "  (6): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (7): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/5000 [00:18<25:34:07, 18.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [6:35:49<00:00,  4.75s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6401b8a2e84ea1916376a1d66d55c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>▆▆▆▆▆▅▆█▇▇▇▇▇▆█▆▆▇▇▇▇▆▇█▃▇▇▆▇▇▂█▇█▂▁▇▇▇▆</td></tr><tr><td>alt_bet_2</td><td>▅▆▅▇▅▃▃█▄▅▆▆▅▄▆▅▄▅▆▆▅▄▆▅▂▆▇▅▆▅▂█▅█▂▁▄▇▅▄</td></tr><tr><td>bfOnlyBets</td><td>█▅▅▄▄▅▆▅▅▂▄▃▂▂▁▂▂▂▂▁▂▂▂▃▃▂▃▂▃▂▂▃▅▂▃▃▂▃▄▃</td></tr><tr><td>bfOnlyProfit</td><td>▅▆▆▇▆▂▄▇▄▅▅▅▅▃▆▄▄▅▅▆▆▄▆▅▂▆▇▅▆▄▂█▅█▁▁▅▆▄▃</td></tr><tr><td>bf_avail</td><td>█▇▆▅▄▂▃▅▄▃▄▄▄▃▃▂▄▃▃▂▂▂▃▂▁▃▃▃▂▁▂▄▂▃▁▂▁▃▂▂</td></tr><tr><td>bfnotavail</td><td>▆▇▃▅▄▅▃▇▄▄▆▇▅▇█▇▄▁▅▅▃▅▃▄▄▄▂▃▄▆▆▃▁▄▅▁▂▅▃▇</td></tr><tr><td>bfprofit</td><td>▅▆▅▇▅▃▃█▄▅▆▆▅▄▆▅▄▅▆▆▅▄▆▅▂▆▇▅▆▅▂█▅█▂▁▄▇▅▄</td></tr><tr><td>correct_conf</td><td>▁▂▄▄▅▅▅▅▅▆▆▅▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇█▇▇▇▇█▇▇▇████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>incorrect_conf</td><td>▁▃▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▆▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▇██▇█</td></tr><tr><td>loss</td><td>▅▄█▇▆▅▅▄▇▃▄▆▄▅▇▅▇▄▃▃▄▃▅▂▁▂▂▃▄▅▄▃▁▃▃▂▃▂▄▃</td></tr><tr><td>num_bets_per</td><td>█▆▅▅▄▄▄▄▄▃▃▄▃▃▃▃▂▂▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁</td></tr><tr><td>profit</td><td>▄▆▄▆▄▁▂█▃▃▅▅▄▂▆▃▄▃▅▄▄▂▅▃▂▅▅▄▄▄▄▇▄▇▂▂▂▆▃▂</td></tr><tr><td>test_accuracy</td><td>█▇▅▅▄▂▃▅▄▃▄▄▄▃▄▃▄▃▃▂▂▃▃▂▁▃▃▃▂▂▂▄▂▄▂▂▁▄▂▃</td></tr><tr><td>test_val</td><td>▂▁▂▂▄██▄▆▄▅▄▂▅▂▅▃▄▄▅▅▅▄▇█▄▅▄▆▇▆▄▇▄▇▆▇▄▇▆</td></tr><tr><td>value_pick_correct</td><td>▄▆█▄▇▁▅▅▄▄▃▂▅▅▁▄█▅█▇▇▄▆▆▅▆▅▅▅▇▆▆▆▄▆▆▃█▅█</td></tr><tr><td>value_pick_roi</td><td>▄▅▅▅▇▃▃▄▃▄▃▄▅▄▁▃█▄▆▇▇▅▆▇▇▆▅▆▆▇▅▄▆█▅▆▄▅▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>281413.14819</td></tr><tr><td>alt_bet_2</td><td>4373.60495</td></tr><tr><td>bfOnlyBets</td><td>6085</td></tr><tr><td>bfOnlyProfit</td><td>771.93539</td></tr><tr><td>bf_avail</td><td>1471</td></tr><tr><td>bfnotavail</td><td>278</td></tr><tr><td>correct_conf</td><td>0.57909</td></tr><tr><td>epoch</td><td>4999</td></tr><tr><td>incorrect_conf</td><td>0.5399</td></tr><tr><td>num_bets_per</td><td>2.36998</td></tr><tr><td>profit</td><td>-470.54</td></tr><tr><td>test_val</td><td>4614</td></tr><tr><td>value_pick_correct</td><td>1979</td></tr><tr><td>value_pick_roi</td><td>-0.18331</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">wise-dust-23</strong>: <a href=\"https://wandb.ai/nickojelly/new%20customs%2071.7K/runs/6lrmaoq8\" target=\"_blank\">https://wandb.ai/nickojelly/new%20customs%2071.7K/runs/6lrmaoq8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220930_203508-6lrmaoq8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "wandb_config_static = {'batch_size': 64, 'dropout': 0.3, 'epochs': 5000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'learning_rate': 0.000075, 'loss': 'L1', 'l1_beta':0.1, \"len_data\": len_dataset, 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
    "model = model_pipeline(config=wandb_config_static)\n",
    "# wandb.init()\n",
    "# model = model_pipeline(config=wandb_config_static, prev_model=r'C:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\good models\\vocal-sweep-80_90.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 15fs83lq\n",
      "Sweep URL: https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c4vme6xl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf2_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_beta: 0.2417975253099406\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005910150927922034\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlen_data: 71768\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: L1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\wandb\\run-20221001_031108-c4vme6xl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/c4vme6xl\" target=\"_blank\">sleek-sweep-1</a></strong> to <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 8, 'dropout': 0.4, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.2417975253099406, 'learning_rate': 0.0005910150927922034, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "1000\n",
      "{'batch_size': 8, 'dropout': 0.4, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.2417975253099406, 'learning_rate': 0.0005910150927922034, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "adamW\n",
      "HERE\n",
      "here\n",
      "Sequential(\n",
      "  (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.4, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.4, inplace=False)\n",
      "  (6): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (7): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [6:28:00<00:00, 23.28s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0667445a2a4759b97f8f07f21b5bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>▃▃▂▂▁▂▁▂▃▂▂█▁▅▃▂▂▄▂▄▁▂▃▃▂▂▁▂▃▂▁▃▂▁▂▂▃▃▂▂</td></tr><tr><td>alt_bet_2</td><td>▆▅▂▅▁▄▃▄▇▄▄█▃▆▅▄▄▇▂▄▄▄▅▄▄▃▂▄▆▅▁▄▄▄▅▄▇▆▆▄</td></tr><tr><td>bfOnlyBets</td><td>▁▆▇▇▇▇▇▇█▇█▇▇▇▇▇██▅▇▆█▆▆▆▇▆▆▆▇▇▆▆▅▆▇▆▆█▆</td></tr><tr><td>bfOnlyProfit</td><td>█▅▃▅▁▄▂▃▇▄▄█▃▆▅▄▄▆▂▃▃▄▄▄▅▃▁▄▆▅▁▄▅▃▄▃▆▅▅▄</td></tr><tr><td>bf_avail</td><td>▁▆██▇▇▇▇▇█▇▇▇▆▆▇█▇▆▆▇▇▇▇▇▇▇▇▆▇▇▇█▇▇▇▇▇▆▇</td></tr><tr><td>bfnotavail</td><td>▁▅▇▇▇▇▇▇▇▇▅▇▇▅▄▇▇▆▇▇▇▆▇▆▇▇▇▆▇▆▆▇▆██▇▇█▆▇</td></tr><tr><td>bfprofit</td><td>▆▅▂▅▁▄▃▄▇▄▄█▃▆▅▄▄▇▂▄▄▄▅▄▄▃▂▄▆▅▁▄▄▄▅▄▇▆▆▄</td></tr><tr><td>correct_conf</td><td>▁▆▅▆▅▅▇▆▆▆▇▆▆██▇▆▇▆▇▇▆▇▆▆▇▇▇▇▇▆▆▆▇▇▆█▆▇▇</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>incorrect_conf</td><td>▁▆▅▆▆▆▇▆▆▆▇▇▆██▇▆█▇▇▇▇▇▇▇▇▇▇▇█▇▆▇▇▇▆█▇▇▇</td></tr><tr><td>loss</td><td>▅▃▂▅▅▆▃▆▆▅▄▄▄▆▄▃█▃▃▄▇▂▆▃▅▁▄▃▄▅▄▃▃▄▃▃▂▄▂▆</td></tr><tr><td>num_bets_per</td><td>█▅▆▅▅▅▃▄▄▄▂▃▄▂▁▃▄▂▂▃▂▃▃▃▃▂▂▂▃▁▃▃▃▂▃▄▁▃▃▂</td></tr><tr><td>profit</td><td>▃▅▂▆▂▄▃▄█▄▄▅▂▅▅▃▄▇▂▄▄▄▄▄▅▄▂▄▅▆▁▅▅▄▆▄█▅▅▄</td></tr><tr><td>test_accuracy</td><td>▁▆██▇▇▇▇▇█▇▇▇▆▅▇█▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇</td></tr><tr><td>test_val</td><td>█▄▁▁▂▂▃▂▂▁▃▃▂▄▄▂▂▃▃▃▃▃▂▂▁▂▂▂▃▂▃▂▁▂▂▃▂▂▃▂</td></tr><tr><td>value_pick_correct</td><td>▁▇▅▆▆▆▇▇▅▆▇▆▆▇▇▇▆▇▆▇▆▆▇▆▆▆▇▆▇█▇▇▆▆█▇█▆▇▇</td></tr><tr><td>value_pick_roi</td><td>▁▅▃▃▃▄▄▄▁▅▆▆▄▅▅▅▄▇▄▆▃▃▅▄▆▅█▅▆▇▅█▄▄▇▅▆▆▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>-13971.84144</td></tr><tr><td>alt_bet_2</td><td>280.88083</td></tr><tr><td>bfOnlyBets</td><td>6094</td></tr><tr><td>bfOnlyProfit</td><td>79.57457</td></tr><tr><td>bf_avail</td><td>1562</td></tr><tr><td>bfnotavail</td><td>280</td></tr><tr><td>correct_conf</td><td>0.2858</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>incorrect_conf</td><td>0.26409</td></tr><tr><td>num_bets_per</td><td>3.92726</td></tr><tr><td>profit</td><td>-871.08</td></tr><tr><td>test_val</td><td>4532</td></tr><tr><td>value_pick_correct</td><td>1614</td></tr><tr><td>value_pick_roi</td><td>-0.27031</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">sleek-sweep-1</strong>: <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/c4vme6xl\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/c4vme6xl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221001_031108-c4vme6xl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y3fhiuqp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf2_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_beta: 0.4158672897091345\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0006116159359712829\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlen_data: 71768\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: L1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\wandb\\run-20221001_093921-y3fhiuqp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/y3fhiuqp\" target=\"_blank\">fresh-sweep-2</a></strong> to <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 8, 'dropout': 0.4, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.4158672897091345, 'learning_rate': 0.0006116159359712829, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "1000\n",
      "{'batch_size': 8, 'dropout': 0.4, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.4158672897091345, 'learning_rate': 0.0006116159359712829, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "adamW\n",
      "HERE\n",
      "here\n",
      "Sequential(\n",
      "  (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.4, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.4, inplace=False)\n",
      "  (6): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (7): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:32<8:53:33, 32.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [6:29:29<00:00, 23.37s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc05c53e5c345df878da3a9002db2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>▁▃▄▅▃▄▄▃▃█▇▄▅▄▄▆▃▄▄▃▄▅█▄▃█▄▃▅█▄▄█▄▅▃▄▄▄▄</td></tr><tr><td>alt_bet_2</td><td>▁▄▄▄▃▄▆▃▂▇▆▆▆▆▄▆▂▃▆▂▃▅▆▄▂█▆▃▅▇▅▄█▇▅▃▄▅▄▄</td></tr><tr><td>bfOnlyBets</td><td>▁█▆▇▅█▆▆▆▅▅▆▅▆▅▆▆▄▄▅▅▅▄▅▅▇▇▆▆▃▆▆▅▆▄▅▇▆▅▆</td></tr><tr><td>bfOnlyProfit</td><td>▂▃▃▃▃▃▅▃▁▇▅▅▆▅▄▆▁▃▆▂▃▅▆▃▂█▆▄▄▆▄▂█▆▅▃▃▅▄▄</td></tr><tr><td>bf_avail</td><td>▁▇▇▇▆▇█▇▇▇▇▇▇█▇▇▇▇▇▇▇▆▇▇▇██▇▆▇█▇▆██▇▆▇▇▇</td></tr><tr><td>bfnotavail</td><td>▁▆▆▇▆▆▇▆▇▆▆▇▅▇▆▆▇▆▇▆▆▄▆▇▇▇▇▆▆▇▆▇▅██▆▆▆▇▆</td></tr><tr><td>bfprofit</td><td>▁▄▄▄▃▄▆▃▂▇▆▆▆▆▄▆▂▃▆▂▃▅▆▄▂█▆▃▅▇▅▄█▇▅▃▄▅▄▄</td></tr><tr><td>correct_conf</td><td>▁▅▅▆▇▆▆▆▆▆▅▆▆▆▇▆▆▇▆▆▆▇▇▆▆▆▆▆▇▆▆▆█▆▆▆▆▇▆▆</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>incorrect_conf</td><td>▁▅▆▆▇▆▆▆▆▆▆▇▆▆▇▇▆▇▆▆▆▇▇▆▆▆▆▇▇▆▆▆█▆▆▇▇▇▇▆</td></tr><tr><td>loss</td><td>▆▅▅▆▅▆▄▃▂▂▆▅▃▂▄▅▃█▅▆▄▅▄▇▅▃▃▄█▄▄▄▆▄▂▄▄▆▆▁</td></tr><tr><td>num_bets_per</td><td>█▆▅▅▃▅▄▄▅▅▄▃▅▅▃▄▄▃▄▄▅▂▃▅▄▄▄▄▃▄▄▄▁▄▅▄▄▄▄▄</td></tr><tr><td>profit</td><td>▁▅▅▄▃▅▇▄▃▆▅▇█▇▅▇▃▄▇▃▄▆▅▅▃▇▇▄▅▅▆▅▆█▇▄▅▇▄▅</td></tr><tr><td>test_accuracy</td><td>▁▇▇▇▆▇█▇▇▇▇▇▇█▇▇▇▇▇▇▇▆▇▇▇██▇▆▇▇▇▆██▇▆▇▇▇</td></tr><tr><td>test_val</td><td>█▂▂▃▃▂▁▂▂▂▂▂▁▁▂▂▃▂▁▂▂▃▂▂▂▁▁▂▃▂▂▂▃▁▁▂▃▂▂▂</td></tr><tr><td>value_pick_correct</td><td>▁▄▆▆█▅▅▅▇▆▆▇▇▆▇▆▇▇▇▅▆▇█▇▇▇▇▆█▇▇▇█▇▇▅▆▆▆▇</td></tr><tr><td>value_pick_roi</td><td>▁▂▄▄▅▃▃▄▄▄▅▇▄▃▃▅▆▅▇▃▅▆▇▇▇▆▆▅▅▆█▆▇▆▅▃▃▄▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>-20556.03157</td></tr><tr><td>alt_bet_2</td><td>136.28758</td></tr><tr><td>bfOnlyBets</td><td>6109</td></tr><tr><td>bfOnlyProfit</td><td>134.95512</td></tr><tr><td>bf_avail</td><td>1579</td></tr><tr><td>bfnotavail</td><td>267</td></tr><tr><td>correct_conf</td><td>0.28379</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>incorrect_conf</td><td>0.26956</td></tr><tr><td>num_bets_per</td><td>3.95373</td></tr><tr><td>profit</td><td>-873.54</td></tr><tr><td>test_val</td><td>4530</td></tr><tr><td>value_pick_correct</td><td>1665</td></tr><tr><td>value_pick_roi</td><td>-0.26887</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fresh-sweep-2</strong>: <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/y3fhiuqp\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/y3fhiuqp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221001_093921-y3fhiuqp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8x93voyu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf2_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_beta: 0.681343499222867\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0008311765831103832\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlen_data: 71768\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: L1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\wandb\\run-20221001_160902-8x93voyu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/8x93voyu\" target=\"_blank\">exalted-sweep-3</a></strong> to <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 1000, 'dropout': 0.5, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.681343499222867, 'learning_rate': 0.0008311765831103832, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "1000\n",
      "{'batch_size': 1000, 'dropout': 0.5, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.681343499222867, 'learning_rate': 0.0008311765831103832, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "adamW\n",
      "HERE\n",
      "here\n",
      "Sequential(\n",
      "  (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (7): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:11<3:08:31, 11.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [46:26<00:00,  2.79s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10741170c1a04918a861f7cda3ce3ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>▁▇▆▇▇▇▇▇█▇▇█▇█▇▇█▇▇▇█▆███▇▆▆▇▆█▇▆▆▆▇▆▇▆▆</td></tr><tr><td>alt_bet_2</td><td>▁█▆█▇█▇██▇▇▇▇▇▇▇█▆▇▇█▆▇█▇▇▅▆▆▆▆▇▆▆▆▇▆▆▆▆</td></tr><tr><td>bfOnlyBets</td><td>▁▇▇██▇█▇▇▇█▇█▇▇▇▆▇▇▆▇▇▆▇▇▇▇▆▇▆▇▇▆▆▇▇▇▇▆▆</td></tr><tr><td>bfOnlyProfit</td><td>▁█▆█▇█▆██▇▇▇▇▇▇▆█▅▇▇▇▆▆█▇▇▅▆▆▆▆▇▆▆▅▇▆▆▆▆</td></tr><tr><td>bf_avail</td><td>▁█████▇██▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▆▇▇▆▆▇▇▇▇▆▆▆▆▇</td></tr><tr><td>bfnotavail</td><td>▁▆▆▇▇▇▇▇▆▇▆▆▆▆▆▆▇▇▇▆▇▆█▇▇▇▆▇▇▇▆▆▆▇▆▆▆▆▆▆</td></tr><tr><td>bfprofit</td><td>▁█▆█▇█▇██▇▇▇▇▇▇▇█▆▇▇█▆▇█▇▇▅▆▆▆▆▇▆▆▆▇▆▆▆▆</td></tr><tr><td>correct_conf</td><td>▁▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇███████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>incorrect_conf</td><td>▁▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇███████████</td></tr><tr><td>loss</td><td>█▆▆▆▄▆▇▆▅▅▆▆▇▄▃▄▅▃▄▄▆▅▃▄▃▂▃▂▁▃▃▂▁▃▂▃▂▁▁▃</td></tr><tr><td>num_bets_per</td><td>█▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>profit</td><td>▁█▆█▇█▇██▇▇▇▇▇▆▆▇▅▇▆▇▅▇▇▇▇▅▆▆▆▆▆▆▆▆▇▅▆▅▆</td></tr><tr><td>test_accuracy</td><td>▁█████▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▆▇▇▇</td></tr><tr><td>test_val</td><td>█▁▁▂▂▁▂▂▂▂▂▂▂▂▂▃▂▃▂▂▂▃▂▃▂▃▃▂▃▃▃▂▂▂▃▃▃▃▃▂</td></tr><tr><td>value_pick_correct</td><td>▁▇█▇██▇██▇▇██▇▇▇▇▇█▇█▇█▇██▇▇▇█▇█▇▇▇▇████</td></tr><tr><td>value_pick_roi</td><td>▁▇▇▆▇▇▅▇▇▇▇▆█▇▆▇▅▇▇▇█▆▇▆██▆▆▇▇▇▇▄▆▇▇▇█▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>200391.26116</td></tr><tr><td>alt_bet_2</td><td>2654.85281</td></tr><tr><td>bfOnlyBets</td><td>6070</td></tr><tr><td>bfOnlyProfit</td><td>604.57696</td></tr><tr><td>bf_avail</td><td>1462</td></tr><tr><td>bfnotavail</td><td>259</td></tr><tr><td>correct_conf</td><td>0.48842</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>incorrect_conf</td><td>0.45571</td></tr><tr><td>num_bets_per</td><td>2.8286</td></tr><tr><td>profit</td><td>-684.04</td></tr><tr><td>test_val</td><td>4608</td></tr><tr><td>value_pick_correct</td><td>1887</td></tr><tr><td>value_pick_roi</td><td>-0.21367</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">exalted-sweep-3</strong>: <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/8x93voyu\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/8x93voyu</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221001_160902-8x93voyu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jnk91ygw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf2_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_beta: 0.9299490493735648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0006162284316253955\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlen_data: 71768\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: L1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\wandb\\run-20221001_165542-jnk91ygw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/jnk91ygw\" target=\"_blank\">lemon-sweep-4</a></strong> to <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 1000, 'dropout': 0.4, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.9299490493735648, 'learning_rate': 0.0006162284316253955, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "1000\n",
      "{'batch_size': 1000, 'dropout': 0.4, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.9299490493735648, 'learning_rate': 0.0006162284316253955, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "adamW\n",
      "HERE\n",
      "here\n",
      "Sequential(\n",
      "  (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.4, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.4, inplace=False)\n",
      "  (6): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (7): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:10<2:55:41, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [46:23<00:00,  2.78s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b295a67a74144551bf212ff471e71cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>▁▇▇▇▇▇▆▆▇▇█▆▇▆▆▇▆▆▆▆▆▅▇▆▆▇▆▆▆▆▆▆▆▆▇▆▅▆▇▆</td></tr><tr><td>alt_bet_2</td><td>▁▆▇▇▇▇▇▆▆▇█▆▇▆▆▆▆▅▅▆▅▄▆▅▆▆▅▅▅▆▆▅▆▆▆▆▄▆▇▅</td></tr><tr><td>bfOnlyBets</td><td>▁█▇▇▇▇▆▆▅▆▅▆▆▅▆▇▆█▆▅▆▆█▆▆▇▆▆▇▇▇▆▇▇▇▆▇▇▅▇</td></tr><tr><td>bfOnlyProfit</td><td>▁▇▇▇▇▇▇▆▆▇█▆▇▆▆▆▆▅▆▇▆▄▆▅▆▆▅▆▅▇▆▆▆▆▆▆▅▆█▅</td></tr><tr><td>bf_avail</td><td>▁██████▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▆▇▇▇</td></tr><tr><td>bfnotavail</td><td>▁▇█▆▇▇██▇▇▇█▇▇▇▇▇▇▇▆▆▇▇▇▇█▇▆▆▆▆▇▆▇▇▇▆█▆▆</td></tr><tr><td>bfprofit</td><td>▁▆▇▇▇▇▇▆▆▇█▆▇▆▆▆▆▅▅▆▅▄▆▅▆▆▅▅▅▆▆▅▆▆▆▆▄▆▇▅</td></tr><tr><td>correct_conf</td><td>▁▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇██████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>incorrect_conf</td><td>▁▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>loss</td><td>▇█▆▇▇▅▆▄▄▆▆▃▅▄▄▅▄▆▄▃▃▄▃▄▃▂▃▂▃▂▃▃▃▃▃▃▁▁▁▃</td></tr><tr><td>num_bets_per</td><td>█▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>profit</td><td>▁▇▇▇▇▇▇▆▆▇█▆▇▅▆▆▅▄▅▆▄▃▆▄▅▆▄▅▄▅▆▅▅▅▆▅▄▆▆▄</td></tr><tr><td>test_accuracy</td><td>▁██████▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▆▇▇▆</td></tr><tr><td>test_val</td><td>█▂▁▁▂▂▁▂▂▁▁▂▂▂▂▂▂▃▂▂▂▂▃▂▃▃▃▃▃▂▃▃▃▃▃▃▃▃▂▃</td></tr><tr><td>value_pick_correct</td><td>▁▇▇▇▇▇██▇█▇▇▇▇▇▇▇▇▇▇▇▇██▇▇██▇█▇▇▇█▇▇▇█▇▇</td></tr><tr><td>value_pick_roi</td><td>▁▆▆▆▇▆▇▇▇█▆▆▆▆▆▇▆▆▅▆▆▆▇▇▇▇▇▆▇▇▆▆▆▇▆▇▇█▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>223948.99089</td></tr><tr><td>alt_bet_2</td><td>3035.75225</td></tr><tr><td>bfOnlyBets</td><td>6118</td></tr><tr><td>bfOnlyProfit</td><td>723.81285</td></tr><tr><td>bf_avail</td><td>1456</td></tr><tr><td>bfnotavail</td><td>249</td></tr><tr><td>correct_conf</td><td>0.47648</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>incorrect_conf</td><td>0.45044</td></tr><tr><td>num_bets_per</td><td>2.87793</td></tr><tr><td>profit</td><td>-704.18</td></tr><tr><td>test_val</td><td>4662</td></tr><tr><td>value_pick_correct</td><td>1882</td></tr><tr><td>value_pick_roi</td><td>-0.21264</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lemon-sweep-4</strong>: <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/jnk91ygw\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/jnk91ygw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221001_165542-jnk91ygw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p51xj6wc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf2_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_beta: 0.055206038485219655\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0004473689865945445\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlen_data: 71768\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: L1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\wandb\\run-20221001_174218-p51xj6wc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/p51xj6wc\" target=\"_blank\">bumbling-sweep-5</a></strong> to <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 8, 'dropout': 0.5, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.055206038485219655, 'learning_rate': 0.0004473689865945445, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "1000\n",
      "{'batch_size': 8, 'dropout': 0.5, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.055206038485219655, 'learning_rate': 0.0004473689865945445, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "adamW\n",
      "HERE\n",
      "here\n",
      "Sequential(\n",
      "  (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (7): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:32<8:55:13, 32.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [6:29:12<00:00, 23.35s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48284181a73a4a4fa3320e5d3ea09c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>▄▃▄▆▄▄▄▁▃▃▃▃▃▂▃▃▂▄▄▃▃▄▁▃▂▂▄▃█▂▃▃▃▄▂▄▃▃▃▄</td></tr><tr><td>alt_bet_2</td><td>▇▃▆█▆▆▅▂▅▄▃▆▆▅▅▆▃▆▇▅▄█▁▅▄▄█▅▇▃▅▄▆▅▄▅▄▄▇▆</td></tr><tr><td>bfOnlyBets</td><td>▁█▅█▆▇▇▅▆▆▆▆▆▄▇▆▅▇█▆▅▆▆▆▅▅▄▅▆▅▆▅▅▅▆▅▆█▅█</td></tr><tr><td>bfOnlyProfit</td><td>█▂▆█▅▅▅▂▅▅▂▆▆▅▅▆▂▅▆▅▄▇▁▄▃▄▆▃▇▂▄▄▆▄▄▅▃▃▅▅</td></tr><tr><td>bf_avail</td><td>▁▇██▇██▆▇█▆███▇▇▇█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▇█</td></tr><tr><td>bfnotavail</td><td>▁▆▅▆▆▇▆▅▅▅▅▆▅▆▆▆▇▆▅▆▆▆▅▆▆▆█▇▆▇▆▆▅▆▆▆▆▅▇▆</td></tr><tr><td>bfprofit</td><td>▇▃▆█▆▆▅▂▅▄▃▆▆▅▅▆▃▆▇▅▄█▁▅▄▄█▅▇▃▅▄▆▅▄▅▄▄▇▆</td></tr><tr><td>correct_conf</td><td>▁▆▇▇▇▇▇█▇▇█▇▇▇█▇▇▇▇▇▇█▇▇▇█▇▇▇▇█▇█▇▇███▇▇</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>incorrect_conf</td><td>▁▆▇▇▇▇▇█▇▇▇▇▇▇█▇▇▇▇▇▇█▇▇██▇▇▇▇█▇██▇█████</td></tr><tr><td>loss</td><td>▆▇▄▇▅▃▅█▇▃▅▂▂▅▆▄▅▆▅▂▄▃▃▂▂▇▅▅▃▆▇▆▅▁▅▆▂▅▄▂</td></tr><tr><td>num_bets_per</td><td>█▄▂▃▂▂▂▁▂▂▂▂▃▂▁▂▂▂▂▂▂▁▂▂▂▁▂▂▂▂▁▂▁▂▂▁▁▁▂▁</td></tr><tr><td>profit</td><td>▄▃▇▇▆▆▆▂▅▅▃▇▆▅▅▇▂▇▇▆▄█▁▅▄▅█▅▅▃▄▄▇▅▄▅▄▄▇▆</td></tr><tr><td>test_accuracy</td><td>▁▇█████▆▇█▇███▇█▇█▇█▇▇▇▇▇▇█▇▇▇▇█▇▇▇▇▇▆▇█</td></tr><tr><td>test_val</td><td>█▂▁▁▂▂▁▃▂▁▃▁▁▁▂▂▂▁▂▁▂▂▂▂▂▂▁▂▂▂▂▁▂▂▂▂▃▃▂▂</td></tr><tr><td>value_pick_correct</td><td>▁▆▆▇▆▆▇▆▇▇▆▆▆▇▇▇▆▇▆▆▇█▇▆▇▇▇▆▆▆▇▇▇▇▇▆▇▇▇▇</td></tr><tr><td>value_pick_roi</td><td>▁▃▆▅▄▄▆▄▆▆▄▅▄▅▅▆▅▇▄▅▆█▇▄▇▅▅▅▅▄▇▆▅▆▆▇▅▇▇▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>35337.05777</td></tr><tr><td>alt_bet_2</td><td>1226.95069</td></tr><tr><td>bfOnlyBets</td><td>6126</td></tr><tr><td>bfOnlyProfit</td><td>243.01574</td></tr><tr><td>bf_avail</td><td>1575</td></tr><tr><td>bfnotavail</td><td>271</td></tr><tr><td>correct_conf</td><td>0.47097</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>incorrect_conf</td><td>0.44179</td></tr><tr><td>num_bets_per</td><td>2.98885</td></tr><tr><td>profit</td><td>-703.24</td></tr><tr><td>test_val</td><td>4551</td></tr><tr><td>value_pick_correct</td><td>1907</td></tr><tr><td>value_pick_roi</td><td>-0.22838</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">bumbling-sweep-5</strong>: <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/p51xj6wc\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/p51xj6wc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221001_174218-p51xj6wc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y06a0y7u with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf2_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_beta: 0.7582108932323924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.000946029214329087\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlen_data: 71768\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: L1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\wandb\\run-20221002_001142-y06a0y7u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/y06a0y7u\" target=\"_blank\">expert-sweep-6</a></strong> to <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 1000, 'dropout': 0.4, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.7582108932323924, 'learning_rate': 0.000946029214329087, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "1000\n",
      "{'batch_size': 1000, 'dropout': 0.4, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.7582108932323924, 'learning_rate': 0.000946029214329087, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "adamW\n",
      "HERE\n",
      "here\n",
      "Sequential(\n",
      "  (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.4, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.4, inplace=False)\n",
      "  (6): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (7): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:10<2:54:20, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [46:15<00:00,  2.78s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423c19d6ce0344a58c928b067bf4f748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>▁▆▆▆▇▇▆▆▇▆▇▇▆▅▆▇▆▆▇▇▆▅█▇▇▇▇▇█▆▆▆▇▇▆▇█▇▇▆</td></tr><tr><td>alt_bet_2</td><td>▁▆▆▆▆▆▆▇▇▆▇▇▆▄▆▇▆▆▆▆▆▅█▇▆▇▇▇▇▇▆▆▇▇▇▇█▇▆▆</td></tr><tr><td>bfOnlyBets</td><td>▁█▇▆▆▆▆▇▇▆▆▇▅▇▆▆▆▇▆▇▆▇▇▆▆▆▆▆▅▆▅▄▆▅▆▆▆▆▅▆</td></tr><tr><td>bfOnlyProfit</td><td>▁▆▇▆▆▆▆▇█▆▇▇▅▄▆▆▇▆▇▆▆▅█▇▆▇▇▇▇▇▆▆▇▇▇▇█▇▆▅</td></tr><tr><td>bf_avail</td><td>▁███▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇</td></tr><tr><td>bfnotavail</td><td>▁▆█▇▇█▇█▇▆▇▆▇▆▇▇▆▇▇▆▆▆▆▆▆▇▆▆▇▇▇▇▆▇█▆▇▆▆▆</td></tr><tr><td>bfprofit</td><td>▁▆▆▆▆▆▆▇▇▆▇▇▆▄▆▇▆▆▆▆▆▅█▇▆▇▇▇▇▇▆▆▇▇▇▇█▇▆▆</td></tr><tr><td>correct_conf</td><td>▁▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇█▇████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>incorrect_conf</td><td>▁▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█▇███████████████</td></tr><tr><td>loss</td><td>█▇▇▇▅▄▅▅▄▅▅▄▄▄▅▄▃▃▃▃▃▄▄▃▄▃▄▃▂▃▃▃▃▂▃▃▁▃▂▂</td></tr><tr><td>num_bets_per</td><td>█▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>profit</td><td>▁▆▇▆▅▇▆▇█▅▇▆▅▃▆▆▆▆▆▆▆▄▇▇▅▇▆▆▇▇▆▅▆▆▇▆█▇▆▅</td></tr><tr><td>test_accuracy</td><td>▁███▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>test_val</td><td>█▁▁▁▂▂▁▂▁▂▂▂▂▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>value_pick_correct</td><td>▁▇▇█▇▇▇▇█▇▇▇▇▇▇▇▇█▇▇▇█▇████▇███▇███▇▇██▇</td></tr><tr><td>value_pick_roi</td><td>▁▆▆▆▆▆▇▆▇▆▆▆▆▆▆▅▆█▆▇▆█▆▇▇██▅▇▇▇▆▇█▇▇▆█▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>230437.31505</td></tr><tr><td>alt_bet_2</td><td>3700.48646</td></tr><tr><td>bfOnlyBets</td><td>6106</td></tr><tr><td>bfOnlyProfit</td><td>747.68769</td></tr><tr><td>bf_avail</td><td>1463</td></tr><tr><td>bfnotavail</td><td>259</td></tr><tr><td>correct_conf</td><td>0.48315</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>incorrect_conf</td><td>0.45288</td></tr><tr><td>num_bets_per</td><td>2.86427</td></tr><tr><td>profit</td><td>-675.94001</td></tr><tr><td>test_val</td><td>4643</td></tr><tr><td>value_pick_correct</td><td>1883</td></tr><tr><td>value_pick_roi</td><td>-0.20916</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">expert-sweep-6</strong>: <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/y06a0y7u\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/y06a0y7u</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221002_001142-y06a0y7u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bx53e7z8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf2_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_beta: 0.8925105867564896\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003601039722296959\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlen_data: 71768\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: L1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\wandb\\run-20221002_005816-bx53e7z8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/bx53e7z8\" target=\"_blank\">decent-sweep-7</a></strong> to <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 1000, 'dropout': 0.5, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.8925105867564896, 'learning_rate': 0.0003601039722296959, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "1000\n",
      "{'batch_size': 1000, 'dropout': 0.5, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.8925105867564896, 'learning_rate': 0.0003601039722296959, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "adamW\n",
      "HERE\n",
      "here\n",
      "Sequential(\n",
      "  (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (7): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:11<3:06:00, 11.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [46:23<00:00,  2.78s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fa29dd9acc479b806498c5e7738e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>▁█▆▃███▅▅▆▆▄▄▅▇▄▆▅▇██▃▇▅▅█▆▅▃▇▂▅▇▆▄▇▅▅▄▄</td></tr><tr><td>alt_bet_2</td><td>▁██▅█▇▇▆▇▇▅▅▅▆▆▆▇▆▇██▃▆▆▆▇▆▆▄▇▃▆▅▇▅▆▄▅▅▅</td></tr><tr><td>bfOnlyBets</td><td>▁▇█▇█▇▆▇▆▇▇▇▆▆▆▆▇▆▆▅▇▅▆▆▆▆▅▅▆▆▅▆▅▅▅▆▇▅▆▄</td></tr><tr><td>bfOnlyProfit</td><td>▁▇▆▅▇▇▆▅▇▆▅▄▄▆▅▅▅▆▆██▂▆▅▅▇▆▆▄▆▂▅▄▇▅▆▄▅▅▆</td></tr><tr><td>bf_avail</td><td>▁█████▇▇██▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▆▇▆▇▇▇▆▇▇▇</td></tr><tr><td>bfnotavail</td><td>▁▇█▆▇▆▇▇▆▇▅▆▆▇▇▇▇▅▇▇▅▇▆▆▆▆▆▇▇▆▆▆▆▅▆▅▆▆▆▅</td></tr><tr><td>bfprofit</td><td>▁██▅█▇▇▆▇▇▅▅▅▆▆▆▇▆▇██▃▆▆▆▇▆▆▄▇▃▆▅▇▅▆▄▅▅▅</td></tr><tr><td>correct_conf</td><td>▁▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>incorrect_conf</td><td>▁▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>loss</td><td>▇█▇█▇▄▇▅▆▇▆▅▆▅▅▆▅▄▆▆▄▅▅▄▅▃▅▅▅▄▄▃▄▆▄▃▃▂▁▄</td></tr><tr><td>num_bets_per</td><td>█▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>profit</td><td>▁██▆███▇██▆▆▆▇▆▆▇▇▇██▄▆▆▆█▇▇▅▇▄▆▅▇▆▆▅▆▆▆</td></tr><tr><td>test_accuracy</td><td>▁████▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▆▇▇▇▆▇▇▇</td></tr><tr><td>test_val</td><td>█▁▁▁▁▁▁▂▁▁▂▂▂▂▂▁▂▂▂▁▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▃▂▂▂</td></tr><tr><td>value_pick_correct</td><td>▁█▇█▇█▇▇▇▇███▇▇▇▇▇▇▇█▇▇▇▇█▇▇▇▇▇▇▇█▇▇▇▇▇█</td></tr><tr><td>value_pick_roi</td><td>▁▆▅▇▆▇▆▅▆▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▇▆▇▅▆▆▅▇▆▆▇▆▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>247283.91642</td></tr><tr><td>alt_bet_2</td><td>4197.34547</td></tr><tr><td>bfOnlyBets</td><td>6074</td></tr><tr><td>bfOnlyProfit</td><td>959.8563</td></tr><tr><td>bf_avail</td><td>1485</td></tr><tr><td>bfnotavail</td><td>256</td></tr><tr><td>correct_conf</td><td>0.46485</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>incorrect_conf</td><td>0.44151</td></tr><tr><td>num_bets_per</td><td>2.92795</td></tr><tr><td>profit</td><td>-480.28</td></tr><tr><td>test_val</td><td>4589</td></tr><tr><td>value_pick_correct</td><td>1925</td></tr><tr><td>value_pick_roi</td><td>-0.19083</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">decent-sweep-7</strong>: <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/bx53e7z8\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/bx53e7z8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221002_005816-bx53e7z8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qhbw6sgv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf2_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_beta: 0.311911303308922\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0004582367928559925\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlen_data: 71768\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: L1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\wandb\\run-20221002_014504-qhbw6sgv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/qhbw6sgv\" target=\"_blank\">celestial-sweep-8</a></strong> to <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 8, 'dropout': 0.4, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.311911303308922, 'learning_rate': 0.0004582367928559925, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "1000\n",
      "{'batch_size': 8, 'dropout': 0.4, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.311911303308922, 'learning_rate': 0.0004582367928559925, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "adamW\n",
      "HERE\n",
      "here\n",
      "Sequential(\n",
      "  (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.4, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.4, inplace=False)\n",
      "  (6): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (7): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:32<8:53:49, 32.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [6:28:28<00:00, 23.31s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21b94e5c5d9467798cad9cd49746142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>▁▁▂▆▃▆▁▂▃▃▂▂▁▁▃▁▂▂▁▂▂▂▂▂▂▄▂▃▁▃▁▂▂▂▂▇█▁▆▂</td></tr><tr><td>alt_bet_2</td><td>▃▁▄▃▅▄▂▃▄▃▄▂▁▁▅▁▃▂▂▂▂▃▂▃▃▅▃▄▂▄▁▃▂▂▃▅█▂▆▃</td></tr><tr><td>bfOnlyBets</td><td>▁▆█▄▆▅▆▄▅▆▅▆▆▄▆▄▆▅▅█▇█▅▄▆▆▅▆▇▆▆▄▅█▅▅▆▄▇▅</td></tr><tr><td>bfOnlyProfit</td><td>▄▁▄▃▅▅▂▄▄▃▅▃▂▃▅▂▃▃▂▃▄▃▂▄▃▅▃▄▃▅▂▄▂▃▄▆█▂▇▃</td></tr><tr><td>bf_avail</td><td>▁▇▇▆▇▇▇▇▇▆▆▆▇▇▇▇▇▇▇▇▇█▆▇▆▇▇▇▇▇▇▆▆▇▇▇█▆██</td></tr><tr><td>bfnotavail</td><td>▁█▇██▇▇▆▇▆▆▆▇▆▇▇█▇▇▆▆▆▇▇▇▇▇█▅▇▇█▇▆▆▇▇█▇▇</td></tr><tr><td>bfprofit</td><td>▃▁▄▃▅▄▂▃▄▃▄▂▁▁▅▁▃▂▂▂▂▃▂▃▃▅▃▄▂▄▁▃▂▂▃▅█▂▆▃</td></tr><tr><td>correct_conf</td><td>▁▆▆▆▆▆▆▇▇▇▇▇▆▇█▇▇▇▇▇▇▇█▇▇█▇▇█▇▇█▇▇▇█▇█▇▇</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>incorrect_conf</td><td>▁▅▆▆▆▆▆▇▇▇▇▇▇▇█▇▇▇▇▇▇▇████▇▇██▇█▇▇▇█▇██▇</td></tr><tr><td>loss</td><td>▆▆▄▄▄▆█▆▅▄▄▄▄▇▄▅▇▄▄▄▅▆▄▄▇▆▆▆█▄▄▁▆▆▅▄▅▅▅█</td></tr><tr><td>num_bets_per</td><td>█▆▅▄▄▄▄▃▃▂▃▃▃▂▂▃▃▃▃▂▃▃▁▂▂▁▂▃▁▂▃▁▂▂▃▁▃▁▂▂</td></tr><tr><td>profit</td><td>▄▁▅▁▆▂▃▄▆▃▄▃▂▁▆▂▄▃▃▃▃▄▂▄▅▇▄▄▃▆▂▅▃▃▅▄█▂▆▅</td></tr><tr><td>test_accuracy</td><td>▁▇▇▇█▇▇▇▇▆▆▆▇▇▇▇▇▇▇▇▇█▆▇▇▇▇█▇▇▇▇▆▇▇▇█▆██</td></tr><tr><td>test_val</td><td>█▂▂▂▁▂▂▁▂▃▂▃▂▂▂▂▂▂▂▂▂▂▃▁▃▂▂▂▂▂▂▂▃▂▂▂▁▃▁▁</td></tr><tr><td>value_pick_correct</td><td>▁▆▆█▅▅▅█▆▅▇▆▆▆█▅▅▇▇▇▆▇▆▆▅█▄▆██▆▇▇▆▇▇▇▇▇█</td></tr><tr><td>value_pick_roi</td><td>▁▃▁▆▂▃▂▅▃▃▃▂▃▄▃▂▃▄▃▄▃▆▃▃▁▆▂▆▇▆▄▅▅▃▅▆▅▅▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>-54980.23923</td></tr><tr><td>alt_bet_2</td><td>84.01698</td></tr><tr><td>bfOnlyBets</td><td>6083</td></tr><tr><td>bfOnlyProfit</td><td>64.8658</td></tr><tr><td>bf_avail</td><td>1576</td></tr><tr><td>bfnotavail</td><td>267</td></tr><tr><td>correct_conf</td><td>0.29911</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>incorrect_conf</td><td>0.28266</td></tr><tr><td>num_bets_per</td><td>3.85702</td></tr><tr><td>profit</td><td>-852.38</td></tr><tr><td>test_val</td><td>4507</td></tr><tr><td>value_pick_correct</td><td>1690</td></tr><tr><td>value_pick_roi</td><td>-0.25387</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">celestial-sweep-8</strong>: <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/qhbw6sgv\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/qhbw6sgv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221002_014504-qhbw6sgv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h6uj5mhl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf2_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_beta: 0.4395210402986449\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009049722556309372\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlen_data: 71768\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: L1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\wandb\\run-20221002_091345-h6uj5mhl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/h6uj5mhl\" target=\"_blank\">visionary-sweep-9</a></strong> to <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 1000, 'dropout': 0.3, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.4395210402986449, 'learning_rate': 0.0009049722556309372, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "1000\n",
      "{'batch_size': 1000, 'dropout': 0.3, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.4395210402986449, 'learning_rate': 0.0009049722556309372, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "adamW\n",
      "HERE\n",
      "here\n",
      "Sequential(\n",
      "  (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.3, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.3, inplace=False)\n",
      "  (6): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (7): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:10<2:53:25, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [46:19<00:00,  2.78s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448ce11a55d7415fb97b495db5046f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>▁▆▇▆▇▇▇▇█▇█▆█▆█▇▆▇▇▇▇█▇█▆▇▇▇▇▇▇▆▆▆█▇▇▇▇▇</td></tr><tr><td>alt_bet_2</td><td>▁▆▆▅▇▆▇▆▇▅▆▆█▅▇▅▄▆▆▆▇▇▅▇▄▆▆▆▆▆▆▄▆▆▇▆▇▆▆▅</td></tr><tr><td>bfOnlyBets</td><td>▁█▇█▇▇▇▆▇▇█▆▇▆▆█▆█▆▆▆▇▆▆▆▆▅▆▆▆▆▅▆▇▆▇▆▆▆▅</td></tr><tr><td>bfOnlyProfit</td><td>▁▆▆▅▇▆▇▇▆▅▆▆█▅▇▆▅▆▆▇▇▇▅▇▅▆▆▆▆▆▆▅▆▆▇▆▇▆▇▆</td></tr><tr><td>bf_avail</td><td>▁██▇███▇▇▇▇▇█▇▇▆▆▇▇▇▇▇▆▇▆▆▆▇▆▇▆▆▇▆▇▆▇▇▇▆</td></tr><tr><td>bfnotavail</td><td>▁▇▇▇██▇▇▆▇▇▇▆▅▆▅▆▅▅▇▇▆▇█▇▆▇▆▇▇▅▆▆▅▇▆▆▅▄▆</td></tr><tr><td>bfprofit</td><td>▁▆▆▅▇▆▇▆▇▅▆▆█▅▇▅▄▆▆▆▇▇▅▇▄▆▆▆▆▆▆▄▆▆▇▆▇▆▆▅</td></tr><tr><td>correct_conf</td><td>▁▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██▇█████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>incorrect_conf</td><td>▁▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██▇████████████</td></tr><tr><td>loss</td><td>▇▇▇██▇▆▄▅▆▇▆▅▅▅▅▄▅▄▄▅▄▃▄▄▃▃▅▄▄▃▂▄▃▃▃▁▁▂▃</td></tr><tr><td>num_bets_per</td><td>█▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>profit</td><td>▁▆▆▅▇▆▇▆▆▅▆▆█▄▇▄▃▆▅▅▆▆▄▇▄▅▅▅▅▆▅▃▅▅▇▅▆▅▅▄</td></tr><tr><td>test_accuracy</td><td>▁██▇███▇▇▇▇▇█▇▇▆▆▇▇▇▇▇▆▇▆▆▆▆▆▇▆▆▇▆▇▆▇▆▆▆</td></tr><tr><td>test_val</td><td>█▁▁▂▁▂▂▂▂▂▂▂▂▃▂▄▃▃▂▂▂▂▃▂▃▃▂▂▃▂▃▃▂▃▃▃▂▃▂▃</td></tr><tr><td>value_pick_correct</td><td>▁▇█▇▇▇███▇▇▇▇██▇▇██▇███▇▇██▇██▇▇▇██▇▇▇▇█</td></tr><tr><td>value_pick_roi</td><td>▁▅▆▅▅▅▆▆▇▆▅▅▆▆▆▆▆▇▆▆█▇▇▇▆▇▇▆▇▇▇▅▆▇▇▇▇▆█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>alt_bet_1</td><td>277162.67909</td></tr><tr><td>alt_bet_2</td><td>3417.70882</td></tr><tr><td>bfOnlyBets</td><td>6075</td></tr><tr><td>bfOnlyProfit</td><td>746.66017</td></tr><tr><td>bf_avail</td><td>1421</td></tr><tr><td>bfnotavail</td><td>257</td></tr><tr><td>correct_conf</td><td>0.48817</td></tr><tr><td>epoch</td><td>999</td></tr><tr><td>incorrect_conf</td><td>0.45206</td></tr><tr><td>num_bets_per</td><td>2.88545</td></tr><tr><td>profit</td><td>-674.34</td></tr><tr><td>test_val</td><td>4654</td></tr><tr><td>value_pick_correct</td><td>1909</td></tr><tr><td>value_pick_roi</td><td>-0.19921</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">visionary-sweep-9</strong>: <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/h6uj5mhl\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/h6uj5mhl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221002_091345-h6uj5mhl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4h25qj0x with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf1_layer_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tf2_layer_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl1_beta: 0.4788932838032196\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00031161009217158046\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlen_data: 71768\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: L1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalidation_split: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\wandb\\run-20221002_100015-4h25qj0x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/runs/4h25qj0x\" target=\"_blank\">spring-sweep-10</a></strong> to <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq\" target=\"_blank\">https://wandb.ai/nickojelly/grv_priced_sweep_custom_new/sweeps/15fs83lq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 8, 'dropout': 0.4, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.4788932838032196, 'learning_rate': 0.00031161009217158046, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "1000\n",
      "{'batch_size': 8, 'dropout': 0.4, 'epochs': 1000, 'f1_layer_size': 256, 'f2_layer_size': 64, 'l1_beta': 0.4788932838032196, 'learning_rate': 0.00031161009217158046, 'len_data': 71768, 'loss': 'L1', 'num_layers': 2, 'optimizer': 'adamW', 'validation_split': 0.1}\n",
      "adamW\n",
      "HERE\n",
      "here\n",
      "Sequential(\n",
      "  (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.4, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.4, inplace=False)\n",
      "  (6): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (7): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:31<8:47:28, 31.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 943/1000 [6:06:25<23:04, 24.29s/it]  "
     ]
    }
   ],
   "source": [
    "#model_pipeline(config)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"grv_priced_sweep_custom_new\")\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "wandb.agent(sweep_id, function=model_pipeline, count=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=120, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=128, out_features=8, bias=True)\n",
       "  (7): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43115443075634c02a7c247a87b0dd9d74842892e56d473b9e19f544f3149aff"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
