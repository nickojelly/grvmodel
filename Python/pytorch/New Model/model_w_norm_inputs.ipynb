{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import wandb\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "with torch.profiler.profile() as profiler:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnickojelly\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nick\\.conda\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluded =  0\n"
     ]
    }
   ],
   "source": [
    "REBUILD_DATA = True\n",
    "\n",
    "\n",
    "class GRV:\n",
    "    # class to store training data\n",
    "\n",
    "    # reading data from pickle\n",
    "    file = open(r\"DATA/total_list_w_price_bf.npy\", \"rb\")\n",
    "    data = pickle.load(file)\n",
    "    # seperate out classes from inputs\n",
    "    raceIDs, inputs, classes, prices, win_price, margins, betfairSP = zip(*data)\n",
    "    # removing nan from inputs and convert to float\n",
    "    inputs_df = pd.DataFrame(inputs)\n",
    "    inputs_df.fillna(value=-1, inplace=True)\n",
    "    inputs = inputs_df.values.tolist()\n",
    "    inputs = [[float(i) for i in j] for j in inputs]\n",
    "\n",
    "    # data\n",
    "    training_data = []\n",
    "\n",
    "    def make_training_data(self):\n",
    "        excluded = 0\n",
    "        for i in range(len(self.inputs)):\n",
    "            if len(self.classes[i]) == 8:\n",
    "                self.training_data.append(\n",
    "                    [\n",
    "                        np.array(self.inputs[i]),\n",
    "                        np.array(self.classes[i]),\n",
    "                        np.array(self.prices[i]),\n",
    "                        np.array(self.margins[i]),\n",
    "                        np.array(self.betfairSP[i]),\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                adjustedList = self.classes[i] + ([8] * (8 - len(self.classes[i])))\n",
    "                adjustedListP = self.prices[i] + ([0] * (8 - len(self.prices[i])))\n",
    "                adjustedListM = self.margins[i] + ([100] * (8 - len(self.margins[i])))\n",
    "                adjustedListSP = self.margins[i] + ([0] * (8 - len(self.betfairSP[i])))\n",
    "                self.training_data.append(\n",
    "                    [\n",
    "                        np.array(self.inputs[i]),\n",
    "                        np.array(adjustedList),\n",
    "                        np.array(adjustedListP),\n",
    "                        np.array(adjustedListM),\n",
    "                        np.array(adjustedListSP)\n",
    "                    ]\n",
    "                )\n",
    "                if len(adjustedList) != 8:\n",
    "                    print(adjustedList)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print(\"excluded = \", excluded)\n",
    "\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    grv = GRV()\n",
    "    grv.make_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_25112\\2076601069.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  Y_w = torch.tensor([i for i in Y_w])\n"
     ]
    }
   ],
   "source": [
    "softmin = nn.Softmin(dim=1)\n",
    "\n",
    "# dataset setup\n",
    "training_data = grv.training_data\n",
    "\n",
    "X = torch.Tensor(np.array([i[0] for i in training_data]))\n",
    "Y = torch.Tensor(np.array([i[1] for i in training_data]))\n",
    "P = torch.Tensor(np.array([i[2] for i in training_data]))\n",
    "Y_m = softmin(torch.Tensor(np.array([i[3] for i in training_data])))\n",
    "bfSP = torch.Tensor(np.array([i[4] for i in training_data]))\n",
    "\n",
    "# Generate winner only class\n",
    "Y_w = []\n",
    "for i in Y:\n",
    "    n = np.zeros(8)\n",
    "    index = torch.argmin(i)\n",
    "    n[index] = float(1)\n",
    "    Y_w.append(n)\n",
    "\n",
    "Y_w = torch.tensor([i for i in Y_w])\n",
    "Xn = nn.functional.normalize(X, dim = 1)\n",
    "X = Xn.to(device)\n",
    "Y_w = Y_w.to(device)\n",
    "\n",
    "\n",
    "Y_m = Y_m.to(device)\n",
    "P = P.to(device)\n",
    "bfSP = bfSP.to('cuda')\n",
    "bfSP = torch.nan_to_num(bfSP, nan=0)\n",
    "my_dataset = TensorDataset(X, Y_m, P, bfSP)\n",
    "my_dataloader = DataLoader(my_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([23.6000, 31.9000, 26.3000,  ..., -1.0000, 28.4000, 29.3000], device='cuda:0')\n",
      "tensor([ 0.0443,  0.0598,  0.0606,  ..., -0.0022,  0.0710,  0.0637])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X[:, 119])\n",
    "print(Xn[:, 119])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to pass in dataset to get_data (created above)\n",
    "def make_loader(dataset, config, train=True):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(config[\"validation_split\"] * dataset_size))\n",
    "    random_seed = 42\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    if train:\n",
    "        dataset_sampler = SubsetRandomSampler(train_indices)\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "            num_workers=0,\n",
    "            sampler=dataset_sampler\n",
    "        )\n",
    "    else:\n",
    "        dataset_sampler = SubsetRandomSampler(val_indices)\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "            num_workers=0,\n",
    "            sampler=dataset_sampler,\n",
    "        )\n",
    "\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(f1_layer_size, f2_layer_size, dropout, num_layers=2):\n",
    "\n",
    "    if num_layers == 2:\n",
    "        network = nn.Sequential(  # fully-connected, dual hidden layer\n",
    "            nn.Linear(120, f1_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(f1_layer_size, f2_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(f2_layer_size, 8),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        network = nn.Sequential(  # fully-connected, dual hidden layer\n",
    "            nn.Linear(120, f1_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(f1_layer_size, f2_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(f2_layer_size, f2_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(f2_layer_size, 8),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_MSE(output, target):\n",
    "    sorts = torch.argsort(target)\n",
    "    out = sorts.narrow(1,0,3)\n",
    "    ohe = torch.nn.functional.one_hot(out, num_classes=8).sum(1)\n",
    "    out_first3 = ohe*output\n",
    "    target_ohe = ohe*target\n",
    "    loss = torch.sum((((target_ohe+1)*abs(out_first3-target_ohe)+1)**2))\n",
    "    # loss = torch.sum(((out_first3-target_ohe))**2)\n",
    "    return loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,3,5,6]])\n",
    "y =torch.tensor([[6,2,3,4,5]]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_order_MSE(output, target):\n",
    "    sorts = torch.argsort(target)\n",
    "    print(sorts)\n",
    "    out = sorts.narrow(1,0,3)\n",
    "    ohe = torch.nn.functional.one_hot(out, num_classes=8).sum(1)\n",
    "    out_first3 = ohe*output\n",
    "    target_ohe = ohe*target\n",
    "    loss = torch.mean(((abs(out_first3-target_ohe)*10)**2))\n",
    "    # loss = torch.sum(((out_first3-target_ohe))**2)\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_customiser(x,y,loss_func=nn.HuberLoss(reduction='none',delta=0.1)):\n",
    "    sorts_t = torch.argsort(y)\n",
    "    out = sorts_t.narrow(1,0,3)\n",
    "    ohe = torch.nn.functional.one_hot(out, num_classes=8).sum(1)\n",
    "    loss = loss_func(x,y)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_l1(x=None,y=None,beta=0.1):\n",
    "    loss_func=nn.SmoothL1Loss(reduction='none',beta=beta)\n",
    "    sorts_t = torch.argsort(y)\n",
    "    sorts_o = torch.argsort(x)\n",
    "    diff_place = abs(sorts_t-sorts_o)\n",
    "    out = sorts_t.narrow(1,0,3)\n",
    "    ohe = torch.nn.functional.one_hot(out, num_classes=8).sum(1)\n",
    "    loss = loss_func(x,y)*ohe\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_l1_place(x=None,y=None,beta=0.1):\n",
    "    loss_func=nn.SmoothL1Loss(reduction='none',beta=beta)\n",
    "    sorts_t = torch.argsort(y)\n",
    "    sorts_o = torch.argsort(x)\n",
    "    diff_place = abs(sorts_t-sorts_o)\n",
    "    out = sorts_t.narrow(1,0,3)\n",
    "    ohe = torch.nn.functional.one_hot(out, num_classes=8).sum(1)\n",
    "    loss = loss_func(x,y)*diff_place\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config, dataset):\n",
    "    # Make the data\n",
    "\n",
    "    train_loader = make_loader(dataset, config, train=True)\n",
    "    test_loader = make_loader(dataset, config, train=False)\n",
    "    # Make the model\n",
    "    # model = Net().to(device)\n",
    "    model = build_network(\n",
    "        config[\"f1_layer_size\"], config[\"f2_layer_size\"], config[\"dropout\"], config[\"num_layers\"]\n",
    "    )\n",
    "\n",
    "    loss_functions = {\n",
    "        \"Huber\":nn.HuberLoss(),\n",
    "        \"Huber_custom\":loss_customiser,\n",
    "        \"l1_custom\":custom_l1_place,\n",
    "        \"MSE\":nn.MSELoss(),\n",
    "        \"L1\":nn.SmoothL1Loss(reduction='sum', beta=config[\"l1_beta\"]),\n",
    "        \"BCE\":nn.CrossEntropyLoss(),\n",
    "        \"Custom\":custom_MSE,\n",
    "        \"KL\":nn.KLDivLoss(reduction='batchmean'),\n",
    "        \"NLL\":nn.NLLLoss()\n",
    "    }\n",
    "    # Make the loss and optimizer\n",
    "    #  criterion = nn.NLLLoss()\n",
    "    loss_f = loss_functions[config['loss']]\n",
    "    criterion = loss_f\n",
    "    optimizer = config[\"optimizer\"]\n",
    "\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(), lr=config[\"learning_rate\"], momentum=0.9\n",
    "        )\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    return model, train_loader, test_loader, criterion, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, batch_ct):\n",
    "    model.eval()\n",
    "    classL, predL, maxL, correctL, priceP, priceR, bfPriceR, pred_odds, model_outputs = [], [], [], [], [], [], [], [], []\n",
    "    # Run the model on some test examples\n",
    "    with torch.no_grad():\n",
    "        correct, total, max_sum, max_w_sum, profit, bfprofit, bfnotavail = 0, 0, 0, 0, 0,0,0\n",
    "        value_pick_correct, value_pick_profit = 0, 0\n",
    "        num_bets = 0\n",
    "        for images, labels, prices, bfspPrices in test_loader:\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            #converts prices from tensor to list\n",
    "            prices = prices[0,].tolist()\n",
    "            bfspPrices = bfspPrices[0,].tolist()\n",
    "\n",
    "            #gets the prediction and its confidence and the real class\n",
    "            max, predicted = torch.max(outputs.data, 1)\n",
    "            _, real = torch.max(labels.data, 1)\n",
    "\n",
    "            #converts prediction from tensor to item\n",
    "            prediction = predicted.item()\n",
    "            real_item = real.item()\n",
    "\n",
    "            #appends prediction and likelyhood to lists\n",
    "            predL.append(prediction)\n",
    "            maxL.append(max.item())\n",
    "\n",
    "        \n",
    "            total += labels.size(0)\n",
    "            correct += prediction == real_item\n",
    "\n",
    "            correctL.append(int(prediction == real_item))\n",
    "            classL.append(real_item)\n",
    "\n",
    "            priceR.append(prices[real_item])\n",
    "            priceP.append(prices[prediction])\n",
    "            bfPriceR.append(bfspPrices[real_item])\n",
    "            # print(outputs.data.flatten().tolist())\n",
    "\n",
    "            predicted_odds = [\n",
    "                1 / ((x + 10**-7)) for x in outputs.data.flatten().tolist()\n",
    "            ]\n",
    "\n",
    "            pred_odds.append(predicted_odds)\n",
    "            model_outputs.append(outputs.data.flatten().tolist())\n",
    "\n",
    "            if prices[real_item] > (predicted_odds[real_item]):\n",
    "                value_pick_correct += 1\n",
    "                value_pick_profit += prices[real_item]\n",
    "\n",
    "            bets = [x > (y) for x, y in zip(prices, predicted_odds)]\n",
    "            num_bets += sum(bets)\n",
    "\n",
    "            value_pick_profit += -sum(bets)\n",
    "\n",
    "            if prediction == real_item:\n",
    "                max_sum += max\n",
    "                profit += prices[real_item]\n",
    "                if bfspPrices[real_item]:\n",
    "                    bfprofit += bfspPrices[real_item]\n",
    "                else:\n",
    "                    bfprofit += prices[real_item]\n",
    "                    bfnotavail += 1\n",
    "            else:\n",
    "                max_w_sum += max\n",
    "\n",
    "            profit += -1\n",
    "            bfprofit += -1\n",
    "\n",
    "            # print(f\"{correct=}\")\n",
    "\n",
    "        # print(f\"Accuracy of the model on the {total} \" +\n",
    "        #       f\"test images: {100 * correct / total}%\" +\n",
    "        #       f\"profit: {profit}\"+\n",
    "        #       f\"profit: {value_pick_profit}\")\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"test_accuracy\": correct / total,\n",
    "                \"correct_conf\": max_sum / correct,\n",
    "                \"incorrect_conf\": (max_w_sum) / (total - correct),\n",
    "                \"profit\": profit,\n",
    "                \"bfprofit\": bfprofit,\n",
    "                \"bfnotavail\": bfnotavail,\n",
    "                \"value_pick_roi\": value_pick_profit / num_bets,\n",
    "                \"value_pick_correct\": value_pick_correct,\n",
    "                \"num_bets_per\": num_bets / total\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # logdf = pd.DataFrame(\n",
    "        #     data={\n",
    "        #         \"class\": classL,\n",
    "        #         \"pred\": predL,\n",
    "        #         \"max\": maxL,\n",
    "        #         \"correct\": correctL,\n",
    "        #         \"priceR\": priceR,\n",
    "        #         \"priceP\": priceP,\n",
    "        #         \"bets\": sum(bets),\n",
    "        #         \"pred_odds\": pred_odds,\n",
    "        #         \"model_outputs\": model_outputs,\n",
    "        #         \"bfodds\" : bfPriceR\n",
    "        #     }\n",
    "        # )\n",
    "        # table = wandb.Table(dataframe=logdf)\n",
    "        # wandb.log({\"table_key\": table})\n",
    "        # classCounts = logdf[\"class\"].value_counts()\n",
    "        # predCounts = logdf[\"pred\"].value_counts()\n",
    "        # boxplot = logdf.boxplot(column=['priceR'],by='correct')\n",
    "        # print(classCounts, predCounts)\n",
    "        # boxplot\n",
    "        # plt.savefig(\"boxplot.png\")\n",
    "        # wandb.log({\"boxplot\":boxplot})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(loss, example_ct, epoch):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
    "    #print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_saver(model, optimizer, epoch, loss):\n",
    "    \n",
    "    pathtofolder = \"C:/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New Model\"\n",
    "    model_name = wandb.run.name\n",
    "    isExist = os.path.exists(\n",
    "        f\"C:/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New Model/savedmodel/{model_name}/\"\n",
    "    )\n",
    "    if isExist:\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": loss,\n",
    "            },\n",
    "            f\"C:/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New Model/savedmodel/{model_name}/{model_name}_{epoch}.pt\",\n",
    "        )\n",
    "    else:\n",
    "        print(\"created path\")\n",
    "        os.makedirs(\n",
    "            f\"C:/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New Model/savedmodel/{model_name}/\"\n",
    "        )\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": loss,\n",
    "            },\n",
    "            f\"C:/Users/Nick/Documents/GitHub/grvmodel/Python/pytorch/New Model/savedmodel/{model_name}/{model_name}_{epoch}.pt\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader,test_loader, criterion, optimizer, config):\n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Run training and track with wandb\n",
    "    total_batches = len(loader) * config.epochs\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "\n",
    "    raw_inputs = True\n",
    "    if config['loss'] == \"KL\":\n",
    "\n",
    "        for epoch in tqdm(range(config.epochs)):\n",
    "            for _, (images, labels, _ , _) in enumerate(loader):\n",
    "\n",
    "                loss = train_batch_lsftmax(images, labels, model, optimizer, criterion, btch_count=batch_ct, raw_inputs=True)\n",
    "                example_ct +=  len(images)\n",
    "                batch_ct += 1\n",
    "\n",
    "                # Report metrics every 25th batch\n",
    "                if ((batch_ct + 1) % 250) == 0:\n",
    "                    train_log(loss, example_ct, epoch)\n",
    "                    \n",
    "\n",
    "            if epoch %10 ==0:\n",
    "                test(model,test_loader, epoch)\n",
    "                model_saver(model,optimizer,epoch,loss)\n",
    "\n",
    "    else:\n",
    "        for epoch in tqdm(range(config.epochs)):\n",
    "            for _, (images, labels, _ , _) in enumerate(loader):\n",
    "\n",
    "                loss = train_batch(images, labels, model, optimizer, criterion, btch_count=batch_ct, raw_inputs=True)\n",
    "                example_ct +=  len(images)\n",
    "                batch_ct += 1\n",
    "\n",
    "                # Report metrics every 25th batch\n",
    "                if ((batch_ct + 1) % 250) == 0:\n",
    "                    train_log(loss, example_ct, epoch)\n",
    "\n",
    "            if epoch %10 ==0:\n",
    "                test(model,test_loader, epoch)\n",
    "                model_saver(model, optimizer, epoch, loss)\n",
    "\n",
    "def train_batch(images, labels, model, optimizer, criterion, btch_count=0, raw_inputs=True, beta=0.1):\n",
    "    images, labels = images, labels\n",
    "    \n",
    "\n",
    "    # Forward pass ➡\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels.float())\n",
    "    \n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_batch_lsftmax(images, labels, model, optimizer, criterion, btch_count=0, raw_inputs=True):\n",
    "    images, labels = images, labels\n",
    "    \n",
    "\n",
    "    # Forward pass ➡\n",
    "    outputs = model(images)\n",
    "    loss = criterion(F.log_softmax(outputs), labels.float())\n",
    "    \n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(config=None,prev_model=None):\n",
    "    dataset = my_dataset\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"new customs\", config=config):\n",
    "      # access all HPs through wandb.config, so logging matches execution!\n",
    "      wandb.define_metric(\"loss\", summary=\"min\")\n",
    "      wandb.define_metric(\"test_accuracy\", summary=\"max\")\n",
    "      wandb.define_metric(\"bfprofit\", summary=\"max\")\n",
    "      config = wandb.config\n",
    "      pprint.pprint(config)\n",
    "      pprint.pprint(config.epochs)\n",
    "      print(config)\n",
    "\n",
    "      # make the model, data, and optimization problem\n",
    "      model, train_loader, test_loader, criterion, optimizer = make(config, dataset)\n",
    "      if prev_model:\n",
    "        checkpoint = torch.load(prev_model, map_location=\"cuda:0\")\n",
    "        print(\"here\")\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "      model = model.to(device)\n",
    "      #optimizer = optimizer.to(device)\n",
    "      print(model)\n",
    "\n",
    "      # and use them to train the model\n",
    "      train(model, train_loader,test_loader, criterion, optimizer, config)\n",
    "\n",
    "      # and test its final performance\n",
    "      #test(model, test_loader)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'profit'},\n",
      " 'parameters': {'batch_size': {'values': [32, 64, 128, 360, 720]},\n",
      "                'dropout': {'values': [0.3, 0.4, 0.5]},\n",
      "                'epochs': {'values': [100]},\n",
      "                'f1_layer_size': {'values': [32, 64, 128, 256]},\n",
      "                'f2_layer_size': {'values': [32, 64, 128, 256]},\n",
      "                'l1_beta': {'distribution': 'uniform', 'max': 1, 'min': 0},\n",
      "                'learning_rate': {'distribution': 'uniform',\n",
      "                                  'max': 0.001,\n",
      "                                  'min': 0.0001},\n",
      "                'loss': {'value': 'l1_custom'},\n",
      "                'num_layers': {'values': [2]},\n",
      "                'optimizer': {'value': 'adam'},\n",
      "                'validation_split': {'value': 0.1}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'method': 'random',\n",
       " 'metric': {'name': 'profit', 'goal': 'maximize'},\n",
       " 'parameters': {'optimizer': {'value': 'adam'},\n",
       "  'f1_layer_size': {'values': [32, 64, 128, 256]},\n",
       "  'f2_layer_size': {'values': [32, 64, 128, 256]},\n",
       "  'dropout': {'values': [0.3, 0.4, 0.5]},\n",
       "  'epochs': {'values': [100]},\n",
       "  'validation_split': {'value': 0.1},\n",
       "  'loss': {'value': 'l1_custom'},\n",
       "  'num_layers': {'values': [2]},\n",
       "  'learning_rate': {'distribution': 'uniform', 'min': 0.0001, 'max': 0.001},\n",
       "  'l1_beta': {'distribution': 'uniform', 'min': 0, 'max': 1},\n",
       "  'batch_size': {'values': [32, 64, 128, 360, 720]}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sweep_config = {\"method\": \"random\"}\n",
    "\n",
    "metric = {\"name\": \"profit\", \"goal\": \"maximize\"}\n",
    "\n",
    "sweep_config[\"metric\"] = metric\n",
    "\n",
    "\n",
    "parameters_dict = {\n",
    "    \"optimizer\": {\"value\": \"adam\"},\n",
    "    \"f1_layer_size\": {\"values\": [32,64,128,256]},\n",
    "    \"f2_layer_size\": {\"values\": [32,64,128,256]},\n",
    "    \"dropout\": {\"values\": [0.3, 0.4, 0.5]},\n",
    "}\n",
    "\n",
    "sweep_config[\"parameters\"] = parameters_dict\n",
    "\n",
    "parameters_dict.update(\n",
    "    {\n",
    "        \"epochs\": {\"values\": [500]},\n",
    "        \"validation_split\": {\"value\": 0.1},\n",
    "        \"loss\": {\n",
    "            # \"values\": [ \"l1_custom\", \"Huber_custom\"],\n",
    "            # \"values\": [\"Huber\", \"MSE\", \"L1\", \"BCE\", \"Custom\", \"KL\"]\n",
    "            'value': 'l1_custom'\n",
    "        },\n",
    "        \"num_layers\": {\"values\": [2]},\n",
    "    }\n",
    ")\n",
    "\n",
    "parameters_dict.update(\n",
    "    {\n",
    "        \"learning_rate\": {\n",
    "            # a flat distribution between 0 and 0.1\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.0001,\n",
    "            \"max\": 0.001,\n",
    "        },\n",
    "        \"l1_beta\": {\n",
    "            # a flat distribution between 0 and 0.1\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0,\n",
    "            \"max\": 1,\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            # 'value': 1000\n",
    "            \"values\": [32, 64, 128, 360,720]\n",
    "            # 'values':[4,8,16,32,64,128,360]\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)\n",
    "\n",
    "\n",
    "sweep_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\wandb\\run-20220901_160211-1qzd29b2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nickojelly/new%20customs/runs/1qzd29b2\" target=\"_blank\">soft-pond-28</a></strong> to <a href=\"https://wandb.ai/nickojelly/new%20customs\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 32, 'dropout': 0.5, 'epochs': 1000, 'f1_layer_size': 128, 'f2_layer_size': 128, 'learning_rate': 0.0001, 'loss': 'KL', 'l1_beta': 0.2, 'num_layers': 2, 'optimizer': 'adam', 'validation_split': 0.1}\n",
      "1000\n",
      "{'batch_size': 32, 'dropout': 0.5, 'epochs': 1000, 'f1_layer_size': 128, 'f2_layer_size': 128, 'learning_rate': 0.0001, 'loss': 'KL', 'l1_beta': 0.2, 'num_layers': 2, 'optimizer': 'adam', 'validation_split': 0.1}\n",
      "Sequential(\n",
      "  (0): Linear(in_features=120, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (7): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_25112\\2741069703.py:69: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = criterion(F.log_softmax(outputs), labels.float())\n",
      "  0%|          | 1/1000 [00:17<4:48:18, 17.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 66/1000 [08:08<1:46:14,  6.83s/it]"
     ]
    }
   ],
   "source": [
    "wandb_config_static = {'batch_size': 32, 'dropout': 0.5, 'epochs': 1000, 'f1_layer_size': 128, 'f2_layer_size': 128, 'learning_rate': 0.0001, 'loss': 'KL', 'l1_beta':0.2, 'num_layers': 2, 'optimizer': 'adam', 'validation_split': 0.1}\n",
    "model = model_pipeline(config=wandb_config_static)\n",
    "# wandb.init()\n",
    "# model = model_pipeline(config=wandb_config_static, prev_model=r'C:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\pytorch\\New Model\\good models\\vocal-sweep-80_90.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pause\n",
    "#model_pipeline(config)\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"grv_priced_sweep_custom\")\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "wandb.agent(sweep_id, function=model_pipeline, count=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=120, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=128, out_features=8, bias=True)\n",
       "  (7): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43115443075634c02a7c247a87b0dd9d74842892e56d473b9e19f544f3149aff"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
