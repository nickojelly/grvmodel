{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import pandas as pd \n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25104\n"
     ]
    }
   ],
   "source": [
    "training_data = np.load(\"training_data.npy\", allow_pickle=True)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(r'C:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\finalnew.txt', 'rb')\n",
    "data = pickle.load(file)\n",
    "#seperate out classes from inputs\n",
    "classes, inputs = zip(*data)\n",
    "\n",
    "#removing nan from inputs and convert to float\n",
    "inputs_df = pd.DataFrame(inputs)\n",
    "inputs_df.fillna(value=-1,inplace = True)\n",
    "inputs = inputs_df.values.tolist()\n",
    "inputs = [[float(i) for i in j] for j in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=409, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=8, bias=True)\n",
      ")\n",
      "2510\n",
      "22594 2510\n"
     ]
    }
   ],
   "source": [
    "REBUILD_DATA = False\n",
    "\n",
    "class GRV():\n",
    "    #class to store training data\n",
    "    \n",
    "    #reading data from pickle\n",
    "    file = open(r'C:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\finalnew.txt', 'rb')\n",
    "    data = pickle.load(file)\n",
    "    #seperate out classes from inputs\n",
    "    classes, inputs = zip(*data)\n",
    "    \n",
    "    #removing nan from inputs and convert to float\n",
    "    inputs_df = pd.DataFrame(inputs)\n",
    "    inputs_df.fillna(value=-1,inplace = True)\n",
    "    inputs = inputs_df.values.tolist()\n",
    "    inputs = [[float(i) for i in j] for j in inputs]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #data\n",
    "    training_data = []\n",
    "\n",
    "    def make_training_data(self):\n",
    "        excluded = 0\n",
    "        for i in range(len(inputs)):\n",
    "            if len(classes[i]) == 8:\n",
    "                self.training_data.append([np.array(inputs[i]),np.array(classes[i])])\n",
    "            else:\n",
    "                excluded += 1\n",
    "        np.save('training_data.npy', self.training_data)\n",
    "        print(\"excluded = \", excluded)\n",
    "        \n",
    "if REBUILD_DATA:\n",
    "    grv = GRV()\n",
    "    grv.make_training_data()\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(409, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x #nn.Softmax(x, dim=1)\n",
    "        \n",
    "        \n",
    "net = Net().to(device)\n",
    "print(net)    \n",
    "#define tensors\n",
    "\n",
    "X = torch.Tensor([i[0] for i in training_data])\n",
    "Y = torch.Tensor([i[1] for i in training_data])\n",
    "#Generate winner only class\n",
    "Y_w = []\n",
    "for i in Y:\n",
    "    n = np.zeros(8)\n",
    "    index = torch.argmin(i)\n",
    "    n[index] = float(1)\n",
    "    Y_w.append(n)\n",
    "    \n",
    "Y_w = torch.tensor([i for i in Y_w])\n",
    "    \n",
    "VAL_PCT = 0.1  # lets reserve 10% of our data for validation\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(val_size)\n",
    "train_X = X[:-val_size]\n",
    "train_y = Y_w[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = Y_w[-val_size:]\n",
    "print(len(train_X), len(test_X))\n",
    "\n",
    "test_X = test_X.to(device)\n",
    "test_y = test_y.to(device)\n",
    "    \n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 1000\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.05)\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(EPOCHS)):\n",
    "        for i in range(0, len(train_X), BATCH_SIZE): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "            #print(f\"{i}:{i+BATCH_SIZE}\")\n",
    "            batch_X = train_X[i:i+BATCH_SIZE]\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            m = nn.LogSoftmax(dim = 1)\n",
    "            outputs = m(net(batch_X))\n",
    "            loss = loss_function(outputs, batch_y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()    # Does the update\n",
    "\n",
    "\n",
    "\n",
    "        #print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "        #test(net)\n",
    "        if (epoch%100 == 0):\n",
    "            print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "            test(net)\n",
    "            \n",
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(len(test_X))):\n",
    "            #print(\"test_y[i] \\n\", test_y[i], \"\\noutput y =\", net(test_X[i]))\n",
    "            real_class = torch.argmax(test_y[i]).to(device)\n",
    "            net_out = net(test_X[i]).to(device)  # returns a list, \n",
    "            #print(net_out)\n",
    "            predicted_class = torch.argmax(net_out)\n",
    "            #print(\"predicted_class = \", predicted_class,\"\\nreal class = \", real_class)\n",
    "            \n",
    "            #print(predicted_class)\n",
    "            if predicted_class == real_class:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor([i[0] for i in training_data])\n",
    "Y = torch.Tensor([i[1] for i in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#Generate winner only class\n",
    "Y_w = []\n",
    "for i in Y:\n",
    "    n = np.zeros(8)\n",
    "    index = torch.argmin(i)\n",
    "    n[index] = float(1)\n",
    "    Y_w.append(n)\n",
    "Y_w = torch.tensor([i for i in Y_w])\n",
    "Y_w[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([3, 4, 1,  ..., 0, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "#indexed win only\n",
    "Y_wi = []\n",
    "for i in Y:\n",
    "    #print(torch.argmin(i))\n",
    "    Y_wi.append(torch.argmin(i))\n",
    "Y_w = torch.tensor(Y_wi)\n",
    "print(Y_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates places\n",
    "Y_p = []\n",
    "for i in Y:\n",
    "    place = [x*x for x in i]\n",
    "    Y_p.append(place)\n",
    "    \n",
    "Y_p[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (fc1): Linear(in_features=409, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=32, bias=True)\n  (fc3): Linear(in_features=32, out_features=8, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(409, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "net = Net().to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fc1.weight Parameter containing:\ntensor([[ 0.0003, -0.0287,  0.0260,  ..., -0.0343, -0.0237,  0.0453],\n        [ 0.0209,  0.0221,  0.0139,  ..., -0.0159, -0.0340,  0.0090],\n        [ 0.0118, -0.0334, -0.0321,  ...,  0.0333,  0.0097,  0.0253],\n        ...,\n        [-0.0230,  0.0069,  0.0276,  ...,  0.0223,  0.0233,  0.0301],\n        [ 0.0048, -0.0066,  0.0234,  ..., -0.0289, -0.0260,  0.0068],\n        [ 0.0278, -0.0488,  0.0087,  ..., -0.0198,  0.0134,  0.0193]],\n       device='cuda:0', requires_grad=True)\nfc1.bias Parameter containing:\ntensor([ 0.0071,  0.0395,  0.0162,  0.0059, -0.0032,  0.0381,  0.0053,  0.0232,\n        -0.0292,  0.0161, -0.0021,  0.0048, -0.0166,  0.0407, -0.0308,  0.0355,\n        -0.0285, -0.0333,  0.0283,  0.0160, -0.0229, -0.0045,  0.0448,  0.0307,\n         0.0433, -0.0219, -0.0026, -0.0287,  0.0424, -0.0359,  0.0212, -0.0476,\n        -0.0193, -0.0339, -0.0136,  0.0336, -0.0298, -0.0115,  0.0385, -0.0333,\n        -0.0348,  0.0341, -0.0251,  0.0473,  0.0294,  0.0296,  0.0293,  0.0018,\n        -0.0064,  0.0292, -0.0345, -0.0060, -0.0246,  0.0346, -0.0390,  0.0234,\n        -0.0079,  0.0163,  0.0259,  0.0109,  0.0043,  0.0187, -0.0030, -0.0351],\n       device='cuda:0', requires_grad=True)\nfc2.weight Parameter containing:\ntensor([[ 0.0467,  0.1146,  0.0495,  ...,  0.0674, -0.0782, -0.0997],\n        [ 0.0515, -0.0499,  0.0730,  ..., -0.0009,  0.0437,  0.0609],\n        [-0.0804,  0.0136,  0.0788,  ...,  0.1169,  0.0965,  0.0187],\n        ...,\n        [ 0.0319, -0.1062, -0.1246,  ...,  0.0670, -0.0442, -0.0566],\n        [-0.0952,  0.0131,  0.0295,  ...,  0.1224,  0.1098,  0.0679],\n        [-0.1094,  0.0193,  0.0693,  ..., -0.0249, -0.1012, -0.0057]],\n       device='cuda:0', requires_grad=True)\nfc2.bias Parameter containing:\ntensor([ 0.0990, -0.1130,  0.1213, -0.0474, -0.0274,  0.0917, -0.1002, -0.1173,\n         0.0547, -0.0168, -0.0197,  0.0075, -0.0475, -0.0321, -0.0749,  0.0788,\n        -0.1062,  0.0188,  0.0108, -0.0908, -0.1160,  0.0396, -0.0958,  0.0523,\n        -0.0144,  0.0138,  0.1162, -0.0504, -0.0625, -0.0565,  0.1195,  0.0488],\n       device='cuda:0', requires_grad=True)\nfc3.weight Parameter containing:\ntensor([[-0.0958, -0.0470, -0.0876, -0.1177,  0.1314, -0.0027, -0.0145, -0.1461,\n          0.1212,  0.1520,  0.0807, -0.1603, -0.0007,  0.0142,  0.0325, -0.1492,\n         -0.1361, -0.0208, -0.0711,  0.1191,  0.1209,  0.0393,  0.0402, -0.1363,\n          0.0702, -0.0336, -0.1722, -0.0825,  0.1406,  0.0042, -0.0449, -0.0993],\n        [ 0.1463,  0.1197, -0.0086, -0.1382,  0.1361,  0.0416, -0.0635, -0.0055,\n         -0.0195, -0.0041, -0.1189, -0.0101,  0.1398,  0.1036,  0.0762,  0.0934,\n         -0.0625,  0.0249, -0.0718,  0.1207, -0.0972,  0.1072,  0.1666,  0.1205,\n          0.1052, -0.0770,  0.0800,  0.0314, -0.1275, -0.1441, -0.0159,  0.1002],\n        [ 0.1225, -0.0929,  0.1690, -0.0662, -0.0816,  0.1175, -0.1446, -0.1738,\n         -0.1083, -0.0191,  0.0881, -0.1113, -0.0622, -0.0574,  0.0348,  0.1222,\n         -0.0033,  0.0624,  0.0400, -0.1287,  0.0662,  0.1702, -0.0488,  0.0029,\n          0.1172,  0.0551, -0.0210, -0.1513,  0.0706,  0.1170,  0.0677, -0.0405],\n        [ 0.0242, -0.0534, -0.0325, -0.0232,  0.0755, -0.0049,  0.0722,  0.0155,\n         -0.0834,  0.1451, -0.0233, -0.0109, -0.1286, -0.0234,  0.0634, -0.0799,\n         -0.0005, -0.0014, -0.1735,  0.0711, -0.1546,  0.1025,  0.0211,  0.1305,\n         -0.1453, -0.1752, -0.0781,  0.0633, -0.0605,  0.1584,  0.0150,  0.1030],\n        [ 0.0607,  0.1225,  0.0405, -0.0840, -0.1369,  0.0764,  0.0743,  0.0015,\n         -0.0411,  0.0728,  0.0696,  0.0133, -0.0708,  0.1000, -0.0607,  0.1035,\n         -0.0954,  0.0022,  0.0643, -0.1320, -0.0633, -0.0029,  0.1757,  0.1353,\n          0.1662, -0.0301, -0.1555,  0.0726, -0.0152,  0.1660,  0.0324, -0.0206],\n        [-0.1026, -0.0633,  0.0932,  0.0485, -0.0309,  0.1635, -0.0389,  0.1657,\n          0.0167,  0.1578,  0.0315, -0.0457, -0.0408,  0.0254,  0.0960,  0.1015,\n          0.1598,  0.1459, -0.1313, -0.1550,  0.0779,  0.1344, -0.0052,  0.0387,\n          0.0395, -0.0209, -0.1629,  0.0216, -0.1678, -0.0712,  0.1631, -0.1641],\n        [ 0.1176, -0.1404,  0.1071, -0.1742, -0.1596,  0.0863, -0.0835,  0.0549,\n          0.0848,  0.0705, -0.1505,  0.1220,  0.1304, -0.0835, -0.0242, -0.0488,\n         -0.0411, -0.0636,  0.1368, -0.1288, -0.1717, -0.1486,  0.0229,  0.1384,\n         -0.0398,  0.1427,  0.0116, -0.0512, -0.0069, -0.1621,  0.0459, -0.1274],\n        [-0.0339,  0.1189, -0.0240,  0.1255,  0.0541, -0.1198,  0.0234, -0.1432,\n          0.1202, -0.0255,  0.0317, -0.1286, -0.1208, -0.0766,  0.0021,  0.0683,\n         -0.1570,  0.0173,  0.1493, -0.1520,  0.1217,  0.1207,  0.1344,  0.1141,\n         -0.0005,  0.0846,  0.1218, -0.0957,  0.0449,  0.1230,  0.0323, -0.1112]],\n       device='cuda:0', requires_grad=True)\nfc3.bias Parameter containing:\ntensor([ 0.0832, -0.0136, -0.1752,  0.1037,  0.0386,  0.1158, -0.0046,  0.1189],\n       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#validating model is new\n",
    "for (i,j) in net.named_parameters():\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2510\n22594 2510\n"
     ]
    }
   ],
   "source": [
    "VAL_PCT = 0.1  # lets reserve 10% of our data for validation\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(val_size)\n",
    "train_X = X[:-val_size]\n",
    "train_y = Y_w[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = Y_w[-val_size:]\n",
    "print(len(train_X), len(test_X))\n",
    "\n",
    "test_X = test_X.to(device)\n",
    "test_y = test_y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2510\n22594 2510\n"
     ]
    }
   ],
   "source": [
    "#DONT RUN\n",
    "\n",
    "\n",
    "VAL_PCT = 0.1  # lets reserve 10% of our data for validation\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(val_size)\n",
    "train_X = X[:-val_size]\n",
    "train_y = Y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = Y[-val_size:]\n",
    "print(len(train_X), len(test_X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utalized Win only class, not usefull for ranking placement\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 100\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(EPOCHS)):\n",
    "        for i in range(0, len(train_X), BATCH_SIZE): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "            #print(f\"{i}:{i+BATCH_SIZE}\")\n",
    "            batch_X = train_X[i:i+BATCH_SIZE]\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            #print(batch_X, batch_y)\n",
    "\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            m = nn.LogSoftmax(dim = 1)\n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()    # Does the update\n",
    "\n",
    "\n",
    "\n",
    "        #print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "        #test(net)\n",
    "        if (epoch%10 == 0):\n",
    "            print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "            test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Epoch: 0. Loss: 25.059072494506836\n",
      "  1%|          | 1/100 [00:01<01:41,  1.03s/it]Accuracy:  0.133\n",
      " 10%|█         | 10/100 [00:03<00:27,  3.22it/s]Epoch: 10. Loss: 4.179200649261475\n",
      " 11%|█         | 11/100 [00:04<00:47,  1.89it/s]Accuracy:  0.135\n",
      " 20%|██        | 20/100 [00:07<00:24,  3.21it/s]Epoch: 20. Loss: 3.0556342601776123\n",
      " 21%|██        | 21/100 [00:08<00:41,  1.89it/s]Accuracy:  0.139\n",
      " 30%|███       | 30/100 [00:11<00:21,  3.23it/s]Epoch: 30. Loss: 2.9657440185546875\n",
      " 31%|███       | 31/100 [00:12<00:36,  1.91it/s]Accuracy:  0.138\n",
      " 40%|████      | 40/100 [00:14<00:18,  3.23it/s]Epoch: 40. Loss: 2.7551536560058594\n",
      " 41%|████      | 41/100 [00:15<00:30,  1.90it/s]Accuracy:  0.138\n",
      " 50%|█████     | 50/100 [00:18<00:15,  3.20it/s]Epoch: 50. Loss: 2.5377955436706543\n",
      " 51%|█████     | 51/100 [00:19<00:25,  1.89it/s]Accuracy:  0.145\n",
      " 60%|██████    | 60/100 [00:22<00:12,  3.21it/s]Epoch: 60. Loss: 2.4798874855041504\n",
      " 61%|██████    | 61/100 [00:23<00:20,  1.90it/s]Accuracy:  0.138\n",
      " 70%|███████   | 70/100 [00:26<00:09,  3.19it/s]Epoch: 70. Loss: 2.4326119422912598\n",
      " 71%|███████   | 71/100 [00:27<00:15,  1.87it/s]Accuracy:  0.14\n",
      " 80%|████████  | 80/100 [00:29<00:06,  3.23it/s]Epoch: 80. Loss: 2.3152732849121094\n",
      " 81%|████████  | 81/100 [00:31<00:09,  1.91it/s]Accuracy:  0.145\n",
      " 90%|█████████ | 90/100 [00:33<00:03,  3.28it/s]Epoch: 90. Loss: 2.3623619079589844\n",
      " 91%|█████████ | 91/100 [00:34<00:04,  1.92it/s]Accuracy:  0.147\n",
      "100%|██████████| 100/100 [00:37<00:00,  2.67it/s]\n"
     ]
    }
   ],
   "source": [
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    realclasslist = []\n",
    "    predictedClassList = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_X)):\n",
    "            #print(\"test_y[i] \\n\", test_y[i], \"\\noutput y =\", net(test_X[i]))\n",
    "            real_class = torch.argmax(test_y[i]).to(device)\n",
    "            realclasslist.append(test_y[i])\n",
    "            net_out = net(test_X[i]).to(device)  # returns a list, \n",
    "            predictedClassList.append(net_out.tolist())\n",
    "            #print(net_out)\n",
    "            predicted_class = torch.argmax(net_out)\n",
    "            #print(\"predicted_class = \", predicted_class,\"\\nreal class = \", real_class)\n",
    "            \n",
    "            #print(predicted_class)\n",
    "            if predicted_class == real_class:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    print(\"Accuracy: \", round(correct/total, 3))\n",
    "    return (predictedClassList, realclasslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for indexed win only\n",
    "\n",
    "\n",
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    realclasslist = []\n",
    "    predictedClassList = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_X)):\n",
    "            #print(\"test_y[i] \\n\", test_y[i], \"\\noutput y =\", net(test_X[i]))\n",
    "            real_class = test_y[i]\n",
    "            #print(real_class)\n",
    "            realclasslist.append(test_y[i])\n",
    "            net_out = net(test_X[i]).to(device)  # returns a list, \n",
    "            predictedClassList.append(net_out.tolist())\n",
    "            #print(net_out)\n",
    "            predicted_class = torch.argmax(net_out)\n",
    "            #print(\"predicted_class = \", predicted_class,\"\\nreal class = \", real_class)\n",
    "            \n",
    "            #print(predicted_class)\n",
    "            if predicted_class == real_class:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    print(\"Accuracy: \", round(correct/total, 3))\n",
    "    return (predictedClassList, realclasslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy:  0.139\n"
     ]
    }
   ],
   "source": [
    "results = test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "winners = [i.item() for i in Y_wi]\n",
    "from collections import Counter\n",
    "  \n",
    "def most_frequent(List):\n",
    "    occurence_count = Counter(List)\n",
    "    return occurence_count.most_common(1)[0][0]\n",
    "    \n",
    "List = winners\n",
    "print(most_frequent(List))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTing\n",
    "m = nn.LogSoftmax(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    print(torch.argmax(net(test_X)[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Number' : Y_wi})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "interpreter": {
   "hash": "bfbc89c8622a84d0dd68be4d1447c40cc119f7a41d05894739daaf06a08adac1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}