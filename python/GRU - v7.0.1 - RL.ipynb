{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import setup\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import wandb\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "from operator import itemgetter\n",
    "import operator\n",
    "from random import randint\n",
    "# from rnn_classes import Dog, DogInput, Race, Races, GRUNet, smallGRUNet, smalll_lin_GRUNet, smalll_prelin_GRUNet\n",
    "import rnn_tools.rnn_classes as rnn_classes\n",
    "from rnn_tools.raceDB import build_dataset\n",
    "import importlib\n",
    "import datetime\n",
    "from rnn_tools.model_saver import model_saver, model_saver_wandb\n",
    "import rnn_tools.training_testing_gru as training_testing_gru\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pack_sequence, pad_packed_sequence,pad_sequence, unpack_sequence, unpad_sequence\n",
    "import rnn_tools.training_testing_gru_double as training_testing_gru_double\n",
    "import rnn_tools.training_testing_lstm as training_testing_lstm\n",
    "from goto_conversion import goto_conversion\n",
    "\n",
    "import rnn_tools.training_testing_gru_extra_data as training_testing_gru_extra_data\n",
    "import rnn_tools.training_testing_gru_ensemble as training_testing_gru_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rnn_tools.rl_grv as rl_grv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'kind-sponge-234'\n",
    "\n",
    "data = pd.read_feather(F'./model_all_price/RLkind-sponge-234 - val_all_price_df.fth')\n",
    "test_data =  pd.read_feather(F'./model_all_price/RLkind-sponge-234 - all_price_df.fth')\n",
    "all_data = pd.concat([data, test_data]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.dog_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['pred_price'] = 1/all_data['output_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.sort_values(['raceID','dog_box'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_data.win_price.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.dog_box.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.prices = all_data.prices.fillna(0)\n",
    "all_data['price_probs'] = 1/all_data.prices\n",
    "print(all_data.price_probs.value_counts(dropna=False))\n",
    "all_data['price_probs'] = all_data.price_probs.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "print(all_data.price_probs.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.price_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_g = all_data.groupby('raceID')\n",
    "\n",
    "prices = []\n",
    "price_preds = []\n",
    "model_preds = []\n",
    "dogs = []\n",
    "results = []\n",
    "print(all_data.columns)\n",
    "for name, group in all_data_g:\n",
    "    # print(group.columns)\n",
    "    if group['prices'].sum() == 0:\n",
    "        continue\n",
    "    prices.append(group['price_probs'].values)\n",
    "    price_preds.append(group['pred_price'].values)\n",
    "    model_preds.append(group['pred_prob'].values)\n",
    "    dogs.append(group['dog_name'].values)\n",
    "    results.append(group['one_hot_win'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from wandb.integration.sb3 import WandbCallback \n",
    "import rnn_tools.rl_grv as rl_grv\n",
    "importlib.reload(rl_grv)\n",
    "import importlib\n",
    "from stable_baselines3 import A2C\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "#     \"policy_type\": \"MlpPolicy\",\n",
    "# }\n",
    "# run = wandb.init(\n",
    "#     project=\"sb3\",\n",
    "#     config=config,\n",
    "#     sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "#     monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "#     # save_code=True,  # optional\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "dogs_train, dogs_test, prices_train, prices_test, price_preds_train, price_preds_test, model_preds_train, model_preds_test, results_train, results_test = train_test_split(dogs, prices, price_preds, model_preds, results, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainraceData = rl_grv.RaceData(dogs_train,prices_train, price_preds_train, model_preds_train, results_train)\n",
    "eval_raceData = rl_grv.RaceData(dogs_test,prices_test, price_preds_test, model_preds_test, results_test)\n",
    "env = rl_grv.GreyhoundRacingEnv(trainraceData)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Create a separate environment for evaluation\n",
    "eval_env = rl_grv.GreyhoundRacingEnv(eval_raceData)\n",
    "eval_env = DummyVecEnv([lambda: eval_env])\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# eval_env = Monitor(eval_env) \n",
    "\n",
    "\n",
    "# Initialize the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=0, learning_rate=0.01)\n",
    "\n",
    "# model = A2C(\"MlpPolicy\", env, verbose=0)\n",
    "print(f\"{len(eval_raceData.dogs)=} {len(trainraceData.dogs)=}\")\n",
    "# Train model \n",
    "# Initialize W&B\n",
    "wandb.init(project=\"sb3\",save_code=False)\n",
    "# wandb_callback = WandbCallback()\n",
    "class CustomWandbCallback(WandbCallback):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.rewards = []\n",
    "        self.log_interval = 1000\n",
    "        self.actions = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        super()._on_step()\n",
    "        # Append the reward to the list of rewards\n",
    "        self.rewards.append(self.training_env.get_attr('reward')[0])\n",
    "        # If the number of steps is a multiple of the log interval, log the average reward\n",
    "        # self.actions.append(self.training_env.get_attr('action')[0])\n",
    "        if len(self.rewards) % self.log_interval == 0:\n",
    "            avg_reward = sum(self.rewards) / len(self.rewards)\n",
    "            wandb.log({'avg_reward': avg_reward})\n",
    "            # Clear the list of rewards\n",
    "            # wandb.log({'action': self.actions})\n",
    "            \n",
    "            self.rewards = []\n",
    "            self.actions = []\n",
    "        return True\n",
    "\n",
    "# Use the custom callback\n",
    "wandb_callback = CustomWandbCallback()\n",
    "\n",
    "\n",
    "# Number of training iterations\n",
    "n_iterations = 100\n",
    "\n",
    "# Number of timesteps to train for each iteration\n",
    "timesteps_per_iteration = 10_000\n",
    "\n",
    "# Number of episodes to test for each iteration\n",
    "test_episodes_per_iteration = 1\n",
    "\n",
    "for iteration in trange(n_iterations):\n",
    "    env.reset()\n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=timesteps_per_iteration, callback=wandb_callback)\n",
    "\n",
    "    # Test the model\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    obs = eval_env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = eval_env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "    std_reward = 0\n",
    "    tqdm.write(f\"Mean reward: {episode_reward/episode_length} +/- {std_reward}\")\n",
    "    # print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "    # Log the rewards to wandb\n",
    "    wandb.log({'mean_reward': episode_reward/episode_length, 'std_reward': std_reward})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from wandb.integration.sb3 import WandbCallback \n",
    "import rnn_tools.rl_grv_v_simple as rl_grv\n",
    "importlib.reload(rl_grv)\n",
    "import importlib\n",
    "from stable_baselines3 import A2C\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "#     \"policy_type\": \"MlpPolicy\",\n",
    "# }\n",
    "# run = wandb.init(\n",
    "#     project=\"sb3\",\n",
    "#     config=config,\n",
    "#     sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "#     monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
    "#     # save_code=True,  # optional\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "dogs_train, dogs_test, prices_train, prices_test, price_preds_train, price_preds_test, model_preds_train, model_preds_test, results_train, results_test = train_test_split(dogs, prices, price_preds, model_preds, results, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainraceData = rl_grv.RaceData(dogs_train,prices_train, price_preds_train, model_preds_train, results_train)\n",
    "eval_raceData = rl_grv.RaceData(dogs_test,prices_test, price_preds_test, model_preds_test, results_test)\n",
    "env = rl_grv.GreyhoundRacingEnv(trainraceData)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Create a separate environment for evaluation\n",
    "eval_env = rl_grv.GreyhoundRacingEnv(eval_raceData)\n",
    "eval_env = DummyVecEnv([lambda: eval_env])\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# eval_env = Monitor(eval_env) \n",
    "\n",
    "\n",
    "# Initialize the agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=0, learning_rate=0.01)\n",
    "\n",
    "# model = A2C(\"MlpPolicy\", env, verbose=0)\n",
    "print(f\"{len(eval_raceData.dogs)=} {len(trainraceData.dogs)=}\")\n",
    "# Train model \n",
    "# Initialize W&B\n",
    "wandb.init(project=\"sb3\",save_code=False)\n",
    "# wandb_callback = WandbCallback()\n",
    "class CustomWandbCallback(WandbCallback):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.rewards = []\n",
    "        self.log_interval = 1000\n",
    "        self.actions = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        super()._on_step()\n",
    "        # Append the reward to the list of rewards\n",
    "        self.rewards.append(self.training_env.get_attr('reward')[0])\n",
    "        # If the number of steps is a multiple of the log interval, log the average reward\n",
    "        self.actions.append(self.training_env.get_attr('action')[0])\n",
    "        if len(self.rewards) % self.log_interval == 0:\n",
    "            avg_reward = sum(self.rewards) / len(self.rewards)\n",
    "            wandb.log({'avg_reward': avg_reward})\n",
    "            # Clear the list of rewards\n",
    "            wandb.log({'action': self.actions})\n",
    "            self.rewards = []\n",
    "            self.actions = []\n",
    "        return True\n",
    "\n",
    "# Use the custom callback\n",
    "wandb_callback = CustomWandbCallback()\n",
    "\n",
    "\n",
    "# Number of training iterations\n",
    "n_iterations = 100\n",
    "\n",
    "# Number of timesteps to train for each iteration\n",
    "timesteps_per_iteration = 10_000\n",
    "\n",
    "# Number of episodes to test for each iteration\n",
    "test_episodes_per_iteration = 1\n",
    "\n",
    "for iteration in trange(n_iterations):\n",
    "    env.reset()\n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=timesteps_per_iteration)\n",
    "\n",
    "    # Test the model\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    obs = eval_env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        # print(eval_env.step(action))\n",
    "        # print(obs)\n",
    "        # print(action)\n",
    "        obs, reward, done, info = eval_env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "    std_reward = 0\n",
    "    tqdm.write(f\"Mean reward: {episode_reward/episode_length} +/- {std_reward}\")\n",
    "    # print(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "    # Log the rewards to wandb\n",
    "    wandb.log({'mean_reward': episode_reward/episode_length, 'std_reward': std_reward})\n",
    "\n",
    "# eval_env.get_episode_rewards() \n",
    "# eval_env.get_episode_lengths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r in enumerate(trainraceData.results):\n",
    "    if i > 5:\n",
    "        break\n",
    "    print(i,r,np.argmax(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import importlib\n",
    "importlib.reload(rl_grv)\n",
    "from rnn_tools.rl_grv_v_simple import MaxValueEnv\n",
    "# Create the vectorized environment\n",
    "vec_env = make_vec_env(MaxValueEnv, n_envs=1)\n",
    "\n",
    "# Initialize the model\n",
    "model = A2C(\"MlpPolicy\", vec_env, verbose=0)\n",
    "\n",
    "# Number of training iterations\n",
    "n_iterations = 10\n",
    "\n",
    "# Number of timesteps to train for each iteration\n",
    "timesteps_per_iteration = 10_000\n",
    "\n",
    "# import matplotlib.pyplot as plt \n",
    "\n",
    "# reset the environment and see the initial observation\n",
    "obs = env.reset()\n",
    "print(\"The initial observation is {}\".format(obs))\n",
    "random_action = env.action_space.sample()\n",
    "random_action\n",
    "for iteration in range(n_iterations):\n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=timesteps_per_iteration)\n",
    "\n",
    "    # Save the model\n",
    "    model.save(f\"a2c_max_value_{iteration}\")\n",
    "\n",
    "# print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTORCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a48ca33c5a1168302a4f8eae355aad1c03b1396f568d40bc174a6e6aabe725d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
