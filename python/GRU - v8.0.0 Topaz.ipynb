{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import setup\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import wandb\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "from operator import itemgetter\n",
    "import operator\n",
    "from random import randint\n",
    "# from rnn_classes import Dog, DogInput, Race, Races, GRUNet, smallGRUNet, smalll_lin_GRUNet, smalll_prelin_GRUNet\n",
    "import rnn_tools.rnn_classes as rnn_classes\n",
    "from rnn_tools.raceDB import build_dataset\n",
    "import importlib\n",
    "import datetime\n",
    "from rnn_tools.model_saver import model_saver, model_saver_wandb\n",
    "# import rnn_tools.training_testing_gru as training_testing_gru\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pack_sequence, pad_packed_sequence,pad_sequence, unpack_sequence, unpad_sequence\n",
    "import rnn_tools.training_testing_gru_double as training_testing_gru_double\n",
    "from goto_conversion import goto_conversion\n",
    "\n",
    "import rnn_tools.training_testing_gru_extra_data as training_testing_gru_extra_data\n",
    "import rnn_tools.training_testing_gru_extra_data_embedding as training_testing_gru_extra_data_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"GRU - v8.0.0 Topaz.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = './data/topaz_data_w_bsp_new3.fth'\n",
    "df = pd.read_feather(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meetingDate\n",
      "state\n",
      "track\n",
      "distance\n",
      "raceId\n",
      "raceTypeCode\n",
      "raceNumber\n",
      "boxNumber\n",
      "rugNumber\n",
      "runId\n",
      "dogId\n",
      "dogName\n",
      "weightInKg\n",
      "sex\n",
      "trainerId\n",
      "trainerState\n",
      "damId\n",
      "damName\n",
      "sireId\n",
      "sireName\n",
      "win\n",
      "place\n",
      "resultTime\n",
      "resultMargin\n",
      "resultMarginLengths\n",
      "dogAgeScaled\n",
      "startPrice\n",
      "weightInKgScaled\n",
      "rolling_box_win_percentage\n",
      "hasEntryBoxNumberPlus1\n",
      "hasEntryBoxNumberMinus1\n",
      "dog_distance_mean_1\n",
      "dog_boxNumber_mean_1\n",
      "dog_runTimeNorm_mean_1\n",
      "dog_place_mean_1\n",
      "dog_resultMargin_mean_1\n",
      "dog_split_time_margin_mean_1\n",
      "dog_split_runTimeNorm_mean_1\n",
      "dog_time_1_mean_1\n",
      "dog_run_home_TimeNorm_mean_1\n",
      "dog_finishingPlaceMovement_mean_1\n",
      "dog_averageSpeed_mean_1\n",
      "dog_win_mean_1\n",
      "dog_distance_mean_365D\n",
      "dog_boxNumber_mean_365D\n",
      "dog_runTimeNorm_mean_365D\n",
      "dog_place_mean_365D\n",
      "dog_resultMargin_mean_365D\n",
      "dog_split_time_margin_mean_365D\n",
      "dog_split_runTimeNorm_mean_365D\n",
      "dog_time_1_mean_365D\n",
      "dog_run_home_TimeNorm_mean_365D\n",
      "dog_finishingPlaceMovement_mean_365D\n",
      "dog_averageSpeed_mean_365D\n",
      "dog_win_mean_365D\n",
      "trainer_distance_mean_1\n",
      "trainer_boxNumber_mean_1\n",
      "trainer_runTimeNorm_mean_1\n",
      "trainer_place_mean_1\n",
      "trainer_resultMargin_mean_1\n",
      "trainer_split_time_margin_mean_1\n",
      "trainer_split_runTimeNorm_mean_1\n",
      "trainer_time_1_mean_1\n",
      "trainer_run_home_TimeNorm_mean_1\n",
      "trainer_finishingPlaceMovement_mean_1\n",
      "trainer_averageSpeed_mean_1\n",
      "trainer_win_mean_1\n",
      "trainer_distance_mean_365D\n",
      "trainer_boxNumber_mean_365D\n",
      "trainer_runTimeNorm_mean_365D\n",
      "trainer_place_mean_365D\n",
      "trainer_resultMargin_mean_365D\n",
      "trainer_split_time_margin_mean_365D\n",
      "trainer_split_runTimeNorm_mean_365D\n",
      "trainer_time_1_mean_365D\n",
      "trainer_run_home_TimeNorm_mean_365D\n",
      "trainer_finishingPlaceMovement_mean_365D\n",
      "trainer_averageSpeed_mean_365D\n",
      "trainer_win_mean_365D\n",
      "dam_distance_mean_1\n",
      "dam_boxNumber_mean_1\n",
      "dam_runTimeNorm_mean_1\n",
      "dam_place_mean_1\n",
      "dam_resultMargin_mean_1\n",
      "dam_split_time_margin_mean_1\n",
      "dam_split_runTimeNorm_mean_1\n",
      "dam_time_1_mean_1\n",
      "dam_run_home_TimeNorm_mean_1\n",
      "dam_finishingPlaceMovement_mean_1\n",
      "dam_averageSpeed_mean_1\n",
      "dam_win_mean_1\n",
      "dam_distance_mean_365D\n",
      "dam_boxNumber_mean_365D\n",
      "dam_runTimeNorm_mean_365D\n",
      "dam_place_mean_365D\n",
      "dam_resultMargin_mean_365D\n",
      "dam_split_time_margin_mean_365D\n",
      "dam_split_runTimeNorm_mean_365D\n",
      "dam_time_1_mean_365D\n",
      "dam_run_home_TimeNorm_mean_365D\n",
      "dam_finishingPlaceMovement_mean_365D\n",
      "dam_averageSpeed_mean_365D\n",
      "dam_win_mean_365D\n",
      "sire_distance_mean_1\n",
      "sire_boxNumber_mean_1\n",
      "sire_runTimeNorm_mean_1\n",
      "sire_place_mean_1\n",
      "sire_resultMargin_mean_1\n",
      "sire_split_time_margin_mean_1\n",
      "sire_split_runTimeNorm_mean_1\n",
      "sire_time_1_mean_1\n",
      "sire_run_home_TimeNorm_mean_1\n",
      "sire_finishingPlaceMovement_mean_1\n",
      "sire_averageSpeed_mean_1\n",
      "sire_win_mean_1\n",
      "sire_distance_mean_365D\n",
      "sire_boxNumber_mean_365D\n",
      "sire_runTimeNorm_mean_365D\n",
      "sire_place_mean_365D\n",
      "sire_resultMargin_mean_365D\n",
      "sire_split_time_margin_mean_365D\n",
      "sire_split_runTimeNorm_mean_365D\n",
      "sire_time_1_mean_365D\n",
      "sire_run_home_TimeNorm_mean_365D\n",
      "sire_finishingPlaceMovement_mean_365D\n",
      "sire_averageSpeed_mean_365D\n",
      "sire_win_mean_365D\n",
      "date\n",
      "EVENT_ID\n",
      "EVENT_DT\n",
      "SELECTION_ID\n",
      "BSP\n",
      "dog\n",
      "MENU_HINT\n",
      "prev_race\n",
      "prev_race_date\n",
      "prev_race_track\n",
      "prev_race_state\n",
      "next_race\n",
      "stats_topaz\n",
      "dogid\n",
      "raceid\n",
      "stats_cols\n",
      "track_hash\n"
     ]
    }
   ],
   "source": [
    "for c in df.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\python\n",
      "(2504581, 145)\n",
      "54\n",
      "Latest date = 2024-05-06 00:00:00\n",
      "size after state filter (2504581, 147)\n",
      "(2504581, 147)\n",
      "Index(['index', 'meetingDate', 'state', 'track_name', 'dist', 'raceId',\n",
      "       'race_grade', 'race_num', 'box', 'rugNumber',\n",
      "       ...\n",
      "       'prev_race_state', 'next_race', 'stats_topaz', 'dogid', 'raceid',\n",
      "       'stats_cols', 'track_hash', 'stats', 'race_time', 'stats_cuda'],\n",
      "      dtype='object', length=148)\n",
      "state  track_name            \n",
      "SA     Murray Bridge Straight    13096\n",
      "Name: count, dtype: int64\n",
      "Latest date = 2024-04-30 00:00:00\n",
      "stats = [ 1.00000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.00000000e+00  3.00000000e+02  7.00000000e+00\n",
      "  4.57918614e-01  2.00000000e+00  2.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  4.57918614e-01 -2.00000000e+00\n",
      "  1.75644035e+01  0.00000000e+00  3.97814392e+02  4.14261341e+00\n",
      "  4.37547475e-01  4.25662899e+00  7.08025551e+00  1.75884470e-01\n",
      "  4.03887451e-01  6.12835407e+00  4.37547475e-01 -4.25662899e+00\n",
      "  1.70153370e+01  9.28030312e-02  4.01517181e+02  4.22760963e+00\n",
      "  4.31190610e-01  4.23939371e+00  8.15290451e+00  1.94183499e-01\n",
      "  4.17308480e-01  6.72435856e+00  4.31190610e-01 -4.23939371e+00\n",
      "  1.68927441e+01  6.04377091e-02  4.04342560e+02  4.44006729e+00\n",
      "  4.40693259e-01  4.60236120e+00  8.06486797e+00  2.06014678e-01\n",
      "  4.28004205e-01  6.82192135e+00  4.40693259e-01 -4.60236120e+00\n",
      "  1.69425678e+01  9.28742364e-02]\n",
      "stats_cols = len(eval(stats_cols)[0])=12, len(dog_stats_df['stats'].iloc[0])=54\n",
      "stats_cols = ['dogAgeScaled', 'boxNumber', 'weightInKgScaled', 'hasEntryBoxNumberPlus1', 'hasEntryBoxNumberMinus1', 'rolling_box_win_percentage', 'dog_distance_mean_1', 'dog_boxNumber_mean_1', 'dog_runTimeNorm_mean_1', 'dog_place_mean_1', 'dog_resultMargin_mean_1', 'dog_split_time_margin_mean_1', 'dog_split_runTimeNorm_mean_1', 'dog_time_1_mean_1', 'dog_run_home_TimeNorm_mean_1', 'dog_finishingPlaceMovement_mean_1', 'dog_averageSpeed_mean_1', 'dog_win_mean_1', 'trainer_distance_mean_365D', 'trainer_boxNumber_mean_365D', 'trainer_runTimeNorm_mean_365D', 'trainer_place_mean_365D', 'trainer_resultMargin_mean_365D', 'trainer_split_time_margin_mean_365D', 'trainer_split_runTimeNorm_mean_365D', 'trainer_time_1_mean_365D', 'trainer_run_home_TimeNorm_mean_365D', 'trainer_finishingPlaceMovement_mean_365D', 'trainer_averageSpeed_mean_365D', 'trainer_win_mean_365D', 'dam_distance_mean_365D', 'dam_boxNumber_mean_365D', 'dam_runTimeNorm_mean_365D', 'dam_place_mean_365D', 'dam_resultMargin_mean_365D', 'dam_split_time_margin_mean_365D', 'dam_split_runTimeNorm_mean_365D', 'dam_time_1_mean_365D', 'dam_run_home_TimeNorm_mean_365D', 'dam_finishingPlaceMovement_mean_365D', 'dam_averageSpeed_mean_365D', 'dam_win_mean_365D', 'sire_distance_mean_365D', 'sire_boxNumber_mean_365D', 'sire_runTimeNorm_mean_365D', 'sire_place_mean_365D', 'sire_resultMargin_mean_365D', 'sire_split_time_margin_mean_365D', 'sire_split_runTimeNorm_mean_365D', 'sire_time_1_mean_365D', 'sire_run_home_TimeNorm_mean_365D', 'sire_finishingPlaceMovement_mean_365D', 'sire_averageSpeed_mean_365D', 'sire_win_mean_365D']\n",
      "['dogAgeScaled', 'boxNumber', 'weightInKgScaled', 'hasEntryBoxNumberPlus1', 'hasEntryBoxNumberMinus1', 'rolling_box_win_percentage', 'dog_distance_mean_1', 'dog_boxNumber_mean_1', 'dog_runTimeNorm_mean_1', 'dog_place_mean_1', 'dog_resultMargin_mean_1', 'dog_split_time_margin_mean_1', 'dog_split_runTimeNorm_mean_1', 'dog_time_1_mean_1', 'dog_run_home_TimeNorm_mean_1', 'dog_finishingPlaceMovement_mean_1', 'dog_averageSpeed_mean_1', 'dog_win_mean_1', 'trainer_distance_mean_365D', 'trainer_boxNumber_mean_365D', 'trainer_runTimeNorm_mean_365D', 'trainer_place_mean_365D', 'trainer_resultMargin_mean_365D', 'trainer_split_time_margin_mean_365D', 'trainer_split_runTimeNorm_mean_365D', 'trainer_time_1_mean_365D', 'trainer_run_home_TimeNorm_mean_365D', 'trainer_finishingPlaceMovement_mean_365D', 'trainer_averageSpeed_mean_365D', 'trainer_win_mean_365D', 'dam_distance_mean_365D', 'dam_boxNumber_mean_365D', 'dam_runTimeNorm_mean_365D', 'dam_place_mean_365D', 'dam_resultMargin_mean_365D', 'dam_split_time_margin_mean_365D', 'dam_split_runTimeNorm_mean_365D', 'dam_time_1_mean_365D', 'dam_run_home_TimeNorm_mean_365D', 'dam_finishingPlaceMovement_mean_365D', 'dam_averageSpeed_mean_365D', 'dam_win_mean_365D', 'sire_distance_mean_365D', 'sire_boxNumber_mean_365D', 'sire_runTimeNorm_mean_365D', 'sire_place_mean_365D', 'sire_resultMargin_mean_365D', 'sire_split_time_margin_mean_365D', 'sire_split_runTimeNorm_mean_365D', 'sire_time_1_mean_365D', 'sire_run_home_TimeNorm_mean_365D', 'sire_finishingPlaceMovement_mean_365D', 'sire_averageSpeed_mean_365D', 'sire_win_mean_365D']\n",
      "num_features_per_dog=54\n",
      "___Processing 2561 unique dogs___\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race_ids=['479136143', '479136144', '479136145', '479136147', '479136146', '479136148', '479136150', '479136149', '479136152', '479136151', '485632394', '485632399', '485632395', '485632396', '485632397', '485632398', '485632404', '485632402', '485632400', '485632401', '485632403', '485632405', '491085794', '491085799', '491085795', '491085796', '491085797', '491085798', '491085801', '491085804', '491085802', '491085800', '491085803', '491085805', '501809163', '501809167', '501809164', '501809165', '501809166', '501809168', '501809169', '501809170', '501809171', '501809172', '501809173', '516002041', '516002040', '516002044', '516002042', '516002043', '516002047', '516002045', '516002046', '516002048', '516002049', '519461908', '519461907', '519461911', '519461909', '519461910', '519461916', '519461914', '519461913', '519461912', '519461915', '523459917', '523459915', '523459919', '523459916', '523459918', '523459921', '523459922', '523459920', '523459924', '523459923', '525253195', '525253200', '525253197', '525253196', '525253199', '525253198', '525253204', '525253203', '525253201', '525253202', '525253205', '530315667', '530315666', '530315670', '530315669', '530315668', '530315673', '530315675', '530315672', '530315671', '530315674', '534934256', '534934257', '534934255', '534934260', '534934258', '534934259', '534934261', '534934262', '534934264', '534934263', '538975802', '538975801', '538975800', '538975804', '538975803', '538975805', '538975807', '538975806', '538975808', '538975809', '540566057', '540566052', '540566053', '540566054', '540566056', '540566055', '540566058', '540566059', '540566061', '540566060', '540593666', '540593665', '540593664', '540593669', '540593667', '540593668', '540593672', '540593671', '540593670', '540593673', '546205264', '546205269', '546205261', '546205263', '546205266', '546205268', '546205271', '546205275', '546205273', '546205279', '546205277', '550361088', '550361094', '550361089', '550361090', '550361091', '550361092', '550361093', '550361099', '550361097', '550361096', '550361095', '550361098', '550362302', '550362307', '550362303', '550362304', '550362305', '550362306', '550362311', '550362309', '550362308', '550362310', '550362312', '557054180', '557054179', '557054178', '557054182', '557054183', '557054181', '557054185', '557054184', '557054186', '557054187', '561767080', '561767076', '561767075', '561767077', '561767078', '561767079', '561767084', '561767083', '561767082', '561767081', '561767085', '568777842', '568777846', '568777843', '568777844', '568777845', '568777847', '568777848', '568777849', '568777850', '568777851', '572594138', '572594139', '572594143', '572594140', '572594141', '572594142', '572594144', '572594147', '572594145', '572594146', '572594149', '572594148', '576081412', '576081407', '576081410', '576081408', '576081409', '576081411', '576081414', '576081413', '576081417', '576081415', '576081416', '580853450', '580853449', '580853448', '580853453', '580853452', '580853451', '580853457', '580853455', '580853454', '580853456', '585152600', '585152599', '585152601', '585152602', '585152603', '585152604', '585152606', '585152605', '585152607', '585152608', '587850931', '587850932', '587850933', '587850936', '587850934', '587850935', '587850937', '587850940', '587850938', '587850939', '587850942', '587850941', '592673623', '592673622', '592673624', '592673625', '592673626', '592673631', '592673629', '592673628', '592673627', '592673630', '592675819', '592675818', '592675817', '592675822', '592675821', '592675820', '592675827', '592675825', '592675824', '592675823', '592675826', '597418069', '597418068', '597418070', '597418071', '597418072', '597418073', '597418075', '597418074', '597418076', '597418077', '601881461', '601881458', '601881463', '601881462', '601881460', '601881456', '601881459', '601881453', '601881457', '601881455', '601881454', '601814991', '601814989', '601814992', '601814988', '601814993', '601814990', '601814984', '601814987', '601814986', '601814985', '601814983', '611251218', '611251223', '611251219', '611251222', '611251224', '611251225', '611251226', '614166020', '614166019', '614166018', '614166021', '614166023', '614166024', '614166026', '614166025', '614166027', '614166028', '617622885', '617622884', '617622883', '617622887', '617622886', '617622888', '617622889', '617622890', '617622892', '617622891', '620594761', '620594760', '620594759', '620594764', '620594762', '620594763', '620594766', '620594767', '620594765', '620594770', '620594768', '620594769', '623859943', '623859942', '623859946', '623859941', '623859945', '623859948', '623859944', '623859949', '623859950', '623859947', '626218691', '626218690', '626218689', '626218694', '626218692', '626218693', '626218695', '626218697', '626218696', '626218699', '626218698', '632745995', '632745994', '632745993', '632745996', '632745997', '632745998', '632745999', '632746000', '632746001', '632746002', '632746003', '634796614', '634796613', '634796617', '634796612', '634796615', '634796616', '634796620', '634796618', '634796619', '634796623', '634796622', '634796621', '638811642', '638811641', '638811640', '638811645', '638811644', '638811643', '638811646', '638811647', '638811649', '638811648', '640881752', '640881751', '640881755', '640881750', '640881754', '640881753', '640881758', '640881757', '640881756', '640881759', '640881760', '645453380', '645453378', '645453379', '645453384', '645453382', '645453381', '645453387', '645453386', '645453389', '645453391', '647734897', '647734896', '647734898', '647734900', '647734899', '647734905', '647734903', '647734902', '647734901', '647734904', '650237616', '650237615', '650237614', '650237619', '650237618', '650237617', '650237621', '650237620', '650237622', '650237623', '654153253', '654153252', '654153251', '654153256', '654153254', '654153255', '654153257', '654153258', '654153260', '654153259', '656343043', '656343042', '656343041', '656343046', '656343045', '656343044', '656343047', '656343048', '656343050', '656343049', '660802024', '660802028', '660802026', '660802025', '660802030', '660802029', '660802027', '660802035', '660802034', '660802032', '660802031', '660802033', '663047535', '663047538', '663047533', '663047534', '663047536', '663047537', '663047542', '663047543', '663047539', '663047540', '663047541', '663047544', '665337098', '665337097', '665337101', '665337096', '665337100', '665337099', '665337105', '665337103', '665337102', '665337106', '665337104', '667670249', '667670248', '667670247', '667670250', '667670251', '667670253', '667670252', '667670254', '667670255', '667670256', '670592532', '670592531', '670592530', '670592533', '670592534', '670592535', '670592539', '670592537', '670592536', '670592538', '672986822', '672986821', '672986826', '672986823', '672986824', '672986825', '672986828', '672986829', '672986827', '672986830', '675362116', '675362115', '675362117', '675362119', '675362120', '675362118', '675362122', '675362121', '675362124', '675362123', '677734312', '677734318', '677734313', '677734314', '677734315', '677734316', '677734317', '677734319', '677734320', '677734323', '677734321', '677734322', '680665137', '680665143', '680665138', '680665139', '680665141', '680665140', '680665142', '680665146', '680665144', '680665145', '680665147', '680665148', '683143604', '683143603', '683143602', '683143606', '683143605', '683143609', '683143608', '683143607', '683143610', '683143611', '685422684', '685422685', '685422683', '685422688', '685422686', '685422687', '685422693', '685422691', '685422690', '685422689', '685422692', '688280337', '688280338', '688280341', '688280340', '688280339', '688280342', '688280343', '688280346', '688280344', '688280345', '689297043', '689297042', '689297041', '689297044', '689297045', '689297046', '689297047', '689297048', '689297050', '689297049', '695934105', '695934104', '695934110', '695934106', '695934107', '695934108', '695934109', '695934112', '695934111', '695934115', '695934114', '695934113', '698513756', '698513757', '698513755', '698513750', '698513758', '698513759', '698513760', '698513749', '698513751', '698513753', '698513754', '698513752', '698456801', '698456802', '698456803', '698456800', '698456799', '698456795', '698456798', '698456797', '698456796', '698456794', '700936132', '700936133', '700936131', '700936136', '700936135', '700936134', '700936139', '700936138', '700936137', '700936141', '700936140', '702388167', '702388166', '702388170', '702388165', '702388169', '702388168', '702388172', '702388171', '702388174', '702388173', '705713195', '705713194', '705713196', '705713198', '705713197', '705713203', '705713199', '705713201', '705713200', '705713202', '707056336', '707056335', '707056334', '707056337', '707056339', '707056338', '707056344', '707056342', '707056341', '707056340', '707056343', '707237903', '707237902', '707237904', '707237906', '707237905', '707237908', '707237907', '707237909', '707237910', '707237911', '710535265', '710535262', '710535266', '710535264', '710535263', '710535256', '710535260', '710535261', '710535259', '710535258', '710535257', '710515486', '710515485', '710515487', '710515483', '710515484', '710515480', '710515479', '710515482', '710515481', '710515477', '710515478', '715001181', '715001182', '715001180', '715001185', '715001184', '715001183', '715001188', '715001187', '715001186', '715001189', '716426994', '716426999', '716426996', '716426995', '716426998', '716426997', '716427001', '716427000', '716427004', '716427002', '716427003', '716432319', '716432318', '716432317', '716432321', '716432320', '716432322', '716432325', '716432324', '716432323', '716432327', '716432326', '718825921', '718825920', '718825919', '718825924', '718825922', '718825923', '718825928', '718825926', '718825925', '718825927', '718825929', '718826129', '718826128', '718826130', '718826131', '718826132', '718826133', '718826135', '718826134', '718826136', '718826137', '726205047', '726205046', '726205049', '726205045', '726205048', '726205051', '726205050', '726205052', '726205054', '726205053', '729421481', '729421482', '729421486', '729421483', '729421484', '729421485', '729421487', '729421491', '729421489', '729421488', '729421490', '729421492', '731273196', '731273195', '731273199', '731273194', '731273198', '731273197', '731273202', '731273203', '731273200', '731273201', '731273204', '734778632', '734778631', '734778633', '734778629', '734778630', '734778624', '734778626', '734778627', '734778628', '734778625', '734725839', '734725838', '734725840', '734725836', '734725837', '734725834', '734725835', '734725833', '734725831', '734725832', '740281666', '740281665', '740281664', '740281667', '740281668', '740281669', '740281670', '740281672', '740281673', '740349936', '740349935', '740349934', '740349937', '740349938', '740349939', '740349940', '740349941', '740349943', '740349942', '745616283', '745616284', '745616285', '745616277', '745616282', '745616281', '745616279', '745616280', '745616278', '745616276', '745589435', '745589436', '745589437', '745589434', '745589433', '745589429', '745589432', '745589430', '745589431', '745589428', '749000660', '749000657', '749000663', '749000658', '749000659', '749000661', '749000662', '749000665', '749000664', '749000667', '749000668', '749000666', '750418552', '750418551', '750418555', '750418550', '750418554', '750418553', '750418558', '750418557', '750418556', '750418559', '752129496', '752129495', '752129497', '752129499', '752129500', '752129498', '752129504', '752129502', '752129501', '752129503', '755416895', '755416894', '755416893', '755416897', '755416896', '755416898', '755416899', '755416900', '755416901', '755416902', '757959925', '757959930', '757959926', '757959927', '757959928', '757959929', '757959933', '757959932', '757959931', '757959936', '757959934', '757959935', '760357722', '760357713', '760357724', '760357723', '760357718', '760357719', '760357721', '760357720', '760357717', '760357716', '760357715', '760357714', '760322936', '760322931', '760322932', '760322935', '760322934', '760322933', '760322928', '760322927', '760322926', '760322930', '760322929', '760322925', '764145773', '764145772', '764145771', '764145775', '764145774', '764145778', '764145776', '764145777', '764145779', '764145780', '765478017', '765478016', '765478018', '765478019', '765478020', '765478021', '765478022', '765478023', '765478025', '765478024', '765509343', '765509342', '765509341', '765509345', '765509344', '765509346', '765509348', '765509347', '765509349', '765509351', '765509350', '767571777', '767571774', '767571775', '767571780', '767571782', '767571778', '767571787', '767571785', '767571783', '767571789', '767571791', '768664984', '768664986', '768664985', '768664988', '768664987', '768664989', '768664991', '768664990', '768664994', '768664992', '768664993', '771810911', '771810912', '771810910', '771810914', '771810916', '771810918', '771810922', '771810920', '771810925', '771810923', '775475104', '775475105', '775475109', '775475106', '775475107', '775475108', '775475112', '775475110', '775475111', '775475113', '775475114', '775475115', '775475340', '775475342', '775475341', '775475343', '775475344', '775475345', '775475349', '775475347', '775475346', '775475348', '779243606', '779243605', '779243604', '779243609', '779243607', '779243608', '779243610', '779243612', '779243611', '779243614', '779243613', '781970075', '781970080', '781970076', '781970077', '781970078', '781970079', '781970082', '781970081', '781970083', '781970085', '781970084', '785346613', '785346618', '785346614', '785346615', '785346617', '785346616', '785346620', '785346622', '785346619', '785346621', '785346623', '785800293', '785800296', '785800294', '785800295', '785800297', '785800298', '785800299', '785800302', '785800301', '785800300', '785800304', '785800303', '790187243', '790187242', '790187241', '790187246', '790187245', '790187244', '790187250', '790187248', '790187247', '790187249', '792243398', '792243399', '792243396', '792243397', '792243394', '792243390', '792243392', '792243393', '792243391', '792204048', '792204051', '792204055', '792204056', '792204049', '792204050', '792204054', '792204053', '792204052', '792204057', '792135080', '792135075', '792135079', '792135078', '792135077', '792135076', '792135073', '792135074', '792135071', '792135070', '792135072', '798984814', '798984815', '798984818', '798984813', '798984817', '798984816', '798984820', '798984819', '798984821', '798984822', '801490834', '801490835', '801490833', '801490831', '801490832', '801490827', '801490829', '801490830', '801490828', '801490826', '801456038', '801456039', '801456040', '801456035', '801456036', '801456037', '801456032', '801456033', '801456034', '801456031', '804048210', '804048216', '804048211', '804048212', '804048213', '804048214', '804048215', '804048218', '804048217', '804048219', '804048220', '804048221', '807519349', '807519348', '807519347', '807519352', '807519350', '807519351', '807519357', '807519354', '807519353', '807519355', '807519356', '807756512', '807756511', '807756510', '807756514', '807756513', '807756517', '807756516', '807756515', '807756519', '807756518', '811787070', '811787069', '811787073', '811787068', '811787072', '811787071', '811787075', '811787074', '811787076', '811787077', '815100618', '815100616', '815100619', '815100620', '815100617', '815100614', '815100613', '815100615', '815100611', '815100612', '815075203', '815075202', '815075204', '815075200', '815075198', '815075196', '815075197', '815075201', '815075205', '815075199', '815075206', '815075195', '820259286', '820259285', '820259287', '820259288', '820259289', '820259294', '820259291', '820259290', '820259292', '820259293', '823038808', '823038807', '823038806', '823038810', '823038809', '823038811', '823038813', '823038812', '823038815', '823038814', '824820399', '824820398', '824820402', '824820397', '824820400', '824820401', '824820406', '824820405', '824820404', '824820403', '824820408', '824820407', '824847158', '824847157', '824847156', '824847161', '824847159', '824847160', '824847163', '824847162', '824847166', '824847164', '824847165', '827795882', '827795881', '827795880', '827795884', '827795883', '827795885', '827795887', '827795886', '827795888', '827795889', '829146098', '829146097', '829146099', '829146100', '829146103', '829146101', '829146102', '829146104', '829146105', '829146108', '829146107', '829146106', '829146399', '829146398', '829146402', '829146397', '829146400', '829146401', '829146406', '829146405', '829146403', '829146404', '829146407', '829146408', '833066436', '833066437', '833066438', '833066434', '833066435', '833066430', '833066432', '833066433', '833066431', '833066429', '832788600', '832788601', '832788602', '832788598', '832788599', '832788594', '832788595', '832788597', '832788596', '832788593', '841164037', '841164038', '841164039', '841164040', '841164041', '841164042', '841164047', '841164045', '841164043', '841164044', '841164046', '841164048', '842799923', '842799918', '842799919', '842799920', '842799922', '842799921', '842799926', '842799924', '842799925', '842799928', '842799927', '842799929', '846619881', '846619872', '846619883', '846619882', '846619878', '846619879', '846619880', '846619875', '846619876', '846619877', '846619874', '846619873', '846606626', '846606620', '846606624', '846606625', '846606622', '846606623', '846606621', '846606617', '846606619', '846606618', '846606616', '846606615', '850665973', '850665981', '850665976', '850665974', '850665979', '850665978', '850665988', '850665983', '850665985', '850665993', '850665990', '850665991', '853520683', '853520684', '853520686', '853520685', '853520687', '853520688', '853520692', '853520691', '853520689', '853520690', '853520694', '853520693', '854027914', '854027913', '854027915', '854027916', '854027917', '854027918', '854027919', '854027920', '854027922', '854027921', '857130516', '857130519', '857130520', '857130518', '857130511', '857130517', '857130514', '857130515', '857130513', '857130512', '857095219', '857095218', '857095221', '857095216', '857095220', '857095217', '857095212', '857095215', '857095214', '857095213', '857095211', '860994787', '860994786', '860994788', '860994789', '860994790', '860994791', '860994792', '860994795', '860994794', '860994793', '860994797', '860994796', '860994942', '860994941', '860994940', '860994945', '860994943', '860994944', '860994949', '860994946', '860994947', '860994948', '860994950', '860994951', '865597886', '865597885', '865597888', '865597887', '865597889', '865597891', '865597890', '865597893', '865597894', '865597892', '865597896', '865597895', '865598207', '865598202', '865598203', '865598205', '865598209', '865598212', '865598216', '865598214', '865598218', '865598222', '865598220', '865598223', '869979069', '869979066', '869979067', '869979068', '869979072', '869979070', '869979071', '869979076', '869979073', '869979075', '869979074', '869979077', '870678604', '870678602', '870678606', '870678607', '870678596', '870678605', '870678603', '870678599', '870678600', '870678601', '870678598', '870678597', '870434867', '870434868', '870434869', '870434865', '870434866', '870434864', '870434861', '870434862', '870434863', '870434859', '870434860', '875176076', '875176074', '875176079', '875176075', '875176077', '875176078', '875176080', '875176081', '875176082', '875176085', '875176083', '875176084', '877657956', '877657957', '877657958', '877657960', '877657959', '877657961', '877657965', '877657962', '877657964', '877657963', '877657966', '877657967', '877730875', '877730874', '877730876', '877730877', '877730880', '877730878', '877730879', '877730881', '877730882', '877730883', '877730885', '877730884', '881692512', '881692510', '881692509', '881692517', '881692516', '881692513', '881692521', '881692520', '881692525', '881692523', '884739463', '884739462', '884739468', '884739464', '884739465', '884739466', '884739467', '884739471', '884739470', '884739469', '884739473', '884739472', '884739793', '884739792', '884739791', '884739796', '884739794', '884739795', '884739801', '884739798', '884739797', '884739799', '884739800', '889068108', '889068110', '889068109', '889068101', '889068106', '889068090', '889068104', '889068097', '889068095', '889068099', '889068092', '889068093', '888959244', '888959245', '888959243', '888959239', '888959242', '888959240', '888959241', '888959238', '888959235', '888959237', '888959236', '888959234', '892812677', '892812676', '892812675', '892812678', '892812679', '892812680', '892812683', '892812681', '892812682', '892812684', '892812685', '894754583', '894754582', '894754581', '894754584', '894754585', '894754586', '894754590', '894754588', '894754587', '894754589', '898230387', '898230386', '898230390', '898230385', '898230388', '898230389', '898230393', '898230391', '898230392', '898230394', '899363286', '899363290', '899363285', '899363287', '899363288', '899363289', '899363292', '899363291', '899363293', '899363294', '899363296', '899363295', '902043580', '902043579', '902043578', '902043582', '902043581', '902043583', '902043585', '902043587', '902043584', '902043586', '906204359', '906204358', '906204357', '906204362', '906204361', '906204360', '906204364', '906204363', '906204365', '906204366', '906606939', '906606934', '906606936', '906606935', '906606937', '906606938', '906606940', '906606941', '906606942', '906606945', '906606944', '906606943', '910518576', '910518573', '910518579', '910518575', '910518574', '910518577', '910518578', '910518581', '910518580', '910518582', '910518583', '910518584', '914184267', '914184265', '914184270', '914184266', '914184268', '914184269', '914184275', '914184272', '914184271', '914184273', '914184274', '914184276', '914184549', '914184548', '914184552', '914184550', '914184551', '914184556', '914184555', '914184554', '914184553', '914184557', '914186702', '914186707', '914186704', '914186703', '914186705', '914186706', '914186709', '914186710', '914186708', '914186712', '914186711', '920302567', '920302572', '920302568', '920302569', '920302571', '920302570', '920302575', '920302574', '920302573', '920302578', '920302576', '920302577', '920302813', '920302817', '920302812', '920302814', '920302815', '920302816', '920302820', '920302818', '920302819', '920302822', '920302821', '920302823', '926131117', '926131122', '926131118', '926131119', '926131121', '926131120', '926131125', '926131123', '926131124', '926131126', '926131127', '928568989', '928568988', '928568990', '928568993', '928568991', '928568992', '928568995', '928568994', '928568996', '928568997', '928568999', '928568998', '928569355', '928569354', '928569353', '928569358', '928569357', '928569356', '928569363', '928569362', '928569360', '928569359', '928569361', '932723012', '932723014', '932723011', '932723017', '932723013', '932723015', '932723016', '932723021', '932723018', '932723019', '932723020', '932723022', '936077467', '936077466', '936077465', '936077468', '936077469', '936077471', '936077470', '936077472', '936077474', '936077473', '936077719', '936077720', '936077723', '936077721', '936077722', '936077725', '936077724', '936077726', '936077729', '936077728', '936077727', '936077730', '941174598', '941174593', '941174594', '941174595', '941174596', '941174597', '941174601', '941174599', '941174600', '941174602', '941174603', '941174604', '941174824', '941174819', '941174820', '941174821', '941174823', '941174822', '941174828', '941174829', '941174827', '941174825', '941174826', '941174830', '945175777', '945175776', '945175775', '945175780', '945175779', '945175778', '945175782', '945175781', '945175784', '945175783', '947071077', '947071078', '947071079', '947071075', '947071070', '947071074', '947071076', '947071073', '947071071', '947071072', '946805263', '946805258', '946805262', '946805261', '946805260', '946805259', '946805256', '946805257', '946805253', '946805255', '946805254', '946805252', '952249393', '952249399', '952249395', '952249400', '952249397', '952249389', '952249390', '952249391', '952249387', '952249385', '950924026', '950924027', '950924023', '950924028', '950924019', '950924020', '950924025', '950924024', '950924018', '950924022', '950924029', '950924021', '956333623', '956333618', '956333617', '956333619', '956333620', '956333621', '956333622', '956333624', '956333626', '956333625', '956333627', '956333628', '961048127', '961048130', '961048128', '961048129', '961048131', '961048132', '961048138', '961048136', '961048135', '961048133', '961048134', '961048137', '965357034', '965357039', '965357033', '965357036', '965357035', '965357037', '965357038', '965357043', '965357040', '965357042', '965357041', '965357044', '972129378', '972129384', '972129381', '972129379', '972129380', '972129383', '972129382', '972129386', '972129385', '972129387', '972129388', '972129389', '975054779', '975054778', '975054784', '975054780', '975054781', '975054782', '975054783', '975054786', '975054787', '975054785', '975054789', '975054788', '975055041', '975055042', '975055043', '975055044', '975055045', '975055046', '975055047', '975055051', '975055049', '975055048', '975055050', '975055052', '979489266', '979489268', '979489269', '979489272', '979489267', '979489271', '979489270', '979489273', '979489274', '979489275', '979489277', '979489276', '981336457', '981336456', '981336455', '981336460', '981336458', '981336459', '981336463', '981336462', '981336461', '981336464', '981336465', '981336718', '981336723', '981336719', '981336720', '981336721', '981336722', '981336726', '981336724', '981336731', '981336728', '981336729', '987309823', '987309822', '987309821', '987309825', '987309826', '987309824', '987309831', '987309827', '987309829', '987309828', '987309830', '987309832', '990396091', '990396089', '990396094', '990396090', '990396092', '990396093', '990396096', '990396097', '990396095', '990396100', '990396098', '990396099', '994164380', '994164379', '994164377', '994164378', '994164382', '994164381', '994164384', '994164383', '994164388', '994164385', '994164386', '994164387', '996378353', '996378357', '996378347', '996378355', '996378356', '996378346', '996378354', '996378350', '996378351', '996378352', '996378348', '996378349', '996370533', '996370534', '996370530', '996370532', '996370531', '996370529', '996370527', '996370526', '996370528', '996370524', '996370525', '998752257', '998752256', '998752255', '998752254', '998752259', '998752258', '998752260', '998752261', '998752262', '998752263', '998752264', '1002606755', '1002606760', '1002606758', '1002606756', '1002606757', '1002606759', '1002606761', '1002606762', '1002606763', '1002606766', '1002606764', '1002606765', '1005120292', '1005120291', '1005120290', '1005120295', '1005120294', '1005120293', '1005120298', '1005120296', '1005120297', '1005120300', '1005120299', '1007920329', '1007920330', '1007920331', '1010029057', '1010029051', '1010029052', '1010029053', '1010029054', '1010029056', '1010029055', '1010029060', '1010029058', '1010029059', '1010029062', '1010029061', '1012443383', '1012443378', '1012443379', '1012443380', '1012443382', '1012443381', '1012443389', '1012443386', '1012443384', '1012443385', '1012443388', '1012443387', '1014813102', '1014813101', '1014813107', '1014813103', '1014813104', '1014813105', '1014813106', '1014813109', '1014813108', '1014813112', '1014813110', '1014813111', '1018048559', '1018048557', '1018048556', '1018048555', '1018048560', '1018048558', '1018048563', '1018048565', '1018048562', '1018048561', '1018048564', '1021616179', '1021616181', '1021616184', '1021616180', '1021616182', '1021616183', '1021616188', '1021616189', '1021616185', '1021616186', '1021616187', '1021616190', '1025683825', '1025683809', '1025683810', '1025683811', '1025683813', '1025683814', '1025683812', '1025683817', '1025683815', '1025683816', '1025683820', '1025683822']\n",
      "data=<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000020E4F3372D0>\n",
      "number of races = 2138, number of unique dogs = 2562\n",
      "0       (Murray Bridge Straight, 300)\n",
      "1       (Murray Bridge Straight, 300)\n",
      "2       (Murray Bridge Straight, 300)\n",
      "3       (Murray Bridge Straight, 300)\n",
      "4       (Murray Bridge Straight, 300)\n",
      "                    ...              \n",
      "2133    (Murray Bridge Straight, 350)\n",
      "2134    (Murray Bridge Straight, 300)\n",
      "2135    (Murray Bridge Straight, 300)\n",
      "2136    (Murray Bridge Straight, 350)\n",
      "2137    (Murray Bridge Straight, 350)\n",
      "Length: 2138, dtype: object\n"
     ]
    }
   ],
   "source": [
    "os.getcwd()\n",
    "import rnn_tools.raceDB\n",
    "importlib.reload(rnn_tools.raceDB)\n",
    "# os.chdir(r\"C:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\Python\\DATA\")                                                                                                                                                                                                                                                                                                                \n",
    "date = datetime.datetime.strptime(\"2019-01-01\", \"%Y-%m-%d\").date()\n",
    "hidden_size = 32\n",
    "states = [\"NZ\",\"SA\",\"WA\"]\n",
    "# states = [\"VIC\"]\n",
    "# states = [\"NSW\"]\n",
    "states = [\"NZ\"]\n",
    "states = [\"SA\"]\n",
    "# states = ['VIC',  \"SA\"]\n",
    "extra_tracks = [\"Wentworth Park\", \"Wagga\", \"Temora\"]\n",
    "track = ['Murray Bridge Straight']\n",
    "data_file = './data/gru_inputs_kitchen_sink_new_not_simple.fth'\n",
    "# data_file = './data/gru_inputs_kitchen_sink_april.fth'\n",
    "data_file = './data/topaz_data_w_bsp_new3.fth'\n",
    "# data_file = './data/topaz_data_w_bsp_new_w_price.fth'\n",
    "# data_file = './data/topaz_data_w_bsp.fth'\n",
    "# data_file = './data/gru_inputs_kitchen_sink_topaz.fth'\n",
    "raceDB = rnn_tools.raceDB.build_dataset_topaz(data_file, hidden_size,state_filter=states,date_filter=date, track_filter=track,margin_type='sftmin',v6=True,show_stats=False)\n",
    "# raceDB = rnn_tools.raceDB.build_dataset_topaz(data_file, hidden_size,state_filter=states,track_filter=extra_tracks,date_filter=date, margin_type='sftmin',v6=True,show_stats=False)\n",
    "raceDB.create_new_weights_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['dam_finishingPlaceMovement_max_28D',\n",
    " 'dam_finishingPlaceMovement_max_365D',\n",
    " 'dam_finishingPlaceMovement_max_91D',\n",
    " 'dam_finishingPlaceMovement_mean_28D',\n",
    " 'dam_finishingPlaceMovement_mean_365D',\n",
    " 'dam_finishingPlaceMovement_mean_91D',\n",
    " 'dam_finishingPlaceMovement_median_28D',\n",
    " 'dam_finishingPlaceMovement_median_365D',\n",
    " 'dam_finishingPlaceMovement_median_91D',\n",
    " 'dam_finishingPlaceMovement_min_28D',\n",
    " 'dam_finishingPlaceMovement_min_365D',\n",
    " 'dam_finishingPlaceMovement_min_91D',\n",
    " 'dam_finishingPlaceMovement_std_28D',\n",
    " 'dam_finishingPlaceMovement_std_365D',\n",
    " 'dam_finishingPlaceMovement_std_91D',\n",
    " 'dam_marginLog_max_28D',\n",
    " 'dam_marginLog_max_365D',\n",
    " 'dam_marginLog_max_91D',\n",
    " 'dam_marginLog_mean_28D',\n",
    " 'dam_marginLog_mean_365D',\n",
    " 'dam_marginLog_mean_91D',\n",
    " 'dam_marginLog_median_28D',\n",
    " 'dam_marginLog_median_365D',\n",
    " 'dam_marginLog_median_91D',\n",
    " 'dam_marginLog_min_28D',\n",
    " 'dam_marginLog_min_365D',\n",
    " 'dam_marginLog_min_91D',\n",
    " 'dam_marginLog_std_28D',\n",
    " 'dam_marginLog_std_365D',\n",
    " 'dam_marginLog_std_91D',\n",
    " 'dam_placeLog_max_28D',\n",
    " 'dam_placeLog_max_365D',\n",
    " 'dam_placeLog_max_91D',\n",
    " 'dam_placeLog_mean_28D',\n",
    " 'dam_placeLog_mean_365D',\n",
    " 'dam_placeLog_mean_91D',\n",
    " 'dam_placeLog_median_28D',\n",
    " 'dam_placeLog_median_365D',\n",
    " 'dam_placeLog_median_91D',\n",
    " 'dam_placeLog_min_28D',\n",
    " 'dam_placeLog_min_365D',\n",
    " 'dam_placeLog_min_91D',\n",
    " 'dam_placeLog_std_28D',\n",
    " 'dam_placeLog_std_365D',\n",
    " 'dam_placeLog_std_91D',\n",
    " 'dam_prizemoneyLog_max_28D',\n",
    " 'dam_prizemoneyLog_max_365D',\n",
    " 'dam_prizemoneyLog_max_91D',\n",
    " 'dam_prizemoneyLog_mean_28D',\n",
    " 'dam_prizemoneyLog_mean_365D',\n",
    " 'dam_prizemoneyLog_mean_91D',\n",
    " 'dam_prizemoneyLog_median_28D',\n",
    " 'dam_prizemoneyLog_median_365D',\n",
    " 'dam_prizemoneyLog_median_91D',\n",
    " 'dam_prizemoneyLog_min_28D',\n",
    " 'dam_prizemoneyLog_min_365D',\n",
    " 'dam_prizemoneyLog_min_91D',\n",
    " 'dam_prizemoneyLog_std_28D',\n",
    " 'dam_prizemoneyLog_std_365D',\n",
    " 'dam_prizemoneyLog_std_91D',\n",
    " 'dam_runTimeNorm_max_28D',\n",
    " 'dam_runTimeNorm_max_365D',\n",
    " 'dam_runTimeNorm_max_91D',\n",
    " 'dam_runTimeNorm_mean_28D',\n",
    " 'dam_runTimeNorm_mean_365D',\n",
    " 'dam_runTimeNorm_mean_91D',\n",
    " 'dam_runTimeNorm_median_28D',\n",
    " 'dam_runTimeNorm_median_365D',\n",
    " 'dam_runTimeNorm_median_91D',\n",
    " 'dam_runTimeNorm_min_28D',\n",
    " 'dam_runTimeNorm_min_365D',\n",
    " 'dam_runTimeNorm_min_91D',\n",
    " 'dam_runTimeNorm_std_28D',\n",
    " 'dam_runTimeNorm_std_365D',\n",
    " 'dam_runTimeNorm_std_91D',\n",
    " 'dog_finishingPlaceMovement_max_28D',\n",
    " 'dog_finishingPlaceMovement_max_365D',\n",
    " 'dog_finishingPlaceMovement_max_91D',\n",
    " 'dog_finishingPlaceMovement_mean_28D',\n",
    " 'dog_finishingPlaceMovement_mean_365D',\n",
    " 'dog_finishingPlaceMovement_mean_91D',\n",
    " 'dog_finishingPlaceMovement_median_28D',\n",
    " 'dog_finishingPlaceMovement_median_365D',\n",
    " 'dog_finishingPlaceMovement_median_91D',\n",
    " 'dog_finishingPlaceMovement_min_28D',\n",
    " 'dog_finishingPlaceMovement_min_365D',\n",
    " 'dog_finishingPlaceMovement_min_91D',\n",
    " 'dog_finishingPlaceMovement_std_28D',\n",
    " 'dog_finishingPlaceMovement_std_365D',\n",
    " 'dog_finishingPlaceMovement_std_91D',\n",
    " 'dog_marginLog_max_28D',\n",
    " 'dog_marginLog_max_365D',\n",
    " 'dog_marginLog_max_91D',\n",
    " 'dog_marginLog_mean_28D',\n",
    " 'dog_marginLog_mean_365D',\n",
    " 'dog_marginLog_mean_91D',\n",
    " 'dog_marginLog_median_28D',\n",
    " 'dog_marginLog_median_365D',\n",
    " 'dog_marginLog_median_91D',\n",
    " 'dog_marginLog_min_28D',\n",
    " 'dog_marginLog_min_365D',\n",
    " 'dog_marginLog_min_91D',\n",
    " 'dog_marginLog_std_28D',\n",
    " 'dog_marginLog_std_365D',\n",
    " 'dog_marginLog_std_91D',\n",
    " 'dog_placeLog_max_28D',\n",
    " 'dog_placeLog_max_365D',\n",
    " 'dog_placeLog_max_91D',\n",
    " 'dog_placeLog_mean_28D',\n",
    " 'dog_placeLog_mean_365D',\n",
    " 'dog_placeLog_mean_91D',\n",
    " 'dog_placeLog_median_28D',\n",
    " 'dog_placeLog_median_365D',\n",
    " 'dog_placeLog_median_91D',\n",
    " 'dog_placeLog_min_28D',\n",
    " 'dog_placeLog_min_365D',\n",
    " 'dog_placeLog_min_91D',\n",
    " 'dog_placeLog_std_28D',\n",
    " 'dog_placeLog_std_365D',\n",
    " 'dog_placeLog_std_91D',\n",
    " 'dog_prizemoneyLog_max_28D',\n",
    " 'dog_prizemoneyLog_max_365D',\n",
    " 'dog_prizemoneyLog_max_91D',\n",
    " 'dog_prizemoneyLog_mean_28D',\n",
    " 'dog_prizemoneyLog_mean_365D',\n",
    " 'dog_prizemoneyLog_mean_91D',\n",
    " 'dog_prizemoneyLog_median_28D',\n",
    " 'dog_prizemoneyLog_median_365D',\n",
    " 'dog_prizemoneyLog_median_91D',\n",
    " 'dog_prizemoneyLog_min_28D',\n",
    " 'dog_prizemoneyLog_min_365D',\n",
    " 'dog_prizemoneyLog_min_91D',\n",
    " 'dog_prizemoneyLog_std_28D',\n",
    " 'dog_prizemoneyLog_std_365D',\n",
    " 'dog_prizemoneyLog_std_91D',\n",
    " 'dog_runTimeNorm_max_28D',\n",
    " 'dog_runTimeNorm_max_365D',\n",
    " 'dog_runTimeNorm_max_91D',\n",
    " 'dog_runTimeNorm_mean_28D',\n",
    " 'dog_runTimeNorm_mean_365D',\n",
    " 'dog_runTimeNorm_mean_91D',\n",
    " 'dog_runTimeNorm_median_28D',\n",
    " 'dog_runTimeNorm_median_365D',\n",
    " 'dog_runTimeNorm_median_91D',\n",
    " 'dog_runTimeNorm_min_28D',\n",
    " 'dog_runTimeNorm_min_365D',\n",
    " 'dog_runTimeNorm_min_91D',\n",
    " 'dog_runTimeNorm_std_28D',\n",
    " 'dog_runTimeNorm_std_365D',\n",
    " 'dog_runTimeNorm_std_91D',\n",
    " 'sire_finishingPlaceMovement_max_28D',\n",
    " 'sire_finishingPlaceMovement_max_365D',\n",
    " 'sire_finishingPlaceMovement_max_91D',\n",
    " 'sire_finishingPlaceMovement_mean_28D',\n",
    " 'sire_finishingPlaceMovement_mean_365D',\n",
    " 'sire_finishingPlaceMovement_mean_91D',\n",
    " 'sire_finishingPlaceMovement_median_28D',\n",
    " 'sire_finishingPlaceMovement_median_365D',\n",
    " 'sire_finishingPlaceMovement_median_91D',\n",
    " 'sire_finishingPlaceMovement_min_28D',\n",
    " 'sire_finishingPlaceMovement_min_365D',\n",
    " 'sire_finishingPlaceMovement_min_91D',\n",
    " 'sire_finishingPlaceMovement_std_28D',\n",
    " 'sire_finishingPlaceMovement_std_365D',\n",
    " 'sire_finishingPlaceMovement_std_91D',\n",
    " 'sire_marginLog_max_28D',\n",
    " 'sire_marginLog_max_365D',\n",
    " 'sire_marginLog_max_91D',\n",
    " 'sire_marginLog_mean_28D',\n",
    " 'sire_marginLog_mean_365D',\n",
    " 'sire_marginLog_mean_91D',\n",
    " 'sire_marginLog_median_28D',\n",
    " 'sire_marginLog_median_365D',\n",
    " 'sire_marginLog_median_91D',\n",
    " 'sire_marginLog_min_28D',\n",
    " 'sire_marginLog_min_365D',\n",
    " 'sire_marginLog_min_91D',\n",
    " 'sire_marginLog_std_28D',\n",
    " 'sire_marginLog_std_365D',\n",
    " 'sire_marginLog_std_91D',\n",
    " 'sire_placeLog_max_28D',\n",
    " 'sire_placeLog_max_365D',\n",
    " 'sire_placeLog_max_91D',\n",
    " 'sire_placeLog_mean_28D',\n",
    " 'sire_placeLog_mean_365D',\n",
    " 'sire_placeLog_mean_91D',\n",
    " 'sire_placeLog_median_28D',\n",
    " 'sire_placeLog_median_365D',\n",
    " 'sire_placeLog_median_91D',\n",
    " 'sire_placeLog_min_28D',\n",
    " 'sire_placeLog_min_365D',\n",
    " 'sire_placeLog_min_91D',\n",
    " 'sire_placeLog_std_28D',\n",
    " 'sire_placeLog_std_365D',\n",
    " 'sire_placeLog_std_91D',\n",
    " 'sire_prizemoneyLog_max_28D',\n",
    " 'sire_prizemoneyLog_max_365D',\n",
    " 'sire_prizemoneyLog_max_91D',\n",
    " 'sire_prizemoneyLog_mean_28D',\n",
    " 'sire_prizemoneyLog_mean_365D',\n",
    " 'sire_prizemoneyLog_mean_91D',\n",
    " 'sire_prizemoneyLog_median_28D',\n",
    " 'sire_prizemoneyLog_median_365D',\n",
    " 'sire_prizemoneyLog_median_91D',\n",
    " 'sire_prizemoneyLog_min_28D',\n",
    " 'sire_prizemoneyLog_min_365D',\n",
    " 'sire_prizemoneyLog_min_91D',\n",
    " 'sire_prizemoneyLog_std_28D',\n",
    " 'sire_prizemoneyLog_std_365D',\n",
    " 'sire_prizemoneyLog_std_91D',\n",
    " 'sire_runTimeNorm_max_28D',\n",
    " 'sire_runTimeNorm_max_365D',\n",
    " 'sire_runTimeNorm_max_91D',\n",
    " 'sire_runTimeNorm_mean_28D',\n",
    " 'sire_runTimeNorm_mean_365D',\n",
    " 'sire_runTimeNorm_mean_91D',\n",
    " 'sire_runTimeNorm_median_28D',\n",
    " 'sire_runTimeNorm_median_365D',\n",
    " 'sire_runTimeNorm_median_91D',\n",
    " 'sire_runTimeNorm_min_28D',\n",
    " 'sire_runTimeNorm_min_365D',\n",
    " 'sire_runTimeNorm_min_91D',\n",
    " 'sire_runTimeNorm_std_28D',\n",
    " 'sire_runTimeNorm_std_365D',\n",
    " 'sire_runTimeNorm_std_91D',\n",
    " 'trainer_finishingPlaceMovement_max_28D',\n",
    " 'trainer_finishingPlaceMovement_max_365D',\n",
    " 'trainer_finishingPlaceMovement_max_91D',\n",
    " 'trainer_finishingPlaceMovement_mean_28D',\n",
    " 'trainer_finishingPlaceMovement_mean_365D',\n",
    " 'trainer_finishingPlaceMovement_mean_91D',\n",
    " 'trainer_finishingPlaceMovement_median_28D',\n",
    " 'trainer_finishingPlaceMovement_median_365D',\n",
    " 'trainer_finishingPlaceMovement_median_91D',\n",
    " 'trainer_finishingPlaceMovement_min_28D',\n",
    " 'trainer_finishingPlaceMovement_min_365D',\n",
    " 'trainer_finishingPlaceMovement_min_91D',\n",
    " 'trainer_finishingPlaceMovement_std_28D',\n",
    " 'trainer_finishingPlaceMovement_std_365D',\n",
    " 'trainer_finishingPlaceMovement_std_91D',\n",
    " 'trainer_marginLog_max_28D',\n",
    " 'trainer_marginLog_max_365D',\n",
    " 'trainer_marginLog_max_91D',\n",
    " 'trainer_marginLog_mean_28D',\n",
    " 'trainer_marginLog_mean_365D',\n",
    " 'trainer_marginLog_mean_91D',\n",
    " 'trainer_marginLog_median_28D',\n",
    " 'trainer_marginLog_median_365D',\n",
    " 'trainer_marginLog_median_91D',\n",
    " 'trainer_marginLog_min_28D',\n",
    " 'trainer_marginLog_min_365D',\n",
    " 'trainer_marginLog_min_91D',\n",
    " 'trainer_marginLog_std_28D',\n",
    " 'trainer_marginLog_std_365D',\n",
    " 'trainer_marginLog_std_91D',\n",
    " 'trainer_placeLog_max_28D',\n",
    " 'trainer_placeLog_max_365D',\n",
    " 'trainer_placeLog_max_91D',\n",
    " 'trainer_placeLog_mean_28D',\n",
    " 'trainer_placeLog_mean_365D',\n",
    " 'trainer_placeLog_mean_91D',\n",
    " 'trainer_placeLog_median_28D',\n",
    " 'trainer_placeLog_median_365D',\n",
    " 'trainer_placeLog_median_91D',\n",
    " 'trainer_placeLog_min_28D',\n",
    " 'trainer_placeLog_min_365D',\n",
    " 'trainer_placeLog_min_91D',\n",
    " 'trainer_placeLog_std_28D',\n",
    " 'trainer_placeLog_std_365D',\n",
    " 'trainer_placeLog_std_91D',\n",
    " 'trainer_prizemoneyLog_max_28D',\n",
    " 'trainer_prizemoneyLog_max_365D',\n",
    " 'trainer_prizemoneyLog_max_91D',\n",
    " 'trainer_prizemoneyLog_mean_28D',\n",
    " 'trainer_prizemoneyLog_mean_365D',\n",
    " 'trainer_prizemoneyLog_mean_91D',\n",
    " 'trainer_prizemoneyLog_median_28D',\n",
    " 'trainer_prizemoneyLog_median_365D',\n",
    " 'trainer_prizemoneyLog_median_91D',\n",
    " 'trainer_prizemoneyLog_min_28D',\n",
    " 'trainer_prizemoneyLog_min_365D',\n",
    " 'trainer_prizemoneyLog_min_91D',\n",
    " 'trainer_prizemoneyLog_std_28D',\n",
    " 'trainer_prizemoneyLog_std_365D',\n",
    " 'trainer_prizemoneyLog_std_91D',\n",
    " 'trainer_runTimeNorm_max_28D',\n",
    " 'trainer_runTimeNorm_max_365D',\n",
    " 'trainer_runTimeNorm_max_91D',\n",
    " 'trainer_runTimeNorm_mean_28D',\n",
    " 'trainer_runTimeNorm_mean_365D',\n",
    " 'trainer_runTimeNorm_mean_91D',\n",
    " 'trainer_runTimeNorm_median_28D',\n",
    " 'trainer_runTimeNorm_median_365D',\n",
    " 'trainer_runTimeNorm_median_91D',\n",
    " 'trainer_runTimeNorm_min_28D',\n",
    " 'trainer_runTimeNorm_min_365D',\n",
    " 'trainer_runTimeNorm_min_91D',\n",
    " 'trainer_runTimeNorm_std_28D',\n",
    " 'trainer_runTimeNorm_std_365D',\n",
    " 'trainer_runTimeNorm_std_91D',\n",
    " 'dogAgeScaled',\n",
    " 'lastFiveWinPercentage',\n",
    " 'lastFivePlacePercentage',\n",
    " 'weightInKgScaled',\n",
    " 'rolling_box_win_percentage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure(optimizer, criterion, outs, classes):\n",
    "    optimizer.zero_grad()\n",
    "    loss = nn.functional.mse_loss(outs, classes)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "def load_matching_state_dict(model, state_dict):\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter out unnecessary keys\n",
    "    matching_keys = {k: v for k, v in state_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "    excluded_keys = set(state_dict.keys()) - set(matching_keys.keys())\n",
    "\n",
    "    # Overwrite entries in the existing state dict\n",
    "    model_dict.update(matching_keys)\n",
    "\n",
    "    # Load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    print(\"Loaded keys:\")\n",
    "    for key in matching_keys:\n",
    "        print(key)\n",
    "\n",
    "    print(\"\\nExcluded keys:\")\n",
    "    for key in excluded_keys:\n",
    "        print(key)\n",
    "\n",
    "\n",
    "def squash_state_dict(model, state_dict):\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter out unnecessary keys\n",
    "    matching_keys = {k: v for k, v in state_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "    excluded_keys = set(state_dict.keys()) - set(matching_keys.keys())\n",
    "\n",
    "    \n",
    "\n",
    "    # Overwrite entries in the existing state dict\n",
    "    model_dict.update(matching_keys)\n",
    "\n",
    "    # Load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    print(\"Loaded keys:\")\n",
    "    for key in matching_keys:\n",
    "        print(key, model_dict[key].size())\n",
    "\n",
    "    squashed_keys = {}\n",
    "    print(\"\\nExcluded keys:\")\n",
    "    for key in excluded_keys:\n",
    "\n",
    "        print(key, model_dict[key].size(), state_dict[key].size())\n",
    "        try:\n",
    "            squashed_keys[key] = state_dict[key][model_dict[key].size()[0]:, model_dict[key].size()[1]]\n",
    "        except Exception as e:\n",
    "            print('failed', key)\n",
    "\n",
    "        try:\n",
    "            squashed_keys[key] = state_dict[key][model_dict[key].size()[0]:]\n",
    "        except Exception as e:\n",
    "            print('failed', key)\n",
    "    print(squashed_keys)\n",
    "    model_dict.update(squashed_keys)\n",
    "\n",
    "def model_pipeline(my_dataset=raceDB,config=None,prev_model=None, sweep=True, model_state_dict=None, prev_model_file=None, prev_model_version='450'):\n",
    "    if my_dataset:\n",
    "      dataset = my_dataset    \n",
    "    else:\n",
    "      dataset = raceDB\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"NEW GRU V7 Reporting\", config=config,save_code=False):\n",
    "      #  access all HPs through wandb.config, so logging matches execution!\n",
    "      wandb.define_metric(\"loss_val\", summary=\"min\")\n",
    "      wandb.define_metric(\"accuracy\", summary=\"max\")\n",
    "      wandb.define_metric(\"ROI < 30\", summary=\"max\")\n",
    "      wandb.define_metric(\"val_ROI < 30\", summary=\"max\")\n",
    "      wandb.define_metric(\"relu roi\", summary=\"max\")\n",
    "      \n",
    "      config = wandb.config\n",
    "      pprint.pprint(config)\n",
    "      pprint.pprint(config.epochs)\n",
    "      print(config)\n",
    "      wandb.run.log_code('rnn_tools/')\n",
    "      # input_size = raceDB.get_race_input([0,1])[0].full_input.shape[0] #create fix so messy\n",
    "      print(config.input_type)\n",
    "\n",
    "      for i in config.stat_list_dict.values():\n",
    "        print(i)\n",
    "\n",
    "      reg_stat_mask = []\n",
    "      for stat,flag in config.stat_list_dict.items():\n",
    "          stat_flag = flag\n",
    "          reg_stat_mask = reg_stat_mask+[stat_flag]\n",
    "\n",
    "      print(reg_stat_mask)\n",
    "      stat_mask = [1]*6+reg_stat_mask+[0]*80\n",
    "      # stat_mask = [1]*30\n",
    "      stat_mask = [1]*18+36*[0]\n",
    "      data_mask = [0]*26+[1]*20+[1]*20+[1]*20+[1]*20 # Reg, Dist, Box, T_box, T_dist\n",
    "      data_mask = [1]*6+reg_stat_mask+[0]*20+[0]*20+[0]*20+[0]*20 # Reg, Dist, Box,T_box, T_dist\n",
    "      data_mask =  [0]*18+36*[1]\n",
    "      # data_mask =  [1]*30\n",
    "      data_mask_size = sum(data_mask)\n",
    "      print(f\"{data_mask_size=}\")\n",
    "      print(f\"{data_mask=}\")\n",
    "      stat_mask = torch.tensor(stat_mask).type(torch.bool).to(device)\n",
    "      data_mask = torch.tensor(data_mask).type(torch.bool).to(device)\n",
    "      # stat_mask = torch.ones_like(stat_mask).type(torch.bool).to(device)\n",
    "      input_size = sum(stat_mask)\n",
    "\n",
    "      print(stat_mask)\n",
    "      print(stat_mask.shape)\n",
    "\n",
    "\n",
    "      if 'batch_days' in config.keys() and not raceDB.batches_setup:\n",
    "        pass\n",
    "        raceDB.create_test_split_date(config['training_date_end'],val_date='2023-07-01')\n",
    "        raceDB.create_dogs_test_split_date()\n",
    "        raceDB.attach_races_to_dog_inputv2() \n",
    "        raceDB.reset_hidden()\n",
    "        raceDB.create_batches(batch_days=config['batch_days'], end_date = config['training_date_end'], stat_mask=stat_mask,data_mask=data_mask,gen_packed_seq=True)\n",
    "        raceDB.batches_setup = True\n",
    "      if config['input_type'] == 'basic':\n",
    "          print('here')\n",
    "          raceDB.batches['packed_x'] = raceDB.batches['packed_x_basic']\n",
    "          raceDB.batches['packed_y'] = raceDB.batches['packed_y_basic']\n",
    "          raceDB.batches['packed_v'] = raceDB.batches['packed_v_basic']\n",
    "          # input_size = raceDB.batches['packed_x'][0].data[0].shape[0]\n",
    "\n",
    "\n",
    "\n",
    "      print(f\"{input_size=}\")\n",
    "      config['stat_mask_tensor'] = torch.tensor(stat_mask, dtype=torch.uint8).to(device)     \n",
    "      raceDB.reset_hidden(num_layers=config['num_layers'], hidden_size=config['hidden_size'])\n",
    "      model = rnn_classes.GRUNetv3_extra_embedding(input_size,config['hidden_size'], num_layers=config['num_layers'],fc0_size=config['f0_layer_size'], fc1_size=config['f1_layer_size'],data_mask_size=data_mask_size)\n",
    "      optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "      \n",
    "      config['model_name'] = model.__class__.__name__\n",
    "      if model_state_dict:\n",
    "        model.load_state_dict(model_state_dict)\n",
    "      if prev_model_file!=None:\n",
    "        print(f\"Loading model {prev_model_file}, version {prev_model_version}\")\n",
    "        model_name = prev_model_file\n",
    "        model_loc = f\"models/savedmodel/{model_name}/{model_name}_{prev_model_version}.pt\"\n",
    "        model_data = torch.load(model_loc,map_location=torch.device('cuda:0'))\n",
    "        print(model_data['model_state_dict'].keys())\n",
    "        # model.load_state_dict(model_data['model_state_dict'], strict=False)\n",
    "        load_matching_state_dict(model, model_data['model_state_dict'])\n",
    "        config['parent model'] = prev_model_file\n",
    "        # raceDB.fill_hidden_states_dict_v2(model_data['db_train'])\n",
    "        model = model.to(device)\n",
    "        # raceDB.load_hidden_in_dict(hidden_in_dict=model_data['hidden_ins'],output_dict=model_data['output'])\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "        # optimizer.load_state_dict(model_data['optim'])\n",
    "      else:\n",
    "        optimizer = optim.RAdam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "      raceDB.to_cuda()\n",
    "      criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "      wandb.run.name = f\"{' '.join(states)}-{wandb.run.name}\"\n",
    "      model = model.to(device)\n",
    "      print(model)\n",
    "\n",
    "      try:\n",
    "        return training_testing_gru_extra_data_embedding.train_double_v3(model, dataset, criterion, optimizer, 'na', config)\n",
    "      except (KeyboardInterrupt) as error:\n",
    "        print(error)\n",
    "        print(\"finished Early\")\n",
    "        \n",
    "      raceDB.create_hidden_states_dict_v2()\n",
    "      model_saver_wandb(model, optimizer, 450, 0.1, raceDB.hidden_states_dict_gru_v6,raceDB.train_hidden_dict , model_name=\"long nsw new  22000 RUN\")\n",
    "\n",
    "\n",
    "    return (model,dataset, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raceDB.batches_setup = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_list_dict = {'_dist_last__1':1,\n",
    "    '_box_last__1':1,\n",
    "    '_speed_avg_1':1,\n",
    "    '_split_speed_avg_1':0,\n",
    "    '_split_margin_avg_1':1,\n",
    "    '_margin_avg_1':1,\n",
    "    '_margin_time_avg_1':1,\n",
    "    '_RunHomeTime_1':1,\n",
    "    '_run_home_speed_1':1,\n",
    "    '_first_out_avg_1':0,\n",
    "    '_pos_out_avg_1':0,\n",
    "    '_post_change_avg_1':0,\n",
    "    '_races_1':1,\n",
    "    '_wins_1':1,\n",
    "    '_wins_last_1':1,\n",
    "    '_weight_':1,\n",
    "    '_min_time_':1,\n",
    "    '_min_split_time_':1,\n",
    "    '_last_start_price':0,\n",
    "    '_last_start_prob':0,\n",
    "}\n",
    "\n",
    "wandb_config_static = {\n",
    "    'hidden_size': 512,\n",
    "    'stats': raceDB.stats_cols,\n",
    "    'races': states,\n",
    "    'datafile': data_file,\n",
    "    'latest_date': raceDB.latest_date,\n",
    "    'input_type': 'basic',\n",
    "    'num_layers': 2,\n",
    "    'batch_size': 750,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 2500,\n",
    "    'learning_rate': 0.0001,\n",
    "    'optimizer': 'adamW',\n",
    "    'f0_layer_size': 128,\n",
    "    'f1_layer_size': 64,\n",
    "    'training_date_end': '2023-01-01',\n",
    "    'notes':'GRU, with basic add on data looped in',\n",
    "    'batch_days': 90,\n",
    "    'stat_list_dict': stat_list_dict,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "raceDB.reset_hidden(num_layers=wandb_config_static['num_layers'], hidden_size=wandb_config_static['hidden_size'])\n",
    "torch.cuda.empty_cache()\n",
    "raceDB.race_prices_to_prob()\n",
    "print(raceDB.latest_date)\n",
    "for race in raceDB.racesDict.values():\n",
    "    race.loss = torch.tensor(1.0,requires_grad=True,device='cuda:0')  \n",
    "    race.loss_log = torch.tensor(1.0,requires_grad=True,device='cuda:0')  \n",
    "\n",
    "for race in raceDB.racesDict.values():\n",
    "    if race.classes.isnan().sum():\n",
    "        print(race.classes.sum())\n",
    "        print(race.raceid)\n",
    "        race.classes = race.classes.nan_to_num(0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model NZ-stoic-music-566, version 410\n",
      "odict_keys(['h0', 'gru.weight_ih_l0', 'gru.weight_hh_l0', 'gru.bias_ih_l0', 'gru.bias_hh_l0', 'gru.weight_ih_l1', 'gru.weight_hh_l1', 'gru.bias_ih_l1', 'gru.bias_hh_l1', 'fc0.weight', 'fc0.bias', 'batch_norm.weight', 'batch_norm.bias', 'batch_norm.running_mean', 'batch_norm.running_var', 'batch_norm.num_batches_tracked', 'batch_norm_data.weight', 'batch_norm_data.bias', 'batch_norm_data.running_mean', 'batch_norm_data.running_var', 'batch_norm_data.num_batches_tracked', 'track_embedding.weight', 'extra_1.batch_norm.weight', 'extra_1.batch_norm.bias', 'extra_1.batch_norm.running_mean', 'extra_1.batch_norm.running_var', 'extra_1.batch_norm.num_batches_tracked', 'extra_1.fc0_p1.weight', 'extra_1.fc0_p1.bias', 'extra_1.fc0_p2.weight', 'extra_1.fc0_p2.bias', 'extra_1.fc0_p3.weight', 'extra_1.fc0_p3.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "Loaded keys:\n",
      "batch_norm.weight torch.Size([18])\n",
      "batch_norm.bias torch.Size([18])\n",
      "batch_norm.running_mean torch.Size([18])\n",
      "batch_norm.running_var torch.Size([18])\n",
      "batch_norm.num_batches_tracked torch.Size([])\n",
      "batch_norm_data.weight torch.Size([36])\n",
      "batch_norm_data.bias torch.Size([36])\n",
      "batch_norm_data.running_mean torch.Size([36])\n",
      "batch_norm_data.running_var torch.Size([36])\n",
      "batch_norm_data.num_batches_tracked torch.Size([])\n",
      "track_embedding.weight torch.Size([1024, 50])\n",
      "extra_1.batch_norm.weight torch.Size([0])\n",
      "extra_1.batch_norm.bias torch.Size([0])\n",
      "extra_1.batch_norm.running_mean torch.Size([0])\n",
      "extra_1.batch_norm.running_var torch.Size([0])\n",
      "extra_1.batch_norm.num_batches_tracked torch.Size([])\n",
      "extra_1.fc0_p2.weight torch.Size([64, 128])\n",
      "extra_1.fc0_p2.bias torch.Size([64])\n",
      "extra_1.fc0_p3.weight torch.Size([64, 64])\n",
      "extra_1.fc0_p3.bias torch.Size([64])\n",
      "fc2.bias torch.Size([64])\n",
      "fc3.weight torch.Size([8, 64])\n",
      "fc3.bias torch.Size([8])\n",
      "\n",
      "Excluded keys:\n",
      "gru.bias_ih_l0 torch.Size([384]) torch.Size([768])\n",
      "failed gru.bias_ih_l0\n",
      "fc0.bias torch.Size([1024]) torch.Size([2048])\n",
      "failed fc0.bias\n",
      "extra_1.fc0_p1.weight torch.Size([0]) torch.Size([128, 36])\n",
      "failed extra_1.fc0_p1.weight\n",
      "fc0.weight torch.Size([1024, 1587]) torch.Size([2048, 2611])\n",
      "gru.bias_ih_l1 torch.Size([384]) torch.Size([768])\n",
      "failed gru.bias_ih_l1\n",
      "gru.weight_ih_l0 torch.Size([384, 18]) torch.Size([768, 18])\n",
      "failed gru.weight_ih_l0\n",
      "gru.weight_hh_l0 torch.Size([384, 128]) torch.Size([768, 256])\n",
      "fc1.bias torch.Size([512]) torch.Size([1024])\n",
      "failed fc1.bias\n",
      "extra_1.fc0_p1.bias torch.Size([0]) torch.Size([128])\n",
      "failed extra_1.fc0_p1.bias\n",
      "fc1.weight torch.Size([512, 1024]) torch.Size([1024, 2048])\n",
      "fc2.weight torch.Size([64, 512]) torch.Size([64, 1024])\n",
      "h0 torch.Size([2, 128]) torch.Size([2, 256])\n",
      "gru.bias_hh_l1 torch.Size([384]) torch.Size([768])\n",
      "failed gru.bias_hh_l1\n",
      "gru.weight_ih_l1 torch.Size([384, 128]) torch.Size([768, 256])\n",
      "gru.bias_hh_l0 torch.Size([384]) torch.Size([768])\n",
      "failed gru.bias_hh_l0\n",
      "gru.weight_hh_l1 torch.Size([384, 128]) torch.Size([768, 256])\n",
      "{'gru.bias_ih_l0': tensor([-4.5778e-02, -6.6070e-02,  4.5633e-02, -1.1780e-02, -4.4181e-02,\n",
      "        -2.7565e-02, -4.3602e-02,  2.6889e-02, -5.5574e-02, -2.4950e-02,\n",
      "         1.3213e-03, -7.8101e-02,  1.2633e-02,  6.1943e-02, -7.0699e-02,\n",
      "         9.2758e-02, -7.3511e-02, -2.5305e-02, -3.0034e-02,  8.3018e-03,\n",
      "        -8.0175e-03, -6.9926e-02, -5.7206e-03, -5.7854e-03,  5.1552e-04,\n",
      "         1.8354e-02, -1.9414e-02,  5.4659e-02, -7.3531e-03, -7.6959e-02,\n",
      "        -1.3067e-01, -6.4694e-02,  1.6345e-02, -5.2307e-02, -4.5670e-02,\n",
      "        -1.7973e-03,  9.0752e-03, -1.0975e-01, -3.6110e-02, -1.1044e-01,\n",
      "        -3.0729e-04, -7.6392e-02,  1.2872e-02, -3.7215e-02, -5.5699e-03,\n",
      "        -6.9264e-02,  8.3449e-03, -6.1171e-02, -4.9371e-02, -1.1272e-01,\n",
      "         2.4097e-02, -1.5595e-01,  7.1490e-03, -5.2793e-03, -1.0577e-01,\n",
      "        -7.9988e-02,  6.4783e-02, -3.4319e-02, -2.2879e-02, -1.0053e-01,\n",
      "         6.0164e-03, -1.8786e-02,  2.7671e-02, -3.3873e-02, -2.0159e-02,\n",
      "        -3.0623e-02, -4.1800e-03,  1.3769e-02, -7.1199e-02,  2.1727e-02,\n",
      "        -1.4923e-02, -5.7251e-02, -1.3487e-01,  5.5025e-02, -1.3749e-02,\n",
      "         1.3209e-02, -1.9199e-02, -7.6294e-02, -8.2774e-02,  2.4511e-03,\n",
      "        -8.8762e-02,  1.4074e-02,  3.8475e-02, -4.4842e-02,  2.6722e-02,\n",
      "        -5.2465e-02, -2.2809e-02,  9.2009e-02, -6.8026e-02, -6.6290e-03,\n",
      "        -2.5661e-02, -8.1140e-04, -6.9078e-02, -1.4525e-02, -7.2571e-02,\n",
      "         2.3101e-02,  9.1631e-02, -3.9609e-02,  7.0270e-03, -2.3891e-02,\n",
      "        -4.1599e-02,  3.1273e-02,  7.9966e-03, -3.2116e-02, -9.9582e-02,\n",
      "        -4.5349e-02, -7.7297e-02,  3.3900e-02, -1.5136e-03, -7.1758e-02,\n",
      "        -5.8606e-02, -9.8990e-02,  2.0115e-02, -3.2367e-02, -3.0586e-02,\n",
      "        -1.5034e-02, -1.2610e-01,  1.6876e-02, -5.4556e-02, -6.3234e-03,\n",
      "         1.9529e-02, -5.4929e-02, -3.3530e-02,  1.8798e-02, -1.0952e-02,\n",
      "         1.8348e-02,  7.6518e-03, -2.1948e-03, -4.8068e-02,  4.1472e-02,\n",
      "        -3.5560e-03, -8.0016e-02,  4.8138e-02,  4.2785e-02,  8.2583e-02,\n",
      "         3.2735e-02, -6.7361e-02,  3.8730e-02,  8.9909e-02, -8.7982e-02,\n",
      "         5.7559e-02,  1.1788e-02,  1.5620e-02,  6.1838e-02, -3.9424e-02,\n",
      "         6.6660e-02,  4.7863e-02, -4.6244e-02, -5.2741e-02,  2.1001e-02,\n",
      "         2.3444e-02,  2.6080e-03, -2.1918e-02,  3.5219e-02, -5.0344e-02,\n",
      "         4.4035e-02,  7.3740e-02,  2.5761e-02,  4.9595e-02,  4.3181e-03,\n",
      "        -2.9971e-02,  1.3347e-02,  1.4974e-02, -1.3639e-02, -6.9147e-02,\n",
      "        -4.0827e-02, -1.8619e-02, -6.1077e-02, -4.6286e-02,  8.1931e-02,\n",
      "         5.0905e-02,  2.8024e-02, -8.6432e-02,  1.4959e-02, -3.7669e-02,\n",
      "        -4.7388e-02, -7.6021e-02,  2.6644e-02, -7.4483e-02,  6.3601e-02,\n",
      "        -5.8478e-02, -2.0132e-03,  5.0759e-02, -3.7837e-02, -3.5125e-02,\n",
      "         4.0244e-02, -5.9724e-02, -3.0871e-04,  5.5468e-02,  2.2318e-02,\n",
      "         9.0596e-02,  1.8722e-02, -7.8378e-02, -2.8133e-02, -8.6828e-02,\n",
      "         4.3617e-02, -9.6736e-02,  3.0156e-02,  3.0849e-02,  1.9734e-02,\n",
      "        -2.9746e-02,  9.7722e-03, -4.6678e-02, -2.8726e-02, -1.3489e-02,\n",
      "         4.0931e-02, -4.2960e-02, -1.9090e-02,  7.0469e-02, -1.9628e-02,\n",
      "        -5.5861e-02,  3.2909e-02,  7.6485e-02, -1.2241e-02, -7.7344e-02,\n",
      "         2.7951e-02, -3.0765e-02, -4.4014e-02, -8.3409e-03, -8.9271e-02,\n",
      "        -2.2563e-02,  2.4689e-02, -3.4043e-02, -5.6376e-02,  8.8091e-03,\n",
      "        -3.0483e-03, -5.9192e-02, -1.4941e-02, -6.5813e-03, -7.3080e-02,\n",
      "         1.2515e-02,  3.7532e-03, -4.1645e-02, -1.2882e-02, -4.2666e-02,\n",
      "        -7.9517e-02,  6.2151e-03,  5.4717e-02, -3.3527e-02, -1.0041e-01,\n",
      "        -7.2291e-02, -5.0535e-02,  3.4064e-02, -7.0270e-04, -2.4397e-02,\n",
      "        -2.0081e-02, -1.8126e-02,  9.3364e-03,  3.6346e-02,  7.8435e-02,\n",
      "        -5.7110e-02, -4.8638e-02, -2.3616e-02,  2.7513e-02, -2.3049e-02,\n",
      "         2.0166e-03,  6.6939e-02,  3.1074e-02, -7.7442e-03, -3.6119e-02,\n",
      "        -4.2537e-02,  2.6558e-02,  6.8375e-02,  2.6871e-02,  6.5234e-02,\n",
      "        -5.5047e-02,  4.0152e-02, -5.6969e-02,  1.3267e-02,  5.3107e-02,\n",
      "         2.2473e-02, -3.5863e-02, -5.3996e-02,  3.6702e-02,  6.4296e-03,\n",
      "         6.1038e-02,  3.1792e-02, -4.0274e-03, -5.1102e-02, -2.7740e-02,\n",
      "        -2.5185e-02,  6.6486e-02, -8.0928e-03,  3.2292e-02,  4.6284e-02,\n",
      "        -3.5184e-02,  6.4779e-02, -6.1867e-02, -2.0133e-02,  4.3207e-02,\n",
      "        -7.5411e-02, -5.8882e-02,  5.2593e-02, -2.1138e-02, -3.9798e-03,\n",
      "        -2.6017e-02, -3.7385e-02,  5.8244e-02,  5.4889e-02,  4.6874e-02,\n",
      "         4.5705e-02, -4.4335e-02,  3.7685e-02,  7.0856e-02,  9.9937e-02,\n",
      "        -2.6742e-02, -7.9046e-03, -4.5128e-02,  6.9353e-02, -1.4715e-02,\n",
      "         4.1563e-02, -6.6015e-02,  2.8792e-02,  4.2798e-02, -3.2347e-02,\n",
      "        -9.7900e-02, -5.9531e-02, -8.3586e-02, -4.4009e-02,  2.6063e-02,\n",
      "        -7.0073e-02, -2.7998e-02,  2.7252e-02,  3.5400e-02,  7.5969e-02,\n",
      "        -8.2527e-03,  4.9104e-02,  6.4048e-02,  7.0210e-02,  2.8480e-02,\n",
      "        -4.3505e-02, -1.0959e-03,  4.5316e-02,  6.3019e-03, -4.7647e-02,\n",
      "        -4.0294e-02,  1.2350e-02,  7.5891e-03, -2.7273e-02, -2.1821e-02,\n",
      "         7.0521e-02,  7.5474e-02, -4.4817e-02,  4.2407e-02, -4.9155e-02,\n",
      "         8.0076e-02,  3.8217e-02, -1.8615e-02, -9.2289e-03, -3.4945e-02,\n",
      "         7.9898e-04,  5.5998e-02, -1.8529e-02, -9.5021e-02,  2.9296e-02,\n",
      "         6.0880e-02, -1.1355e-04,  6.4894e-02,  5.2818e-02,  6.3392e-02,\n",
      "        -3.1787e-02, -4.1001e-02,  6.7366e-02, -4.5836e-02,  1.4478e-02,\n",
      "        -4.8815e-02,  6.3395e-02,  4.0380e-02, -6.2985e-02,  6.0023e-02,\n",
      "         3.5980e-02,  7.0000e-02, -6.6750e-02, -1.2636e-02, -4.9036e-02,\n",
      "         5.4672e-02,  6.8147e-02,  4.2017e-02, -5.5868e-02,  4.6762e-02,\n",
      "        -5.9483e-02,  1.0415e-02,  8.3593e-02, -3.1073e-02], device='cuda:0'), 'fc0.bias': tensor([ 8.6274e-06,  1.7698e-02, -1.4548e-03,  ...,  9.5397e-03,\n",
      "         8.1275e-03, -9.0419e-03], device='cuda:0'), 'extra_1.fc0_p1.weight': tensor([[ 0.1842,  0.0984,  0.1720,  ..., -0.0308, -0.0215,  0.1611],\n",
      "        [ 0.1785, -0.0480,  0.0762,  ..., -0.0014, -0.0584, -0.0926],\n",
      "        [ 0.0821, -0.0116, -0.0816,  ...,  0.0300,  0.1542,  0.0046],\n",
      "        ...,\n",
      "        [ 0.1554,  0.1037,  0.1269,  ...,  0.1346, -0.1822,  0.1089],\n",
      "        [-0.0816, -0.0749,  0.1328,  ...,  0.0654, -0.0006,  0.1713],\n",
      "        [-0.0832, -0.1459, -0.0914,  ...,  0.0011, -0.0223, -0.0276]],\n",
      "       device='cuda:0'), 'fc0.weight': tensor([[ 0.0067, -0.0019, -0.0084,  ...,  0.0024,  0.0119, -0.0011],\n",
      "        [ 0.0031,  0.0167,  0.0122,  ..., -0.0169,  0.0086, -0.0051],\n",
      "        [-0.0122, -0.0028,  0.0146,  ...,  0.0153, -0.0089, -0.0165],\n",
      "        ...,\n",
      "        [ 0.0061,  0.0009, -0.0205,  ..., -0.0081,  0.0046,  0.0111],\n",
      "        [ 0.0098, -0.0349, -0.0010,  ..., -0.0029, -0.0079, -0.0097],\n",
      "        [-0.0105,  0.0116, -0.0068,  ..., -0.0016, -0.0014, -0.0044]],\n",
      "       device='cuda:0'), 'gru.bias_ih_l1': tensor([ 8.5576e-03,  3.8856e-02, -7.9889e-02, -9.6495e-02, -3.4156e-02,\n",
      "         6.1911e-02,  8.5994e-03,  2.1212e-02,  5.3156e-02, -1.3629e-01,\n",
      "        -2.1409e-03, -7.9259e-03, -6.1799e-02,  1.5864e-02, -4.2186e-02,\n",
      "        -2.4378e-02, -4.9436e-02, -2.5623e-02,  4.5171e-02, -8.7309e-02,\n",
      "        -5.3137e-02, -7.5006e-02, -6.6255e-03,  4.7781e-02, -6.8741e-03,\n",
      "        -6.1595e-02, -9.5884e-02, -4.3673e-02, -1.0229e-01, -4.8454e-02,\n",
      "         1.8981e-02, -5.7648e-02, -7.9992e-02, -9.5012e-03, -9.3890e-02,\n",
      "         1.4351e-02, -9.3118e-02, -2.1665e-02, -1.1912e-01, -7.9031e-02,\n",
      "        -1.0812e-01, -4.8557e-02,  4.9670e-02, -1.1498e-01, -9.0419e-03,\n",
      "        -1.1475e-01, -7.6599e-02, -1.1029e-02, -6.0059e-02, -5.0041e-02,\n",
      "        -1.9041e-02, -1.1189e-01, -7.8191e-02,  2.6790e-02, -1.0301e-01,\n",
      "        -5.0200e-02, -1.3077e-01, -8.0536e-02, -6.3631e-03, -1.1783e-01,\n",
      "        -2.5282e-02, -6.3733e-02, -2.8099e-02, -9.1720e-02, -3.4700e-02,\n",
      "         3.1472e-02, -1.1734e-01, -6.5182e-02, -3.5155e-02, -6.5743e-02,\n",
      "        -3.1498e-02, -3.5629e-02, -3.6970e-02, -5.0856e-02, -4.7509e-02,\n",
      "        -8.2994e-02,  1.7450e-02,  2.4899e-02, -9.9252e-03, -5.2182e-02,\n",
      "        -7.4240e-02, -8.1276e-02, -2.8456e-02,  2.5101e-02, -3.3410e-02,\n",
      "        -6.4002e-02, -1.1926e-01,  4.9030e-02, -2.7746e-02,  2.1950e-02,\n",
      "        -1.4511e-01, -5.0293e-02, -9.3020e-02, -5.2295e-04, -9.5306e-02,\n",
      "        -5.5924e-02, -3.3323e-02,  5.7713e-02, -5.5787e-02, -1.5824e-02,\n",
      "        -3.9955e-02, -3.0603e-02, -2.1564e-02, -1.1337e-01, -7.0825e-03,\n",
      "        -1.4079e-01, -1.1339e-01, -1.0126e-01, -8.0218e-02, -4.7840e-03,\n",
      "        -1.2504e-02, -5.8015e-03, -3.5454e-02, -4.8626e-02, -6.0827e-02,\n",
      "         4.4864e-02, -1.1818e-01, -5.5616e-02, -1.3524e-01, -1.4464e-01,\n",
      "        -3.6559e-02, -7.3934e-02, -4.8581e-02,  4.9969e-02, -4.5863e-02,\n",
      "        -3.0539e-02, -1.0588e-01,  3.1890e-03, -5.4913e-02, -4.6030e-03,\n",
      "         4.4328e-04,  6.8368e-02,  1.6276e-02,  5.9806e-02,  3.9114e-02,\n",
      "        -1.5063e-02,  4.8278e-02,  5.8398e-02,  1.8740e-02,  4.7446e-02,\n",
      "         8.2005e-03,  4.5551e-02,  3.2496e-02,  7.3471e-03, -7.6563e-02,\n",
      "         1.9333e-02,  2.7207e-02, -6.6006e-02, -4.3377e-02, -8.7218e-02,\n",
      "        -5.2388e-02, -3.9889e-02, -6.7898e-02, -2.4711e-02, -2.8089e-03,\n",
      "        -1.7853e-02,  8.2620e-02,  4.6751e-02, -6.8034e-02,  8.2794e-03,\n",
      "        -4.9551e-02,  1.3593e-02,  2.5689e-02,  4.7148e-02, -2.8255e-02,\n",
      "        -4.1905e-02,  3.0435e-02, -7.6728e-03, -6.2564e-02, -5.8127e-02,\n",
      "        -3.2688e-02, -4.7162e-02,  2.9545e-02, -7.6912e-02,  6.6590e-02,\n",
      "         1.7574e-02,  2.3506e-02,  4.7952e-02, -3.4344e-02,  9.0818e-05,\n",
      "         4.7618e-02, -4.4755e-02, -1.3107e-02, -2.4000e-02, -7.1370e-03,\n",
      "        -4.7404e-02, -4.2223e-02, -8.2242e-02, -4.7269e-02,  3.3552e-02,\n",
      "         8.2879e-02,  9.1829e-03,  2.1476e-02,  3.9608e-02, -2.1187e-03,\n",
      "        -2.1220e-02, -2.1644e-02,  4.3086e-02,  1.0645e-01, -1.3008e-02,\n",
      "         8.0850e-02,  9.8686e-02,  8.7906e-02, -7.6270e-02,  8.1781e-02,\n",
      "        -1.2260e-02,  1.0518e-01,  3.5615e-02,  4.9754e-02, -5.6211e-03,\n",
      "         6.2919e-02, -4.9358e-03, -5.1089e-02,  2.0394e-02, -8.5985e-03,\n",
      "         9.9708e-02,  1.3265e-02,  6.9064e-02,  6.0192e-02, -3.8662e-02,\n",
      "        -2.5250e-02,  5.3663e-02, -2.5169e-02,  1.6984e-02,  5.2374e-03,\n",
      "        -3.2395e-02,  1.0099e-01,  8.9240e-02, -1.9180e-02, -2.4815e-02,\n",
      "         3.0759e-02,  9.9959e-02, -3.2524e-02,  8.3309e-03,  3.6824e-02,\n",
      "        -2.1817e-02,  6.0073e-02,  5.4832e-02,  2.4914e-02,  8.7081e-02,\n",
      "        -6.5903e-02,  1.1517e-01,  2.7417e-02, -1.5858e-02,  9.4478e-02,\n",
      "        -3.9499e-03, -2.1058e-02,  3.6853e-03, -2.3822e-02,  8.2300e-02,\n",
      "        -1.1124e-02, -1.9503e-02,  9.9206e-03, -2.8046e-02, -4.8780e-02,\n",
      "         3.7493e-02,  4.1738e-02,  3.5521e-02,  6.9901e-02,  5.8777e-02,\n",
      "         1.5016e-02,  1.3866e-02, -4.1143e-02, -2.4683e-02, -9.9410e-03,\n",
      "         8.6914e-02, -1.2267e-02, -4.6307e-02,  1.0424e-01,  3.0135e-02,\n",
      "         5.6530e-02,  6.1810e-02,  4.8594e-02,  3.2227e-02, -2.3604e-02,\n",
      "         3.3735e-02,  3.8696e-03,  5.0651e-02,  5.1903e-02,  4.7880e-02,\n",
      "         4.5964e-02,  6.6009e-02,  1.0484e-01,  1.0217e-01, -2.3849e-02,\n",
      "         5.4271e-02,  9.6716e-02,  5.9697e-02, -1.1021e-02, -2.8590e-02,\n",
      "         9.5420e-02, -4.5115e-03, -6.1188e-02,  4.8721e-02,  6.3493e-02,\n",
      "         2.0818e-02,  2.6301e-02,  8.0807e-02,  2.3138e-02, -8.1772e-03,\n",
      "        -3.1450e-03,  3.6465e-02,  5.2552e-02, -6.4752e-03,  3.2615e-02,\n",
      "        -2.5600e-02,  7.3475e-02,  7.2517e-02,  5.2408e-02, -4.1546e-02,\n",
      "         7.5764e-02,  9.7320e-02,  9.7433e-02,  5.9231e-02,  8.3708e-02,\n",
      "        -5.2154e-02, -5.2856e-02, -9.6062e-03,  3.1576e-02,  1.0457e-02,\n",
      "         3.3176e-04, -3.2803e-02,  2.5109e-02,  8.8349e-02,  1.3141e-02,\n",
      "        -2.3584e-02, -6.3702e-02,  6.7941e-02, -3.8628e-03,  4.4089e-02,\n",
      "         3.1980e-02, -8.8093e-03, -3.4116e-02, -7.1603e-02,  6.2404e-02,\n",
      "        -4.9694e-02,  4.3782e-02,  3.5126e-02,  1.2590e-02,  8.0435e-02,\n",
      "         5.6747e-02,  9.9255e-02, -2.3009e-02, -3.3847e-02, -6.9590e-02,\n",
      "         5.5507e-02,  1.0113e-01,  9.6514e-03,  6.0645e-02,  1.1583e-02,\n",
      "         3.1572e-02,  1.0388e-01,  6.7827e-02, -1.7535e-02,  6.4836e-03,\n",
      "        -2.8606e-02,  3.4762e-02,  1.1702e-02, -1.1581e-02,  3.0839e-02,\n",
      "        -5.5101e-02,  7.4493e-02,  4.5983e-02,  1.0904e-01,  5.6010e-02,\n",
      "         6.3666e-02,  2.1478e-02,  4.9645e-02,  5.9520e-02, -8.4065e-03,\n",
      "         4.1864e-02, -5.9831e-02,  7.3050e-02,  8.1461e-02,  1.0093e-01,\n",
      "         3.3283e-02,  4.2872e-02,  3.2488e-02,  3.0185e-02, -1.7608e-02,\n",
      "        -6.1206e-03,  3.8603e-02,  7.8963e-02, -4.7731e-03], device='cuda:0'), 'gru.weight_ih_l0': tensor([[ 0.0712,  0.0539, -0.0278,  ..., -0.0551,  0.0818, -0.0349],\n",
      "        [ 0.1261,  0.0245, -0.0271,  ..., -0.0826,  0.0563, -0.0141],\n",
      "        [ 0.0934,  0.0900, -0.0865,  ..., -0.0090,  0.0772,  0.0093],\n",
      "        ...,\n",
      "        [-0.0479, -0.0790, -0.0450,  ..., -0.0001,  0.0595, -0.0313],\n",
      "        [-0.0431, -0.0813,  0.0308,  ..., -0.0081,  0.0098, -0.0170],\n",
      "        [-0.0538,  0.0299, -0.0512,  ..., -0.0230,  0.0246,  0.0616]],\n",
      "       device='cuda:0'), 'gru.weight_hh_l0': tensor([[-0.0252,  0.0639, -0.0265,  ..., -0.0002,  0.0410,  0.0665],\n",
      "        [ 0.0490,  0.0207,  0.0320,  ..., -0.0222, -0.0681, -0.0126],\n",
      "        [ 0.0022,  0.0396,  0.0905,  ..., -0.0579,  0.0001, -0.0129],\n",
      "        ...,\n",
      "        [ 0.0149,  0.0158, -0.0112,  ...,  0.0299,  0.0272,  0.0454],\n",
      "        [ 0.0022,  0.0477,  0.0337,  ..., -0.0461, -0.0484, -0.0199],\n",
      "        [-0.0606,  0.0217,  0.0322,  ...,  0.0233, -0.0377,  0.0074]],\n",
      "       device='cuda:0'), 'fc1.bias': tensor([-1.6921e-02, -6.8727e-03, -2.6413e-03,  2.1070e-02, -5.0190e-03,\n",
      "        -2.4717e-03,  1.5229e-02,  4.7197e-03,  1.3245e-02,  9.5582e-03,\n",
      "        -5.3708e-03, -9.2597e-03, -1.1371e-02, -1.4589e-02,  1.7598e-02,\n",
      "         1.4000e-02,  1.6624e-03, -4.0119e-03,  2.1298e-02, -1.7164e-02,\n",
      "        -6.6848e-03,  8.8847e-03, -5.7704e-03, -1.8798e-02, -5.6649e-03,\n",
      "         6.8472e-03, -1.5062e-02,  5.2833e-03, -1.0956e-02,  7.4820e-03,\n",
      "         1.5184e-02, -7.7093e-03, -1.5247e-02, -1.2432e-02, -1.2250e-02,\n",
      "         7.7672e-03,  1.2871e-02, -1.7781e-02,  3.7184e-03, -4.2263e-04,\n",
      "        -1.3779e-02, -6.5122e-03,  8.5500e-03, -1.2331e-02, -1.6683e-02,\n",
      "         1.2858e-02, -9.3591e-03, -6.8490e-03,  1.5577e-02, -8.0650e-03,\n",
      "         2.2382e-03, -2.0082e-02,  2.0566e-02,  1.7327e-02, -1.1659e-02,\n",
      "        -1.2616e-02, -1.7225e-02,  1.4035e-02,  9.7823e-03,  1.6974e-02,\n",
      "        -2.9018e-03, -1.6904e-02, -7.1562e-03, -1.7636e-02, -1.0327e-02,\n",
      "         4.9850e-03, -4.5860e-03, -2.1453e-03,  6.2997e-03,  6.5897e-04,\n",
      "         2.1370e-02,  2.0404e-02,  1.6928e-02,  5.3929e-03,  1.1466e-02,\n",
      "        -5.3089e-04, -7.7592e-04,  2.0051e-02,  1.9421e-02, -1.4409e-02,\n",
      "        -9.0001e-03,  9.8667e-03, -2.0181e-02, -3.3748e-03, -8.0951e-03,\n",
      "        -1.6850e-02,  7.0667e-03,  8.8306e-03,  1.7602e-02, -1.5507e-02,\n",
      "         1.5759e-02,  1.3908e-02, -1.7042e-02,  2.5052e-02,  1.9317e-02,\n",
      "         1.4256e-02, -9.8960e-03,  2.2328e-02,  1.1601e-02, -1.1111e-02,\n",
      "        -8.1598e-03,  1.0131e-02, -6.8386e-03, -1.2813e-02, -8.7598e-03,\n",
      "         2.4065e-02,  1.4158e-03,  9.8165e-03,  1.2193e-02, -7.0653e-03,\n",
      "         4.0476e-03,  1.8990e-02, -1.2761e-02,  4.4457e-03, -1.6292e-02,\n",
      "         2.2641e-02, -1.3253e-02,  2.8947e-03,  2.5987e-02, -4.2988e-03,\n",
      "         7.0447e-03,  1.5059e-03,  6.7617e-03,  1.5797e-02,  2.2114e-02,\n",
      "         9.1225e-03,  1.1713e-02,  4.3923e-03, -2.7608e-03,  1.4183e-02,\n",
      "         8.9588e-03, -4.3248e-03,  1.1500e-02, -6.5553e-03,  1.8027e-02,\n",
      "        -2.0290e-02,  1.5705e-02,  2.0522e-02, -1.4255e-02, -1.6821e-02,\n",
      "         1.2436e-02, -1.5037e-02, -1.3971e-02,  2.7856e-03, -1.6708e-02,\n",
      "        -2.5389e-03, -7.7701e-03, -8.0863e-03,  7.9793e-04,  1.8062e-02,\n",
      "         4.4158e-03,  1.2479e-02,  4.4910e-03, -3.7997e-03, -6.7465e-03,\n",
      "         1.7812e-02, -1.7006e-02, -1.0835e-02,  1.0399e-02,  1.6459e-02,\n",
      "         5.9550e-03,  1.0914e-02,  1.8443e-02, -1.4583e-02, -1.0965e-02,\n",
      "         5.7410e-03, -8.5738e-03, -6.8965e-03,  7.0130e-03, -4.6024e-03,\n",
      "        -1.0225e-02, -1.4165e-02,  1.2103e-02, -4.8138e-03, -1.9137e-02,\n",
      "         2.2528e-02, -8.8392e-03, -1.6036e-02,  8.8137e-03, -3.6899e-03,\n",
      "         7.1055e-03, -2.7619e-03, -1.2840e-02, -7.6200e-03, -1.8702e-02,\n",
      "         7.4994e-03,  5.6064e-03, -1.3305e-02,  1.4611e-02, -1.4541e-02,\n",
      "         1.0389e-03,  1.2647e-04, -9.6250e-05,  2.3094e-02, -5.2801e-03,\n",
      "         3.1627e-03,  2.2205e-02,  2.0130e-02, -1.1798e-02, -1.7779e-02,\n",
      "         1.8678e-02, -1.0066e-02,  1.8020e-02,  3.9759e-03,  1.3230e-02,\n",
      "         1.5405e-02,  1.8372e-02, -1.6848e-02,  7.6473e-03, -1.1444e-02,\n",
      "         1.1518e-02, -1.6727e-02,  2.4255e-02, -6.9654e-03, -1.0475e-02,\n",
      "        -7.8281e-04, -1.6599e-03,  2.0470e-02,  1.2417e-02, -8.5478e-03,\n",
      "         1.5245e-02, -5.2694e-03, -2.1021e-03,  3.9320e-03, -1.0855e-02,\n",
      "        -2.1440e-02,  1.9991e-02, -1.5489e-03, -9.2863e-04,  3.2433e-03,\n",
      "        -5.7570e-03,  1.8820e-02,  1.5539e-02,  2.1334e-02,  2.8541e-03,\n",
      "        -7.1887e-03, -9.6903e-03,  1.0790e-02,  2.2543e-02, -1.9034e-02,\n",
      "         1.8551e-02,  9.9051e-03,  2.0752e-02,  1.1153e-02,  9.8123e-03,\n",
      "         1.4755e-02,  3.8224e-03,  2.1437e-02,  1.9508e-02,  2.0997e-02,\n",
      "         5.6500e-03, -8.2647e-03,  9.9176e-03, -6.8307e-03, -2.8708e-03,\n",
      "        -1.3594e-02, -5.4643e-03, -1.1582e-02,  6.2403e-03, -1.5124e-02,\n",
      "         6.4228e-03, -3.2971e-03, -5.5385e-03,  1.2910e-02, -1.2184e-02,\n",
      "        -1.7195e-02, -1.3057e-02, -8.9071e-03,  1.4405e-02,  1.1628e-02,\n",
      "         2.3748e-02, -1.2414e-02,  7.3551e-03,  5.5111e-04,  1.8018e-02,\n",
      "         1.5742e-03,  1.2157e-03, -8.2414e-03,  2.3390e-02, -1.5573e-02,\n",
      "        -1.3593e-03,  1.2182e-02,  1.5353e-02, -7.4547e-03,  7.6939e-03,\n",
      "        -1.8113e-02,  5.1977e-03,  1.6933e-02,  5.4247e-03,  5.9910e-03,\n",
      "         9.3495e-04, -1.5695e-02,  6.5304e-03,  1.6646e-02,  9.6754e-03,\n",
      "        -1.3116e-02,  1.7281e-02,  2.0696e-02,  1.4976e-02, -2.9813e-03,\n",
      "         7.2838e-03, -5.0948e-03,  3.9636e-03, -1.2733e-02,  1.1110e-02,\n",
      "        -1.1361e-02,  1.5475e-02,  4.4330e-03,  6.5926e-03,  9.4574e-03,\n",
      "        -4.2257e-03, -1.9056e-02, -1.0190e-02, -4.6917e-03, -5.0491e-03,\n",
      "         2.1883e-02,  1.7159e-02,  1.3572e-02, -8.0332e-04, -7.8726e-03,\n",
      "         1.9341e-02, -1.6872e-02,  2.5752e-02,  2.0393e-02,  2.4361e-04,\n",
      "         1.4872e-02,  1.5182e-02, -5.8347e-03, -6.7384e-03,  9.2996e-03,\n",
      "         4.8792e-04,  2.3840e-02,  2.7142e-03, -7.6073e-03, -4.2840e-03,\n",
      "        -1.3295e-02,  7.6601e-03, -1.6848e-02,  1.4049e-03,  2.5596e-02,\n",
      "         1.9085e-02,  1.2960e-02,  1.5324e-02, -1.1847e-02,  1.8840e-02,\n",
      "        -1.3479e-02, -2.1438e-03,  1.5596e-02,  7.9583e-04, -3.7195e-04,\n",
      "        -3.4602e-03,  1.4567e-03,  1.3942e-02, -1.0863e-02,  4.9021e-03,\n",
      "        -1.6754e-02, -1.0335e-02,  1.2857e-02, -7.0485e-03,  2.1944e-02,\n",
      "        -2.1790e-02,  2.0181e-02,  2.6634e-03,  2.3923e-03,  1.7995e-02,\n",
      "         2.4336e-02, -1.8845e-03,  1.0068e-02, -1.3006e-02,  1.6919e-02,\n",
      "         2.4388e-03,  1.0192e-02, -1.8043e-02, -8.1674e-03, -9.6585e-03,\n",
      "        -1.4773e-02, -1.5498e-02,  7.5107e-03, -1.4286e-02, -8.0866e-03,\n",
      "         1.9472e-02,  7.4589e-03,  2.1519e-02, -4.1108e-03, -2.0898e-03,\n",
      "        -5.8965e-04, -8.0427e-03, -8.4921e-03, -9.6774e-03, -3.3614e-03,\n",
      "        -1.3017e-02, -1.0827e-02, -1.1564e-02,  1.1579e-02, -1.5949e-02,\n",
      "         1.6731e-02,  5.3521e-03, -3.2872e-04,  7.8459e-03,  8.3710e-03,\n",
      "        -1.7510e-02, -1.4831e-02, -9.9253e-03,  1.2422e-03, -1.3815e-02,\n",
      "         3.6834e-03,  1.5327e-02, -9.0681e-04,  1.9258e-02, -8.9753e-03,\n",
      "         6.5190e-03, -1.5798e-02,  1.3311e-02, -1.4074e-03, -5.2779e-03,\n",
      "        -1.4521e-02,  1.4898e-02, -4.6995e-03, -9.8645e-03, -1.1136e-02,\n",
      "         1.8950e-02,  9.7716e-03, -4.1853e-03, -1.6633e-02, -1.2379e-02,\n",
      "         1.1745e-02,  1.3660e-02,  1.3404e-02, -7.6070e-03,  1.0884e-02,\n",
      "         7.9341e-03,  2.1033e-02,  1.0205e-02, -1.0409e-02, -4.6261e-03,\n",
      "         2.4314e-02,  1.4084e-02,  8.2579e-03, -1.7564e-02, -5.2029e-03,\n",
      "         1.6374e-02,  1.5081e-02,  1.9054e-02,  1.4783e-02,  1.8632e-02,\n",
      "        -1.9414e-02, -1.6320e-03,  4.0550e-03, -5.2522e-03,  1.8148e-02,\n",
      "        -4.4127e-03,  1.8115e-02,  2.1652e-02,  1.0613e-02, -1.4760e-02,\n",
      "         2.4416e-03, -1.1035e-02, -1.0185e-02,  5.6411e-03,  1.5710e-02,\n",
      "         1.9336e-03,  1.9380e-02,  1.4843e-02,  2.0259e-02,  1.6708e-02,\n",
      "        -9.4603e-03,  2.1820e-02,  1.5661e-02, -1.8608e-02, -2.0664e-04,\n",
      "        -1.4622e-02,  5.1444e-03, -1.5985e-02, -1.6710e-02,  2.1730e-02,\n",
      "        -1.7452e-02, -8.7095e-03,  2.2283e-02, -4.3590e-03, -2.1515e-02,\n",
      "         9.9633e-03, -2.2794e-02,  1.9697e-02, -1.7049e-02,  6.9138e-03,\n",
      "         1.9838e-02, -1.0856e-03,  1.9788e-03,  1.6143e-02, -4.5639e-03,\n",
      "        -5.9531e-03,  2.3008e-02,  5.3807e-03, -4.8428e-03,  7.4472e-03,\n",
      "        -2.0310e-02, -2.2522e-03,  1.3292e-02,  2.0755e-02, -1.3667e-02,\n",
      "         1.6593e-02, -9.3632e-03, -4.8648e-03,  1.4542e-03, -1.7380e-02,\n",
      "        -3.5706e-03,  9.0124e-03,  1.2931e-02,  1.9971e-02,  1.5362e-02,\n",
      "        -2.9132e-03, -1.1331e-02], device='cuda:0'), 'extra_1.fc0_p1.bias': tensor([-0.1432,  0.1544, -0.0444, -0.0687, -0.0917, -0.0843, -0.0222,  0.1620,\n",
      "        -0.1047,  0.1597, -0.0536,  0.1079, -0.1382,  0.1255,  0.1296, -0.1056,\n",
      "         0.0038, -0.1429, -0.0140,  0.0194, -0.0256, -0.0545, -0.1055,  0.1393,\n",
      "         0.1377, -0.0177, -0.1120, -0.0323, -0.1233, -0.0627, -0.1415,  0.1032,\n",
      "         0.1670, -0.1229,  0.1245, -0.0400, -0.0675, -0.1500,  0.1155,  0.1468,\n",
      "         0.1626, -0.0708,  0.1387, -0.0782,  0.0576, -0.0515, -0.0386, -0.0548,\n",
      "         0.1009, -0.0977,  0.1450,  0.1070,  0.1278,  0.1224,  0.1647,  0.0197,\n",
      "         0.1878,  0.0643,  0.1589, -0.0850, -0.0738,  0.1728,  0.1152,  0.0134,\n",
      "         0.0656,  0.0938, -0.0165, -0.1291, -0.0937,  0.0360, -0.0986,  0.1019,\n",
      "         0.1423, -0.0579,  0.1736,  0.1549, -0.0650,  0.0275,  0.1244, -0.0812,\n",
      "        -0.1997,  0.0169, -0.0589, -0.0910,  0.1588, -0.0086,  0.0562, -0.1199,\n",
      "         0.0214,  0.0254,  0.1044, -0.0138, -0.0117,  0.0095, -0.0889, -0.0915,\n",
      "        -0.0473, -0.0707, -0.1281,  0.1492, -0.0407, -0.0989, -0.1205, -0.0532,\n",
      "         0.0548,  0.0899, -0.0433,  0.1201, -0.0634,  0.1101, -0.0347, -0.0378,\n",
      "        -0.0133,  0.1118, -0.0566, -0.0333, -0.1321, -0.1561,  0.0601,  0.0639,\n",
      "        -0.1548,  0.0734,  0.1578,  0.1045,  0.0885, -0.1439,  0.0352,  0.0010],\n",
      "       device='cuda:0'), 'fc1.weight': tensor([[-1.7817e-02, -1.3848e-02,  1.0159e-02,  ...,  3.8037e-03,\n",
      "         -1.4738e-02, -1.2606e-02],\n",
      "        [ 2.1709e-02,  1.3950e-02, -1.6541e-02,  ...,  2.4428e-03,\n",
      "         -1.3747e-02, -4.5542e-03],\n",
      "        [-1.4202e-02,  2.0455e-02, -3.5169e-03,  ...,  7.7824e-03,\n",
      "         -4.5849e-03,  7.9239e-03],\n",
      "        ...,\n",
      "        [ 1.4701e-02, -6.7571e-05, -3.5760e-03,  ...,  1.3677e-03,\n",
      "         -1.4985e-02,  9.6780e-03],\n",
      "        [ 7.8372e-03,  1.7450e-02,  3.1850e-04,  ..., -1.3320e-02,\n",
      "          1.9287e-02, -8.1711e-03],\n",
      "        [ 3.1246e-03, -1.8674e-02, -2.7132e-03,  ...,  7.9476e-03,\n",
      "          6.7823e-03,  1.7803e-02]], device='cuda:0'), 'fc2.weight': tensor([], device='cuda:0', size=(0, 1024)), 'h0': tensor([], device='cuda:0', size=(0, 256)), 'gru.bias_hh_l1': tensor([ 4.1504e-02, -3.8077e-02, -9.4700e-02, -5.2680e-02,  6.6133e-02,\n",
      "        -3.1510e-02, -8.3894e-02, -3.0558e-02,  2.2730e-02, -7.0034e-02,\n",
      "         3.0797e-02, -5.6493e-02, -1.2636e-01, -2.3787e-03, -4.8800e-02,\n",
      "         5.9573e-02, -9.9529e-02,  6.8244e-03, -4.2705e-02, -1.0386e-01,\n",
      "         1.3878e-02, -3.0933e-02, -2.2379e-02,  7.5560e-03, -2.2997e-02,\n",
      "        -2.9356e-02, -6.1673e-02, -3.4151e-02, -8.9446e-02, -3.1653e-02,\n",
      "         1.1084e-02, -4.3398e-02, -3.1765e-02,  6.2624e-02, -1.1217e-01,\n",
      "        -3.7614e-02, -8.1283e-02, -6.3243e-02, -5.9134e-02, -4.6343e-02,\n",
      "        -3.7128e-02, -1.1906e-01,  1.4507e-02, -6.1654e-02,  2.7678e-02,\n",
      "        -1.2760e-01, -2.0913e-02, -2.1167e-02,  7.5225e-03, -3.0289e-02,\n",
      "        -2.5372e-02, -7.4744e-02, -8.5286e-02,  5.5335e-02, -1.1756e-02,\n",
      "        -2.9529e-02, -1.4907e-01, -8.8068e-02, -1.1988e-02, -8.4371e-02,\n",
      "         2.9596e-02, -6.9523e-03, -3.7935e-02, -1.0899e-01,  5.0337e-03,\n",
      "         7.1375e-02, -5.4752e-02, -2.2617e-02, -6.1950e-02, -5.0553e-02,\n",
      "        -1.6751e-02, -3.0658e-02, -4.2475e-02,  5.6126e-02, -5.7173e-02,\n",
      "        -6.3858e-02,  2.1209e-02, -4.8011e-03, -9.1031e-02, -5.3870e-02,\n",
      "         1.4684e-02, -4.1717e-02, -3.6486e-02,  3.0170e-02,  6.7543e-02,\n",
      "        -9.2624e-02, -8.2041e-02, -4.1496e-02, -2.8879e-02, -4.9613e-02,\n",
      "        -1.3892e-01, -1.0574e-01, -9.0400e-02, -6.2841e-02, -1.0736e-02,\n",
      "        -8.9330e-02, -5.3942e-02,  6.5896e-02,  3.4137e-03,  3.4704e-03,\n",
      "        -6.6449e-02, -5.1196e-02,  5.5822e-02, -1.1129e-01, -3.7248e-02,\n",
      "        -1.1750e-01, -4.8837e-02, -8.5207e-02, -5.8761e-02, -1.1276e-01,\n",
      "        -2.4358e-02,  3.5318e-02, -6.3336e-02, -1.0362e-01, -2.4348e-02,\n",
      "        -2.1578e-02, -1.2823e-01, -7.4217e-02, -1.4020e-01, -1.2235e-01,\n",
      "        -2.2997e-02, -6.5859e-02, -2.6671e-02, -2.7337e-02, -7.6447e-02,\n",
      "         1.7758e-02, -1.2082e-01,  1.8922e-02, -7.8946e-02, -6.9659e-03,\n",
      "         4.8105e-02,  2.2450e-02, -8.8851e-03,  2.9938e-02,  7.5039e-03,\n",
      "        -3.8795e-02,  4.4481e-02, -2.8886e-02, -6.2904e-02, -4.2180e-02,\n",
      "         5.3161e-02, -4.8838e-02, -2.3839e-02,  3.8401e-02, -8.5711e-02,\n",
      "         5.9928e-02,  4.3404e-02,  5.6975e-02,  3.5203e-02, -1.4166e-03,\n",
      "         1.8290e-02, -3.4296e-02, -3.9996e-04, -4.4968e-04,  6.9515e-02,\n",
      "         3.6409e-02,  6.7423e-02, -2.6088e-02, -3.2754e-02,  4.2479e-02,\n",
      "        -8.6483e-02,  5.4803e-02,  7.3937e-02, -4.6083e-03, -1.9559e-02,\n",
      "        -6.4468e-02,  3.8189e-02, -9.4130e-03, -7.7380e-02,  3.2726e-02,\n",
      "        -7.8949e-02, -1.9767e-03,  6.7101e-02, -4.6110e-02,  6.9660e-02,\n",
      "        -3.3760e-02, -1.4110e-02, -6.8757e-02, -4.2322e-02,  7.1625e-02,\n",
      "         1.3134e-02, -6.8232e-02, -1.8819e-02, -3.4574e-02,  2.8643e-02,\n",
      "         3.5364e-02, -1.9639e-02, -5.4607e-02, -5.4316e-02,  5.0573e-02,\n",
      "        -2.4700e-02,  1.6994e-02, -3.1034e-02,  2.8500e-02,  5.8023e-02,\n",
      "         1.0887e-02, -3.3579e-02,  3.8292e-02,  1.2848e-02,  9.8592e-03,\n",
      "         3.0240e-03,  2.9362e-02,  1.0169e-01,  1.8062e-03,  1.0570e-01,\n",
      "         5.1501e-02,  2.7450e-02,  6.0735e-02, -1.1177e-03,  2.7775e-02,\n",
      "        -3.7973e-02,  7.7883e-02, -8.1586e-03,  6.7788e-02, -2.5515e-02,\n",
      "         1.0398e-01, -3.2332e-02,  1.1491e-02,  2.3624e-02, -5.3657e-02,\n",
      "        -3.4828e-03,  1.3436e-02, -6.0862e-03,  4.6364e-02,  2.6688e-02,\n",
      "        -8.0544e-02,  7.1267e-02,  6.9599e-02, -5.3669e-02, -2.6062e-02,\n",
      "         1.1373e-01,  7.3649e-06,  9.7083e-04,  1.3718e-02,  5.9747e-03,\n",
      "        -2.0570e-02,  5.5964e-02,  3.9014e-02,  1.7699e-02,  7.6059e-02,\n",
      "        -2.8085e-03,  1.4434e-02,  3.3833e-02,  7.2783e-02,  8.6524e-02,\n",
      "         6.7813e-02,  1.0049e-01,  5.6865e-02,  7.1692e-04,  5.9095e-02,\n",
      "        -4.5545e-02,  8.3029e-04, -4.1246e-02,  4.4033e-02, -5.6197e-02,\n",
      "        -2.8326e-02, -2.0400e-02, -1.5994e-02,  2.6854e-02,  2.5333e-02,\n",
      "         3.9681e-02, -1.9714e-02, -6.9519e-02, -6.5946e-02,  2.7092e-02,\n",
      "         4.6549e-03,  5.0102e-02,  6.7091e-03, -6.6146e-03,  4.1260e-02,\n",
      "         6.5917e-02,  1.4119e-02,  2.4177e-03,  1.4999e-02,  5.0469e-02,\n",
      "         7.3655e-02, -5.2116e-02, -1.0937e-03,  4.4272e-02,  2.9846e-02,\n",
      "        -1.0954e-02,  5.6530e-02, -2.9317e-02,  6.3474e-02,  7.4136e-02,\n",
      "        -1.7971e-03,  9.4388e-02,  3.5703e-02, -5.6363e-02, -6.6735e-02,\n",
      "         9.7560e-03, -2.2848e-02, -4.3530e-02,  4.7072e-02,  9.1675e-02,\n",
      "         6.3000e-02,  8.9388e-03,  6.0351e-02, -2.0655e-02, -1.2813e-02,\n",
      "         4.8651e-02,  2.7020e-03,  2.7738e-02,  5.1519e-02,  6.3953e-04,\n",
      "        -1.0777e-02,  6.1627e-02, -5.8711e-03,  5.9119e-02, -2.2860e-02,\n",
      "         4.0460e-02,  9.2604e-03,  1.0115e-03,  1.0109e-01, -1.4230e-03,\n",
      "        -2.1314e-02,  7.6255e-03, -4.1070e-02,  3.3226e-04,  1.7430e-02,\n",
      "         2.1786e-02, -8.2923e-04,  7.1917e-03,  7.4922e-02, -5.1191e-02,\n",
      "        -8.8067e-03,  3.8472e-02,  6.2961e-02, -3.1883e-03, -4.3885e-02,\n",
      "         8.5059e-02,  5.1258e-02, -6.9128e-02,  4.3618e-02,  5.5485e-02,\n",
      "        -6.3927e-02,  3.0534e-02,  6.0350e-02, -4.6639e-02,  1.2773e-02,\n",
      "        -4.2108e-02,  1.9905e-02, -8.7477e-03,  3.1874e-02,  9.8259e-03,\n",
      "         9.4666e-02, -3.4278e-03,  9.9530e-02,  9.9727e-02,  3.4233e-02,\n",
      "         5.9441e-02,  8.0644e-02,  1.1640e-01,  4.4676e-02, -2.6201e-02,\n",
      "        -5.4562e-02,  9.2975e-02, -5.7594e-02, -8.7329e-03,  9.5203e-02,\n",
      "        -5.8279e-02,  8.1107e-02,  8.5618e-02,  6.9554e-02,  8.4536e-02,\n",
      "         5.6477e-02,  1.0278e-01,  1.6424e-02, -8.4262e-04, -1.7069e-02,\n",
      "        -1.7467e-02, -7.1160e-02,  9.3129e-02,  8.2478e-02,  1.0312e-01,\n",
      "         5.6726e-02,  5.8408e-02,  6.4568e-02, -4.4956e-02, -7.8069e-03,\n",
      "        -3.8374e-02, -1.8548e-02,  1.0626e-01,  3.2167e-02], device='cuda:0'), 'gru.weight_ih_l1': tensor([[ 0.0117,  0.0555, -0.0408,  ...,  0.0122,  0.0328, -0.0039],\n",
      "        [ 0.0226, -0.0493,  0.0249,  ...,  0.0052,  0.0595,  0.0605],\n",
      "        [ 0.0476,  0.0913,  0.0371,  ..., -0.0967, -0.1065, -0.0584],\n",
      "        ...,\n",
      "        [ 0.0194,  0.0273,  0.0240,  ..., -0.0158,  0.0604, -0.0601],\n",
      "        [-0.0342,  0.0034, -0.0012,  ...,  0.0227,  0.0030,  0.0300],\n",
      "        [ 0.0379, -0.0451,  0.0666,  ...,  0.0127, -0.0040,  0.0411]],\n",
      "       device='cuda:0'), 'gru.bias_hh_l0': tensor([-2.5575e-02, -3.0105e-02,  7.3806e-03, -2.6844e-02, -2.9889e-03,\n",
      "        -6.6931e-02,  2.3303e-03,  1.8596e-02, -6.6231e-02,  5.6762e-02,\n",
      "        -3.5991e-02, -2.8123e-02,  2.2042e-02, -9.7779e-04, -5.8468e-02,\n",
      "         7.2802e-02, -5.1313e-02, -1.9002e-03,  4.1249e-02, -4.3611e-02,\n",
      "        -5.7728e-02, -3.3584e-02,  1.9065e-02, -2.6459e-02, -7.1490e-03,\n",
      "        -7.8940e-02, -4.8574e-03, -5.6185e-02,  7.7599e-02, -2.3706e-02,\n",
      "        -2.3743e-02, -8.9109e-02, -4.4721e-02,  4.3198e-02, -7.5460e-02,\n",
      "         4.2471e-02,  1.1289e-02, -9.0605e-02, -5.2982e-02, -3.3213e-02,\n",
      "         1.4067e-03, -4.7781e-02,  4.3284e-02,  5.1906e-03,  7.6457e-03,\n",
      "        -3.1842e-02,  5.7417e-04, -6.8353e-02, -8.6458e-02, -6.4739e-02,\n",
      "         1.1408e-01, -4.9524e-02,  2.8135e-02,  2.6676e-02,  7.3124e-03,\n",
      "        -2.5198e-02,  5.0223e-02, -3.5205e-02, -4.8707e-02, -2.5965e-02,\n",
      "        -3.1972e-02,  6.1511e-02, -4.9135e-02,  1.6211e-02, -2.3715e-02,\n",
      "        -2.9664e-02,  2.0812e-02, -2.5323e-02, -5.9861e-02, -5.1699e-02,\n",
      "         6.7800e-02, -8.3787e-02, -9.9530e-02,  3.2827e-02, -9.5050e-03,\n",
      "        -5.7670e-02, -7.3641e-02, -8.8507e-02,  3.5780e-03, -9.9633e-02,\n",
      "         2.8193e-02,  7.8075e-02, -2.8514e-02,  3.5973e-02, -2.6698e-02,\n",
      "        -9.8893e-02, -5.1472e-02,  6.3962e-02, -8.0389e-02, -6.3110e-02,\n",
      "        -1.2345e-02, -6.8634e-02, -5.6286e-02, -6.0603e-02, -8.0485e-02,\n",
      "        -7.5857e-02,  3.4680e-02, -9.9101e-02,  1.9996e-02, -8.1631e-02,\n",
      "         1.6433e-02, -7.9166e-02,  3.6604e-03, -1.9869e-02, -4.6232e-02,\n",
      "        -9.0005e-02, -8.2372e-02,  7.1561e-02, -5.2532e-03, -7.3455e-02,\n",
      "        -6.4278e-02, -1.0490e-01, -4.4238e-02, -7.4422e-02, -7.9904e-02,\n",
      "        -2.2496e-02, -2.8065e-02,  4.8104e-02,  1.8923e-02,  1.0019e-01,\n",
      "        -4.4600e-02, -5.3012e-02,  2.2550e-03, -2.5492e-02, -4.1451e-02,\n",
      "        -4.5414e-02, -3.1398e-02,  1.7109e-05, -8.0821e-02, -1.3818e-02,\n",
      "        -5.0851e-02,  9.3436e-03, -3.1634e-02, -2.4952e-02,  9.2827e-02,\n",
      "        -1.7935e-02, -6.6283e-02,  6.8782e-02,  2.1746e-02, -5.6218e-02,\n",
      "        -2.9536e-03, -7.7311e-02,  7.1001e-02,  5.2071e-02,  1.9717e-02,\n",
      "         2.7366e-04, -5.3126e-02,  3.4326e-02, -1.0158e-02,  8.1928e-02,\n",
      "         6.9337e-02,  1.4380e-02, -5.5921e-02,  5.1325e-03, -5.8007e-02,\n",
      "         3.5083e-02,  5.0246e-02, -1.3094e-02, -5.3366e-02,  1.9043e-02,\n",
      "         1.1300e-02,  2.1603e-02, -5.7492e-02,  7.1473e-03,  3.4532e-02,\n",
      "        -6.8330e-02, -8.0277e-02,  2.2777e-02, -1.1939e-02, -3.6651e-03,\n",
      "         2.1349e-02, -2.6181e-02,  2.1880e-03,  3.8674e-02,  2.6421e-02,\n",
      "        -5.6585e-02, -8.4795e-02,  6.2180e-02, -8.7458e-02, -3.7987e-02,\n",
      "        -6.0031e-02,  2.2780e-02,  3.4415e-03, -7.0426e-04,  2.2633e-02,\n",
      "         8.3493e-02, -5.3156e-02, -5.0589e-02,  4.5313e-02, -6.2619e-02,\n",
      "         7.7768e-02, -1.9756e-02, -7.7497e-02,  1.1000e-02, -4.0311e-02,\n",
      "        -2.0815e-02,  1.5236e-02, -3.8625e-02, -2.5649e-02,  4.1535e-02,\n",
      "         3.0304e-02, -6.0112e-03,  3.1038e-02, -5.0942e-02, -1.5513e-02,\n",
      "         1.9490e-02, -8.1712e-02,  6.9292e-03,  5.2813e-02,  1.5754e-02,\n",
      "        -4.1463e-02, -2.6529e-02, -1.4035e-02,  4.1778e-02, -4.6684e-02,\n",
      "        -6.3776e-03, -1.8273e-02,  4.8615e-02,  1.1839e-02, -4.8999e-02,\n",
      "        -1.7950e-02, -3.5496e-02, -1.1806e-02,  1.8524e-02, -1.4514e-02,\n",
      "        -3.0916e-02, -4.3345e-02,  2.0641e-02, -8.4438e-02,  4.5012e-02,\n",
      "         2.0920e-03, -2.5869e-02, -5.9272e-02,  3.9074e-02, -3.1035e-02,\n",
      "        -1.6149e-02,  6.0316e-02, -6.2125e-03, -3.7040e-02, -3.0060e-02,\n",
      "        -6.3121e-02, -7.4101e-02,  9.0140e-02,  4.1833e-02, -7.4204e-02,\n",
      "         3.2375e-02, -8.0409e-02, -2.7281e-02, -6.6079e-02,  3.7019e-02,\n",
      "        -5.9882e-02, -7.3743e-02,  7.4252e-02,  1.5213e-02, -3.2381e-02,\n",
      "        -3.6864e-02,  7.9592e-02, -1.2361e-02,  8.1215e-02,  5.6058e-02,\n",
      "        -1.3746e-02, -2.7407e-02,  4.6970e-02, -4.7435e-02,  6.4810e-02,\n",
      "         3.3761e-02, -3.4522e-02, -7.4928e-02,  2.3425e-02,  1.4640e-03,\n",
      "        -6.7884e-02,  5.5115e-02, -1.9340e-02, -3.9823e-04,  1.8994e-02,\n",
      "        -1.2394e-02,  3.0017e-02,  3.5465e-02,  4.5213e-02,  2.8589e-02,\n",
      "        -2.5739e-02,  6.8849e-02,  2.3641e-02, -4.5873e-02,  7.9324e-02,\n",
      "        -4.3971e-02,  4.7239e-02, -8.6762e-02,  1.4899e-03, -5.3466e-02,\n",
      "        -4.4517e-02, -6.5599e-02,  6.0544e-02,  5.2883e-02, -3.8283e-02,\n",
      "         2.6549e-02, -4.1992e-02,  2.9960e-02, -4.6028e-02,  5.3069e-02,\n",
      "        -1.9356e-02,  4.8896e-02,  6.9524e-02,  3.0865e-02,  6.2262e-02,\n",
      "        -3.9381e-02,  3.7218e-03,  4.5366e-02,  2.0421e-02,  2.8729e-02,\n",
      "         6.6828e-02,  1.2101e-02, -5.4435e-03,  1.0546e-02, -4.5669e-02,\n",
      "        -4.9688e-02, -5.0811e-02, -9.4200e-03,  1.7682e-02,  1.3979e-02,\n",
      "        -1.6782e-02, -4.4419e-02, -4.4949e-02,  3.4540e-02, -2.1712e-02,\n",
      "        -6.4990e-02,  5.8158e-02,  4.6116e-02,  6.9644e-02,  3.6630e-02,\n",
      "        -3.7285e-02, -2.2934e-02,  4.8822e-02,  1.5424e-02, -2.1377e-03,\n",
      "         2.0709e-02,  5.8770e-02, -3.0916e-02,  2.1470e-02, -7.7236e-03,\n",
      "         2.5159e-02,  2.4487e-02, -7.7736e-02,  1.8638e-02, -4.8876e-02,\n",
      "         4.8420e-02,  2.1587e-02, -6.4507e-02,  5.8933e-02,  2.7366e-03,\n",
      "        -5.2406e-02,  5.2635e-02,  1.7592e-02, -7.2645e-02,  1.4188e-02,\n",
      "        -2.7971e-03, -4.7573e-03, -1.7993e-02,  5.3125e-02, -1.2095e-02,\n",
      "         6.1613e-02, -2.4331e-02, -2.3937e-02, -2.9721e-02,  3.3947e-02,\n",
      "         1.5860e-02, -6.4652e-03,  3.8822e-02,  2.3936e-02,  7.1750e-02,\n",
      "         2.8999e-02, -3.7897e-02, -6.2009e-02, -7.6999e-02, -7.2198e-02,\n",
      "        -5.5242e-02, -4.7070e-02,  2.9706e-03,  4.0535e-02,  6.7893e-02,\n",
      "        -2.2285e-02, -4.3948e-02, -2.3853e-02, -4.4174e-02], device='cuda:0'), 'gru.weight_hh_l1': tensor([[ 0.0128,  0.0595,  0.0583,  ..., -0.0660, -0.0018, -0.0164],\n",
      "        [ 0.0094, -0.0383,  0.0458,  ...,  0.0202,  0.0241, -0.0546],\n",
      "        [ 0.0401,  0.0127, -0.0464,  ..., -0.0859, -0.0289,  0.0273],\n",
      "        ...,\n",
      "        [ 0.0350, -0.0261,  0.0064,  ...,  0.0589,  0.0085, -0.0501],\n",
      "        [-0.0313, -0.0190, -0.0312,  ..., -0.0657,  0.0123, -0.0418],\n",
      "        [-0.0320,  0.0053,  0.0107,  ...,  0.0031, -0.0553,  0.0282]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "prev_model_file='NZ-stoic-music-566' \n",
    "prev_model_version = 410\n",
    "config = wandb_config_static\n",
    "model = rnn_classes.GRUNetv3_extra_embedding(18,config['hidden_size'], num_layers=config['num_layers'],fc0_size=config['f0_layer_size'], fc1_size=config['f1_layer_size'],data_mask_size=36)\n",
    "print(f\"Loading model {prev_model_file}, version {prev_model_version}\")\n",
    "model_name = prev_model_file\n",
    "model_loc = f\"models/savedmodel/{model_name}/{model_name}_{prev_model_version}.pt\"\n",
    "model_data = torch.load(model_loc,map_location=torch.device('cuda:0'))\n",
    "print(model_data['model_state_dict'].keys())\n",
    "# model.load_state_dict(model_data['model_state_dict'], strict=False)\n",
    "squash_state_dict(model, model_data['model_state_dict'])\n",
    "config['parent model'] = prev_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnickojelly\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\python\\wandb\\run-20240509_135433-nygki2ki</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/NEW%20GRU%20V7%20Reporting/runs/nygki2ki' target=\"_blank\">deep-dragon-601</a></strong> to <a href='https://wandb.ai/nickojelly/NEW%20GRU%20V7%20Reporting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/NEW%20GRU%20V7%20Reporting' target=\"_blank\">https://wandb.ai/nickojelly/NEW%20GRU%20V7%20Reporting</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/NEW%20GRU%20V7%20Reporting/runs/nygki2ki' target=\"_blank\">https://wandb.ai/nickojelly/NEW%20GRU%20V7%20Reporting/runs/nygki2ki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_size': 512, 'stats': \"['dogAgeScaled', 'boxNumber', 'weightInKgScaled', 'hasEntryBoxNumberPlus1', 'hasEntryBoxNumberMinus1', 'rolling_box_win_percentage', 'dog_distance_mean_1', 'dog_boxNumber_mean_1', 'dog_runTimeNorm_mean_1', 'dog_place_mean_1', 'dog_resultMargin_mean_1', 'dog_split_time_margin_mean_1', 'dog_split_runTimeNorm_mean_1', 'dog_time_1_mean_1', 'dog_run_home_TimeNorm_mean_1', 'dog_finishingPlaceMovement_mean_1', 'dog_averageSpeed_mean_1', 'dog_win_mean_1', 'trainer_distance_mean_365D', 'trainer_boxNumber_mean_365D', 'trainer_runTimeNorm_mean_365D', 'trainer_place_mean_365D', 'trainer_resultMargin_mean_365D', 'trainer_split_time_margin_mean_365D', 'trainer_split_runTimeNorm_mean_365D', 'trainer_time_1_mean_365D', 'trainer_run_home_TimeNorm_mean_365D', 'trainer_finishingPlaceMovement_mean_365D', 'trainer_averageSpeed_mean_365D', 'trainer_win_mean_365D', 'dam_distance_mean_365D', 'dam_boxNumber_mean_365D', 'dam_runTimeNorm_mean_365D', 'dam_place_mean_365D', 'dam_resultMargin_mean_365D', 'dam_split_time_margin_mean_365D', 'dam_split_runTimeNorm_mean_365D', 'dam_time_1_mean_365D', 'dam_run_home_TimeNorm_mean_365D', 'dam_finishingPlaceMovement_mean_365D', 'dam_averageSpeed_mean_365D', 'dam_win_mean_365D', 'sire_distance_mean_365D', 'sire_boxNumber_mean_365D', 'sire_runTimeNorm_mean_365D', 'sire_place_mean_365D', 'sire_resultMargin_mean_365D', 'sire_split_time_margin_mean_365D', 'sire_split_runTimeNorm_mean_365D', 'sire_time_1_mean_365D', 'sire_run_home_TimeNorm_mean_365D', 'sire_finishingPlaceMovement_mean_365D', 'sire_averageSpeed_mean_365D', 'sire_win_mean_365D']\", 'races': ['SA'], 'datafile': './data/topaz_data_w_bsp_new3.fth', 'latest_date': '2024-04-30T00:00:00', 'input_type': 'basic', 'num_layers': 2, 'batch_size': 750, 'dropout': 0.3, 'epochs': 2500, 'learning_rate': 0.0001, 'optimizer': 'adamW', 'f0_layer_size': 128, 'f1_layer_size': 64, 'training_date_end': '2023-01-01', 'notes': 'GRU, with basic add on data looped in', 'batch_days': 90, 'stat_list_dict': {'_dist_last__1': 1, '_box_last__1': 1, '_speed_avg_1': 1, '_split_speed_avg_1': 0, '_split_margin_avg_1': 1, '_margin_avg_1': 1, '_margin_time_avg_1': 1, '_RunHomeTime_1': 1, '_run_home_speed_1': 1, '_first_out_avg_1': 0, '_pos_out_avg_1': 0, '_post_change_avg_1': 0, '_races_1': 1, '_wins_1': 1, '_wins_last_1': 1, '_weight_': 1, '_min_time_': 1, '_min_split_time_': 1, '_last_start_price': 0, '_last_start_prob': 0}}\n",
      "2500\n",
      "{'hidden_size': 512, 'stats': \"['dogAgeScaled', 'boxNumber', 'weightInKgScaled', 'hasEntryBoxNumberPlus1', 'hasEntryBoxNumberMinus1', 'rolling_box_win_percentage', 'dog_distance_mean_1', 'dog_boxNumber_mean_1', 'dog_runTimeNorm_mean_1', 'dog_place_mean_1', 'dog_resultMargin_mean_1', 'dog_split_time_margin_mean_1', 'dog_split_runTimeNorm_mean_1', 'dog_time_1_mean_1', 'dog_run_home_TimeNorm_mean_1', 'dog_finishingPlaceMovement_mean_1', 'dog_averageSpeed_mean_1', 'dog_win_mean_1', 'trainer_distance_mean_365D', 'trainer_boxNumber_mean_365D', 'trainer_runTimeNorm_mean_365D', 'trainer_place_mean_365D', 'trainer_resultMargin_mean_365D', 'trainer_split_time_margin_mean_365D', 'trainer_split_runTimeNorm_mean_365D', 'trainer_time_1_mean_365D', 'trainer_run_home_TimeNorm_mean_365D', 'trainer_finishingPlaceMovement_mean_365D', 'trainer_averageSpeed_mean_365D', 'trainer_win_mean_365D', 'dam_distance_mean_365D', 'dam_boxNumber_mean_365D', 'dam_runTimeNorm_mean_365D', 'dam_place_mean_365D', 'dam_resultMargin_mean_365D', 'dam_split_time_margin_mean_365D', 'dam_split_runTimeNorm_mean_365D', 'dam_time_1_mean_365D', 'dam_run_home_TimeNorm_mean_365D', 'dam_finishingPlaceMovement_mean_365D', 'dam_averageSpeed_mean_365D', 'dam_win_mean_365D', 'sire_distance_mean_365D', 'sire_boxNumber_mean_365D', 'sire_runTimeNorm_mean_365D', 'sire_place_mean_365D', 'sire_resultMargin_mean_365D', 'sire_split_time_margin_mean_365D', 'sire_split_runTimeNorm_mean_365D', 'sire_time_1_mean_365D', 'sire_run_home_TimeNorm_mean_365D', 'sire_finishingPlaceMovement_mean_365D', 'sire_averageSpeed_mean_365D', 'sire_win_mean_365D']\", 'races': ['SA'], 'datafile': './data/topaz_data_w_bsp_new3.fth', 'latest_date': '2024-04-30T00:00:00', 'input_type': 'basic', 'num_layers': 2, 'batch_size': 750, 'dropout': 0.3, 'epochs': 2500, 'learning_rate': 0.0001, 'optimizer': 'adamW', 'f0_layer_size': 128, 'f1_layer_size': 64, 'training_date_end': '2023-01-01', 'notes': 'GRU, with basic add on data looped in', 'batch_days': 90, 'stat_list_dict': {'_dist_last__1': 1, '_box_last__1': 1, '_speed_avg_1': 1, '_split_speed_avg_1': 0, '_split_margin_avg_1': 1, '_margin_avg_1': 1, '_margin_time_avg_1': 1, '_RunHomeTime_1': 1, '_run_home_speed_1': 1, '_first_out_avg_1': 0, '_pos_out_avg_1': 0, '_post_change_avg_1': 0, '_races_1': 1, '_wins_1': 1, '_wins_last_1': 1, '_weight_': 1, '_min_time_': 1, '_min_split_time_': 1, '_last_start_price': 0, '_last_start_prob': 0}}\n",
      "basic\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "[1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "data_mask_size=36\n",
      "data_mask=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False], device='cuda:0')\n",
      "torch.Size([54])\n",
      "Train examples 1388, Test examples 286, Val examples 464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2561/2561 [00:00<00:00, 190221.41it/s]\n",
      "100%|██████████| 2562/2562 [00:00<00:00, 5023.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2019, 10, 9), datetime.date(2020, 1, 7)), (datetime.date(2020, 1, 7), datetime.date(2020, 4, 6)), (datetime.date(2020, 4, 6), datetime.date(2020, 7, 5)), (datetime.date(2020, 7, 5), datetime.date(2020, 10, 3)), (datetime.date(2020, 10, 3), datetime.date(2021, 1, 1)), (datetime.date(2021, 1, 1), datetime.date(2021, 4, 1)), (datetime.date(2021, 4, 1), datetime.date(2021, 6, 30)), (datetime.date(2021, 6, 30), datetime.date(2021, 9, 28)), (datetime.date(2021, 9, 28), datetime.date(2021, 12, 27)), (datetime.date(2021, 12, 27), datetime.date(2022, 3, 27)), (datetime.date(2022, 3, 27), datetime.date(2022, 6, 25)), (datetime.date(2022, 6, 25), datetime.date(2022, 9, 23)), (datetime.date(2022, 9, 23), datetime.date(2022, 12, 22)), (datetime.date(2022, 12, 22), datetime.date(2022, 12, 31))]\n",
      "2020-01-07\n",
      "2020-04-06\n",
      "2020-07-05\n",
      "2020-10-03\n",
      "2021-01-01\n",
      "2021-04-01\n",
      "2021-06-30\n",
      "2021-09-28\n",
      "2021-12-27\n",
      "2022-03-27\n",
      "2022-06-25\n",
      "2022-09-23\n",
      "2022-12-22\n",
      "2022-12-31\n",
      "Train examples [46, 61, 85, 86, 81, 95, 130, 138, 115, 140, 128, 138, 136, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2561/2561 [00:00<00:00, 102439.61it/s]\n",
      "100%|██████████| 2561/2561 [00:00<00:00, 94854.54it/s]\n",
      "100%|██████████| 2561/2561 [00:00<00:00, 73169.26it/s]\n",
      "100%|██████████| 2561/2561 [00:00<00:00, 69215.43it/s]\n",
      "100%|██████████| 2561/2561 [00:00<00:00, 82611.27it/s]\n",
      "100%|██████████| 2561/2561 [00:00<00:00, 69213.65it/s]\n",
      "100%|██████████| 2561/2561 [00:00<00:00, 52265.28it/s]\n",
      "100%|██████████| 2561/2561 [00:00<00:00, 47425.38it/s]\n",
      "100%|██████████| 2561/2561 [00:00<00:00, 56909.21it/s]\n",
      "100%|██████████| 2561/2561 [00:00<00:00, 45731.56it/s]\n",
      "100%|██████████| 2561/2561 [00:00<00:00, 55671.60it/s]\n",
      "100%|██████████| 2561/2561 [00:00<00:00, 52266.55it/s]\n",
      "100%|██████████| 2561/2561 [00:00<00:00, 51221.76it/s]\n",
      "100%|██████████| 2561/2561 [00:00<00:00, 320110.04it/s]\n",
      "14it [00:00, 23.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples [212, 224, 318, 291, 265, 270, 330, 370, 325, 368, 316, 332, 324, 58]\n",
      "Train examples [212, 224, 318, 291, 265, 270, 330, 370, 325, 368, 316, 332, 324, 58]\n",
      "Train examples [46, 61, 85, 86, 81, 95, 130, 138, 115, 140, 128, 138, 136, 10]\n",
      "Train examples [46, 61, 85, 86, 81, 95, 130, 138, 115, 140, 128, 138, 136, 10]\n",
      "here\n",
      "input_size=tensor(18, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_32\\3866453612.py:133: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  config['stat_mask_tensor'] = torch.tensor(stat_mask, dtype=torch.uint8).to(device)\n",
      "c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUNetv3_extra_embedding(\n",
      "  (gru): GRU(18, 512, num_layers=2, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (fc0): Linear(in_features=4659, out_features=4096, bias=True)\n",
      "  (batch_norm): BatchNorm1d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm_data): BatchNorm1d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (track_embedding): Embedding(1024, 50)\n",
      "  (extra_1): GRUNetv3_simple_extra_data(\n",
      "    (batch_norm): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU()\n",
      "    (fc0_p1): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "    (fc0_p1_drop): Dropout(p=0.3, inplace=False)\n",
      "    (fc0_p2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (fc0_p2_drop): Dropout(p=0.3, inplace=False)\n",
      "    (fc0_p3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (fc0_p3_drop): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (relu0): ReLU()\n",
      "  (drop1): Dropout(p=0.3, inplace=False)\n",
      "  (fc1): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  (drop2): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=2048, out_features=64, bias=True)\n",
      "  (drop3): Dropout(p=0.3, inplace=False)\n",
      "  (fc3): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (output_fn): Identity()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.49544039997272193\n",
      "val val pass Time: 0.49547560000792146\n",
      "Test time : 5.600686300022062, Val time : 3.6078693000017665, Total time : 9.208555600023828\n",
      "New Max ROI: 0.2493, -0.1113, 2.111\n",
      "created path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/2500 [02:51<5:25:24,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.5813559000089299\n",
      "val val pass Time: 0.5875004999979865\n",
      "Test time : 4.065537899994524, Val time : 3.663294300000416, Total time : 7.72883219999494\n",
      "New Max ROI: 0.2145, -0.0843, 2.08994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 40/2500 [05:39<4:59:39,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.6389286999765318\n",
      "val val pass Time: 0.6409643999941181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 41/2500 [05:57<7:05:30, 10.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test time : 4.465673300001072, Val time : 3.60039889998734, Total time : 8.066072199988412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 60/2500 [08:25<6:06:00,  9.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.451854299986735\n",
      "val val pass Time: 0.6277623999922071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 61/2500 [08:47<8:39:18, 12.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test time : 4.381535499996971, Val time : 3.9015002000087406, Total time : 8.283035700005712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 80/2500 [10:58<4:42:52,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.3426819999876898\n",
      "val val pass Time: 0.5107388000178616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 81/2500 [11:16<6:50:54, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test time : 4.303205100004561, Val time : 3.968563600006746, Total time : 8.271768700011307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 100/2500 [13:45<5:26:50,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.3733336000004783\n",
      "val val pass Time: 0.5604688000166789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 101/2500 [14:04<7:39:24, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test time : 4.046847899997374, Val time : 3.6789881000004243, Total time : 7.725835999997798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 120/2500 [16:08<4:02:23,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.4730904999887571\n",
      "val val pass Time: 0.5697653999959584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 121/2500 [16:24<5:54:25,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test time : 3.8416870000073686, Val time : 3.2576239999907557, Total time : 7.099310999998124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 140/2500 [18:24<4:05:59,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.41205700000864454\n",
      "val val pass Time: 0.5364960999868345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 141/2500 [18:40<5:57:31,  9.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test time : 3.806033300003037, Val time : 3.3903313000046182, Total time : 7.196364600007655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 160/2500 [20:38<4:01:38,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.40101979998871684\n",
      "val val pass Time: 0.5824022000015248\n",
      "Test time : 3.821446100017056, Val time : 3.3179209000081755, Total time : 7.139367000025231\n",
      "New Max ROI: 0.22, -0.0879, 2.08954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 180/2500 [22:54<4:00:12,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.40623669998603873\n",
      "val val pass Time: 0.6807051000068896\n",
      "Test time : 4.588881100004073, Val time : 3.5490649999992456, Total time : 8.137946100003319\n",
      "New Max ROI: 0.2195, -0.0877, 2.08891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 200/2500 [25:11<3:55:12,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.40469340002164245\n",
      "val val pass Time: 0.629912600008538\n",
      "Test time : 3.700371199985966, Val time : 3.3740108000056352, Total time : 7.074381999991601\n",
      "New Max ROI: 0.2185, -0.0869, 2.08824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 220/2500 [27:27<3:51:25,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.4237313999910839\n",
      "val val pass Time: 0.6034478000074159\n",
      "Test time : 3.7471681000024546, Val time : 3.2956059999996796, Total time : 7.042774100002134\n",
      "New Max ROI: 0.2179, -0.0868, 2.08775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 240/2500 [29:43<3:54:18,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.385421499988297\n",
      "val val pass Time: 0.61835110001266\n",
      "Test time : 3.701189699990209, Val time : 3.3363765999965835, Total time : 7.037566299986793\n",
      "New Max ROI: 0.218, -0.0866, 2.0873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 260/2500 [32:10<4:24:18,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.4819971999968402\n",
      "val val pass Time: 0.6858069000008982\n",
      "Test time : 4.462088500004029, Val time : 3.5586075999890454, Total time : 8.020696099993074\n",
      "New Max ROI: 0.2185, -0.0867, 2.08653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 280/2500 [35:02<5:36:51,  9.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.437209099996835\n",
      "val val pass Time: 0.48236580000957474\n",
      "Test time : 4.264396400016267, Val time : 3.701686799991876, Total time : 7.966083200008143\n",
      "New Max ROI: 0.219, -0.0872, 2.08605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 300/2500 [37:56<4:10:57,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.34428079999634065\n",
      "val val pass Time: 0.5116348000010476\n",
      "Test time : 4.9918495999882, Val time : 3.3322345000051428, Total time : 8.324084099993343\n",
      "New Max ROI: 0.2181, -0.0867, 2.08548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 320/2500 [40:24<4:03:40,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.36015140000381507\n",
      "val val pass Time: 0.5142144999990705\n",
      "Test time : 4.465863799996441, Val time : 3.531122399988817, Total time : 7.996986199985258\n",
      "New Max ROI: 0.2179, -0.0863, 2.0851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 340/2500 [42:59<4:42:50,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.31860280002001673\n",
      "val val pass Time: 0.5538560999848414\n",
      "Test time : 3.9260826999961864, Val time : 3.6064528999850154, Total time : 7.532535599981202\n",
      "New Max ROI: 0.2177, -0.0862, 2.08487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 360/2500 [46:02<4:57:14,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.372460200014757\n",
      "val val pass Time: 0.5940442999999505\n",
      "Test time : 4.320174299995415, Val time : 3.62880460001179, Total time : 7.948978900007205\n",
      "New Max ROI: 0.2179, -0.0866, 2.08451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 380/2500 [48:55<4:04:29,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.45003239999641664\n",
      "val val pass Time: 0.5825290000066161\n",
      "Test time : 3.990634300018428, Val time : 3.4330685999884736, Total time : 7.4237029000069015\n",
      "New Max ROI: 0.2176, -0.0864, 2.08436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 400/2500 [51:26<4:06:04,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.42938929999945685\n",
      "val val pass Time: 0.6467129999946337\n",
      "Test time : 3.8955567000084557, Val time : 3.723033399990527, Total time : 7.618590099998983\n",
      "New Max ROI: 0.2176, -0.0867, 2.0839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 420/2500 [53:55<3:55:20,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.44911520002642646\n",
      "val val pass Time: 0.7080864000017755\n",
      "Test time : 8.60428289999254, Val time : 7.249330299993744, Total time : 15.853613199986285\n",
      "New Max ROI: 0.2171, -0.0865, 2.08373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 440/2500 [56:48<4:34:25,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.3846124999981839\n",
      "val val pass Time: 0.5521796999964863\n",
      "Test time : 9.874122700013686, Val time : 7.315998499980196, Total time : 17.190121199993882\n",
      "New Max ROI: 0.2171, -0.0868, 2.0835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 460/2500 [59:54<4:37:45,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.3799760999972932\n",
      "val val pass Time: 0.5651229999784846\n",
      "Test time : 8.995176000025822, Val time : 7.577858099975856, Total time : 16.573034100001678\n",
      "New Max ROI: 0.2168, -0.0866, 2.08318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 480/2500 [1:04:27<9:51:19, 17.56s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 1.6507798999955412\n",
      "val val pass Time: 1.7297678999893833\n",
      "Test time : 9.7793687999947, Val time : 8.34417259998736, Total time : 18.12354139998206\n",
      "New Max ROI: 0.2165, -0.086, 2.08288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 500/2500 [1:09:43<7:38:43, 13.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.3075699000037275\n",
      "val val pass Time: 2.2784651999827474\n",
      "Test time : 5.266649500001222, Val time : 5.274627400009194, Total time : 10.541276900010416\n",
      "New Max ROI: 0.2159, -0.0855, 2.08268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 520/2500 [1:15:18<10:14:12, 18.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.3119519999891054\n",
      "val val pass Time: 2.4053907000052277\n",
      "Test time : 8.175030700018397, Val time : 5.770393199985847, Total time : 13.945423900004243\n",
      "New Max ROI: 0.2156, -0.0853, 2.08256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 540/2500 [1:21:54<8:06:30, 14.89s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test val pass Time: 0.6010675000143237\n",
      "val val pass Time: 0.6085250999894924\n",
      "Test time : 4.609239699988393, Val time : 4.2401064000150654, Total time : 8.849346100003459\n",
      "New Max ROI: 0.2155, -0.0857, 2.08238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 543/2500 [1:22:44<7:58:09, 14.66s/it] "
     ]
    }
   ],
   "source": [
    "importlib.reload(rnn_tools.rnn_classes)\n",
    "importlib.reload(rnn_classes)\n",
    "importlib.reload(training_testing_gru_extra_data)\n",
    "importlib.reload(training_testing_gru_extra_data_embedding)\n",
    "# importlib.reload(training_testing_gru_extra_data_state_batch)\n",
    "# importlib.reload(training_testing_gru_extra_data_toasty)\n",
    "# importlib.reload(training_testing_gru_extra_data_toasty_og)\n",
    "model_pipeline(raceDB,config=wandb_config_static,sweep=False)\n",
    "# model_pipeline(raceDB,config=wandb_config_static,sweep=False, prev_model_file='NZ-stoic-music-566', prev_model_version = 410)\n",
    "# model_pipeline(raceDB,config=wandb_config_static,sweep=False, prev_model_file='NZ-robust-snow-518', prev_model_version = 290)\n",
    "# model_pipeline(raceDB,config=wandb_config_static,sweep=False, prev_model_file='VIC-zany-cherry-524', prev_model_version = 1025)\n",
    "# (model,dataset, optimizer) = model_pipeline(raceDB,config=wandb_config_static,sweep=False)\n",
    "# all_price_df = model_pipeline(raceDB,config=wandb_config_static,sweep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnickojelly\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Nick\\Documents\\GitHub\\grvmodel\\python\\wandb\\run-20240428_165407-9rkykwuv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nickojelly/NEW%20GRU%20V7%20Reporting/runs/9rkykwuv' target=\"_blank\">logical-dew-450</a></strong> to <a href='https://wandb.ai/nickojelly/NEW%20GRU%20V7%20Reporting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nickojelly/NEW%20GRU%20V7%20Reporting' target=\"_blank\">https://wandb.ai/nickojelly/NEW%20GRU%20V7%20Reporting</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nickojelly/NEW%20GRU%20V7%20Reporting/runs/9rkykwuv' target=\"_blank\">https://wandb.ai/nickojelly/NEW%20GRU%20V7%20Reporting/runs/9rkykwuv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_size': 256, 'stats': \"[['inside', 'midfield', 'wide', 'weight', 'DogGrade', '_dist_last__1', '_box_last__1', '_speed_avg_1', '_split_speed_avg_1', '_split_margin_avg_1', '_margin_avg_1', '_margin_time_avg_1', '_RunHomeTime_1', '_run_home_speed_1', '_first_out_avg_1', '_pos_out_avg_1', '_post_change_avg_1', '_races_1', '_wins_1', '_wins_last_1', '_weight_', '_min_time_', '_min_split_time_', '_last_start_price', '_last_start_prob', 'dist_dist_last__10', 'dist_box_last__10', 'dist_speed_avg_10', 'dist_split_speed_avg_10', 'dist_split_margin_avg_10', 'dist_margin_avg_10', 'dist_margin_time_avg_10', 'dist_RunHomeTime_10', 'dist_run_home_speed_10', 'dist_first_out_avg_10', 'dist_pos_out_avg_10', 'dist_post_change_avg_10', 'dist_races_10', 'dist_wins_10', 'dist_wins_last_10', 'dist_weight_', 'dist_min_time_', 'dist_min_split_time_', 'dist_last_start_price', 'dist_last_start_prob', 'box_dist_last__10', 'box_box_last__10', 'box_speed_avg_10', 'box_split_speed_avg_10', 'box_split_margin_avg_10', 'box_margin_avg_10', 'box_margin_time_avg_10', 'box_RunHomeTime_10', 'box_run_home_speed_10', 'box_first_out_avg_10', 'box_pos_out_avg_10', 'box_post_change_avg_10', 'box_races_10', 'box_wins_10', 'box_wins_last_10', 'box_weight_', 'box_min_time_', 'box_min_split_time_', 'box_last_start_price', 'box_last_start_prob', 'track_box_dist_last__10', 'track_box_box_last__10', 'track_box_speed_avg_10', 'track_box_split_speed_avg_10', 'track_box_split_margin_avg_10', 'track_box_margin_avg_10', 'track_box_margin_time_avg_10', 'track_box_RunHomeTime_10', 'track_box_run_home_speed_10', 'track_box_first_out_avg_10', 'track_box_pos_out_avg_10', 'track_box_post_change_avg_10', 'track_box_races_10', 'track_box_wins_10', 'track_box_wins_last_10', 'track_box_weight_', 'track_box_min_time_', 'track_box_min_split_time_', 'track_box_last_start_price', 'track_box_last_start_prob', 'track_dist_dist_last__10', 'track_dist_box_last__10', 'track_dist_speed_avg_10', 'track_dist_split_speed_avg_10', 'track_dist_split_margin_avg_10', 'track_dist_margin_avg_10', 'track_dist_margin_time_avg_10', 'track_dist_RunHomeTime_10', 'track_dist_run_home_speed_10', 'track_dist_first_out_avg_10', 'track_dist_pos_out_avg_10', 'track_dist_post_change_avg_10', 'track_dist_races_10', 'track_dist_wins_10', 'track_dist_wins_last_10', 'track_dist_weight_', 'track_dist_min_time_', 'track_dist_min_split_time_', 'track_dist_last_start_price', 'track_dist_last_start_prob']]\", 'races': ['NSW'], 'datafile': './data/gru_inputs_kitchen_sink_topaz.fth', 'latest_date': '2024-04-06T00:00:00', 'input_type': 'basic', 'num_layers': 2, 'batch_size': 750, 'dropout': 0.3, 'epochs': 2500, 'learning_rate': 0.0001, 'optimizer': 'adamW', 'f0_layer_size': 128, 'f1_layer_size': 64, 'training_date_end': '2023-01-01', 'notes': 'GRU, with basic add on data looped in', 'batch_days': 390, 'stat_list_dict': {'_dist_last__1': 1, '_box_last__1': 1, '_speed_avg_1': 1, '_split_speed_avg_1': 1, '_split_margin_avg_1': 1, '_margin_avg_1': 1, '_margin_time_avg_1': 1, '_RunHomeTime_1': 1, '_run_home_speed_1': 1, '_first_out_avg_1': 1, '_pos_out_avg_1': 1, '_post_change_avg_1': 1, '_races_1': 1, '_wins_1': 1, '_wins_last_1': 1, '_weight_': 1, '_min_time_': 1, '_min_split_time_': 1, '_last_start_price': 1, '_last_start_prob': 1}}\n",
      "2500\n",
      "{'hidden_size': 256, 'stats': \"[['inside', 'midfield', 'wide', 'weight', 'DogGrade', '_dist_last__1', '_box_last__1', '_speed_avg_1', '_split_speed_avg_1', '_split_margin_avg_1', '_margin_avg_1', '_margin_time_avg_1', '_RunHomeTime_1', '_run_home_speed_1', '_first_out_avg_1', '_pos_out_avg_1', '_post_change_avg_1', '_races_1', '_wins_1', '_wins_last_1', '_weight_', '_min_time_', '_min_split_time_', '_last_start_price', '_last_start_prob', 'dist_dist_last__10', 'dist_box_last__10', 'dist_speed_avg_10', 'dist_split_speed_avg_10', 'dist_split_margin_avg_10', 'dist_margin_avg_10', 'dist_margin_time_avg_10', 'dist_RunHomeTime_10', 'dist_run_home_speed_10', 'dist_first_out_avg_10', 'dist_pos_out_avg_10', 'dist_post_change_avg_10', 'dist_races_10', 'dist_wins_10', 'dist_wins_last_10', 'dist_weight_', 'dist_min_time_', 'dist_min_split_time_', 'dist_last_start_price', 'dist_last_start_prob', 'box_dist_last__10', 'box_box_last__10', 'box_speed_avg_10', 'box_split_speed_avg_10', 'box_split_margin_avg_10', 'box_margin_avg_10', 'box_margin_time_avg_10', 'box_RunHomeTime_10', 'box_run_home_speed_10', 'box_first_out_avg_10', 'box_pos_out_avg_10', 'box_post_change_avg_10', 'box_races_10', 'box_wins_10', 'box_wins_last_10', 'box_weight_', 'box_min_time_', 'box_min_split_time_', 'box_last_start_price', 'box_last_start_prob', 'track_box_dist_last__10', 'track_box_box_last__10', 'track_box_speed_avg_10', 'track_box_split_speed_avg_10', 'track_box_split_margin_avg_10', 'track_box_margin_avg_10', 'track_box_margin_time_avg_10', 'track_box_RunHomeTime_10', 'track_box_run_home_speed_10', 'track_box_first_out_avg_10', 'track_box_pos_out_avg_10', 'track_box_post_change_avg_10', 'track_box_races_10', 'track_box_wins_10', 'track_box_wins_last_10', 'track_box_weight_', 'track_box_min_time_', 'track_box_min_split_time_', 'track_box_last_start_price', 'track_box_last_start_prob', 'track_dist_dist_last__10', 'track_dist_box_last__10', 'track_dist_speed_avg_10', 'track_dist_split_speed_avg_10', 'track_dist_split_margin_avg_10', 'track_dist_margin_avg_10', 'track_dist_margin_time_avg_10', 'track_dist_RunHomeTime_10', 'track_dist_run_home_speed_10', 'track_dist_first_out_avg_10', 'track_dist_pos_out_avg_10', 'track_dist_post_change_avg_10', 'track_dist_races_10', 'track_dist_wins_10', 'track_dist_wins_last_10', 'track_dist_weight_', 'track_dist_min_time_', 'track_dist_min_split_time_', 'track_dist_last_start_price', 'track_dist_last_start_prob']]\", 'races': ['NSW'], 'datafile': './data/gru_inputs_kitchen_sink_topaz.fth', 'latest_date': '2024-04-06T00:00:00', 'input_type': 'basic', 'num_layers': 2, 'batch_size': 750, 'dropout': 0.3, 'epochs': 2500, 'learning_rate': 0.0001, 'optimizer': 'adamW', 'f0_layer_size': 128, 'f1_layer_size': 64, 'training_date_end': '2023-01-01', 'notes': 'GRU, with basic add on data looped in', 'batch_days': 390, 'stat_list_dict': {'_dist_last__1': 1, '_box_last__1': 1, '_speed_avg_1': 1, '_split_speed_avg_1': 1, '_split_margin_avg_1': 1, '_margin_avg_1': 1, '_margin_time_avg_1': 1, '_RunHomeTime_1': 1, '_run_home_speed_1': 1, '_first_out_avg_1': 1, '_pos_out_avg_1': 1, '_post_change_avg_1': 1, '_races_1': 1, '_wins_1': 1, '_wins_last_1': 1, '_weight_': 1, '_min_time_': 1, '_min_split_time_': 1, '_last_start_price': 1, '_last_start_prob': 1}}\n",
      "basic\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "data_mask_size=305\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True], device='cuda:0')\n",
      "torch.Size([305])\n",
      "Train examples 20778, Test examples 6853, Val examples 10708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14183/14183 [00:00<00:00, 57338.09it/s]\n",
      "100%|██████████| 14184/14184 [00:09<00:00, 1542.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2019, 12, 1), datetime.date(2020, 12, 25)), (datetime.date(2020, 12, 25), datetime.date(2022, 1, 19)), (datetime.date(2022, 1, 19), datetime.date(2022, 12, 31))]\n",
      "2020-12-25\n",
      "2022-01-19\n",
      "2022-12-31\n",
      "Train examples [1, 8267, 12511]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14183/14183 [00:00<00:00, 787907.63it/s]\n",
      "100%|██████████| 14183/14183 [00:11<00:00, 1182.11it/s]\n",
      "100%|██████████| 14183/14183 [00:18<00:00, 784.27it/s]\n",
      "3it [00:30, 10.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples [8, 5853, 7341]\n",
      "Train examples [8, 5853, 7341]\n",
      "Train examples [1, 8267, 12511]\n",
      "Train examples [1, 8267, 12511]\n",
      "here\n",
      "input_size=tensor(305, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\AppData\\Local\\Temp\\ipykernel_16964\\2788016069.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  config['stat_mask_tensor'] = torch.tensor(stat_mask, dtype=torch.uint8).to(device)\n",
      "c:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUNetv3_extra(\n",
      "  (gru): GRU(305, 256, num_layers=2, dropout=0.3)\n",
      "  (relu): ReLU()\n",
      "  (fc0): Linear(in_features=2630, out_features=2048, bias=True)\n",
      "  (batch_norm): BatchNorm1d(305, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm_data): BatchNorm1d(305, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (extra_1): GRUNetv3_simple_extra_data(\n",
      "    (batch_norm): LazyBatchNorm1d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU()\n",
      "    (fc0_p1): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "    (fc0_p1_drop): Dropout(p=0.3, inplace=False)\n",
      "    (fc0_p2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (fc0_p2_drop): Dropout(p=0.3, inplace=False)\n",
      "    (fc0_p3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (fc0_p3_drop): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (relu0): ReLU()\n",
      "  (drop1): Dropout(p=0.3, inplace=False)\n",
      "  (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (drop2): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (drop3): Dropout(p=0.3, inplace=False)\n",
      "  (fc3): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (output_fn): Identity()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "importlib.reload(rnn_tools.rnn_classes)\n",
    "importlib.reload(rnn_classes)\n",
    "importlib.reload(training_testing_gru_extra_data)\n",
    "# importlib.reload(training_testing_gru_extra_data_state_batch)\n",
    "# importlib.reload(training_testing_gru_extra_data_toasty)\n",
    "# importlib.reload(training_testing_gru_extra_data_toasty_og)\n",
    "(model,dataset, optimizer) = model_pipeline(raceDB,config=wandb_config_static,sweep=False)\n",
    "# model_pipeline(raceDB,config=wandb_config_static,sweep=False, prev_model_file='NZ-fine-waterfall-435', prev_model_version = 355)\n",
    "# (model,dataset, optimizer) = model_pipeline(raceDB,config=wandb_config_static,sweep=False)\n",
    "# all_price_df = model_pipeline(raceDB,config=wandb_config_static,sweep=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_model = rnn_classes.GRUNetv3_profit(raceDB).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raceDB.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_batches = []\n",
    "for batch in batch_races:\n",
    "    batch_dict = {state:[] for state in raceDB.states}\n",
    "    for race in batch:\n",
    "        batch_dict[race.state].append(race)\n",
    "    new_batch = list(batch_dict.values())\n",
    "    new_batches.append(new_batch)\n",
    "    for k,v in batch_dict.items():\n",
    "        print(f\"{k} races {len(v)}\")\n",
    "raceDB.batches['new_batch_races'] = new_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(len(batch)) for batch in raceDB.batches['new_batch_races']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(batch_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(rnn_tools.rnn_classes)\n",
    "importlib.reload(rnn_classes)\n",
    "importlib.reload(training_testing_gru_extra_data)\n",
    "(model,dataset, optimizer) = model_pipeline(raceDB,config=wandb_config_static,sweep=False)\n",
    "\n",
    "# profit_tensor =  model_pipeline(raceDB,config=wandb_config_static,sweep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grad_fn(tensor):\n",
    "    print(tensor)\n",
    "    if tensor is not None:\n",
    "        if tensor is not None:\n",
    "            for t in tensor.next_functions:\n",
    "                if t[0] is not None:\n",
    "                    # print(f\"{t[0]=}\")\n",
    "                    print_grad_fn(t[0])\n",
    "\n",
    "# Usage:\n",
    "# Assuming `output` is your tensor\n",
    "# print_grad_fn(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_tensor.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_grad_fn(profit_tensor[0].grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_tensor.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_test = len(raceDB.test_race_ids)\n",
    "test_idx = range(0,len_test)\n",
    "race = raceDB.get_test_input(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in race:\n",
    "    print(r.prices)\n",
    "    print(r.raceid)\n",
    "    _, actual = torch.max(r.classes, 0)\n",
    "    onehot_win = F.one_hot(actual, num_classes=8)\n",
    "    print(onehot_win)\n",
    "    print(r.win_price_weight)\n",
    "    print(r.raw_margins)\n",
    "    print(r.race_date)\n",
    "    print(r.track_name)\n",
    "    print('----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.subplots as sp\n",
    "\n",
    "hidden_outs = []\n",
    "margins = []\n",
    "for race in raceDB.dogsDict['410839665'].races.values():\n",
    "    hidden_outs.append(race.hidden_out.detach())\n",
    "    margins.append(race.margin)\n",
    "# Assuming hidden_states is your list of tensors\n",
    "hidden_states = [x[0:256] for x in hidden_outs]\n",
    "hidden_states = [hidden_state.reshape(16, 16) for hidden_state in hidden_states]\n",
    "# Create a heatmap for each step\n",
    "frames = [go.Frame(\n",
    "    data=[go.Heatmap(z=hidden_state.tolist(), colorscale='Viridis')],\n",
    "    name=str(i)\n",
    ") for i, hidden_state in enumerate(hidden_states)]\n",
    "\n",
    "# Create a slider\n",
    "sliders = [dict(\n",
    "    steps=[dict(method='animate',\n",
    "                args=[[frame['name']]],\n",
    "                label=str(i)) for i, frame in enumerate(frames)],\n",
    "    transition=dict(duration=300, easing='cubic-in-out'),\n",
    "    active=0,\n",
    ")]\n",
    "\n",
    "# Create a layout\n",
    "layout = go.Layout(\n",
    "    sliders=sliders,\n",
    "    updatemenus=[dict(type='buttons',\n",
    "                      showactive=False,\n",
    "                      buttons=[dict(label='Play',\n",
    "                                    method='animate',\n",
    "                                    args=[None, {\"frame\": {\"duration\": 500, \"redraw\": False},\n",
    "                                                 \"fromcurrent\": True,\n",
    "                                                 \"transition\": {\"duration\": 300,\n",
    "                                                                \"easing\": \"quadratic-in-out\"}}]),\n",
    "                                  dict(label='Stop',\n",
    "                                       method='animate',\n",
    "                                       args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\n",
    "                                                      \"mode\": \"immediate\",\n",
    "                                                      \"transition\": {\"duration\": 0}}])])],\n",
    ")\n",
    "\n",
    "# Create a figure\n",
    "fig = go.Figure(data=frames[0]['data'], layout=layout, frames=frames)\n",
    "\n",
    "# Display the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "hidden_inits= [hidden_state[1,:].reshape(16, 16) for hidden_state in raceDB.hidden_state_inits]\n",
    "\n",
    "dogs = ['366590730','410839665']\n",
    "\n",
    "hiddens = []\n",
    "margins = []\n",
    "for dog in dogs:\n",
    "    hidden_outs = []\n",
    "    margin = []\n",
    "    for race in raceDB.dogsDict[dog].races.values():\n",
    "\n",
    "        hidden_outs.append(race.hidden_out.detach())\n",
    "        margin.append(race.margin)\n",
    "    hiddens.append(hidden_outs)\n",
    "    margins.append(margin)\n",
    "    \n",
    "min_len = min([len(x) for x in hiddens])\n",
    "print(f\"{min_len=}\")\n",
    "hidden_states = [[hidden_state[0:256].cpu().numpy().reshape(16, 16) for hidden_state in dog[0:min_len]] for dog in hiddens]\n",
    "print(f\"{[len(x) for x in hidden_states]=}\")\n",
    "# Create a heatmap for each step\n",
    "global_min = min(hidden_state.min() for hidden_state in hidden_states[0])\n",
    "global_max = max(hidden_state.max() for hidden_state in hidden_states[0])\n",
    "\n",
    "print(global_min, global_max)\n",
    "# print(hidden_states[0])\n",
    "# Create a heatmap for each step with constant scale\n",
    "# Create a subplot for each dog\n",
    "# Create a subplot for each dog with an additional row for the line plot\n",
    "fig = make_subplots(rows=4, cols=1)\n",
    "\n",
    "# Create a heatmap and line plot for each dog\n",
    "for i, dog_hidden_states in enumerate(hidden_states):\n",
    "    # Create heatmap\n",
    "    heatmap = go.Heatmap(z=dog_hidden_states[0].tolist(), zmin=global_min, zmax=global_max, colorscale='Viridis')\n",
    "    fig.add_trace(heatmap, row=2*i+2, col=1)\n",
    "\n",
    "    # Create line plot\n",
    "    line_plot = go.Scatter(y=[sum(abs(state.flatten())) for state in dog_hidden_states], mode='lines',showlegend=False)\n",
    "    fig.add_trace(line_plot, row=2*i+1, col=1)\n",
    "\n",
    "frames = []\n",
    "\n",
    "# Iterate over dogs and their corresponding hidden states\n",
    "for j in range(min_len):\n",
    "    frame_data = []\n",
    "    for i, dog_hidden_states in enumerate(hidden_states):\n",
    "        hidden_state = dog_hidden_states[j]\n",
    "        heatmap = go.Heatmap(z=hidden_state.tolist(), zmin=global_min, zmax=global_max, colorscale='Viridis')\n",
    "        line_plot = go.Scatter(y=[sum(abs(state.flatten())) for state in dog_hidden_states[:j+1]], mode='lines',showlegend=False)\n",
    "        frame_data.extend([heatmap, line_plot])\n",
    "    frame = go.Frame(data=frame_data, name=str(j))  # Create a frame for each time step\n",
    "    frames.append(frame)\n",
    "\n",
    "# Define the slider\n",
    "# Define the slider\n",
    "sliders = [dict(steps=[dict(method='animate',\n",
    "                            args=[[f.name], {\"mode\": \"immediate\",\n",
    "                                             \"frame\": {\"duration\": 100, \"redraw\": True},\n",
    "                                             \"transition\": {\"duration\": 0}}],\n",
    "                            label=f.name) for f in frames],\n",
    "                active=0)]\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    updatemenus=[dict(type='buttons',\n",
    "                      showactive=False,\n",
    "                      buttons=[dict(label='Play',\n",
    "                                    method='animate',\n",
    "                                    args=[None, {\"frame\": {\"duration\": 100, \"redraw\": False},\n",
    "                                                 \"fromcurrent\": True,\n",
    "                                                 \"mode\": \"immediate\",\n",
    "                                                 \"transition\": {\"duration\": 0,\n",
    "                                                                \"easing\": \"quadratic-in-out\"}}]),\n",
    "                                  dict(label='Stop',\n",
    "                                       method='animate',\n",
    "                                       args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\n",
    "                                                      \"mode\": \"immediate\",\n",
    "                                                      \"transition\": {\"duration\": 0}}])])],\n",
    "    sliders=sliders\n",
    ")\n",
    "# Update frames\n",
    "fig.frames = frames\n",
    "\n",
    "# Display the figure\n",
    "fig.show()\n",
    "import plotly.io as pio\n",
    "\n",
    "# Save the figure as an interactive html file\n",
    "pio.write_html(fig, 'hidden_simple_double.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot for each dog\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, dog_hidden_states in enumerate(hidden_states):\n",
    "    # Calculate the absolute sum of hidden states for each time step\n",
    "    abs_sum = [sum(abs(state.flatten())) for state in dog_hidden_states]\n",
    "    print(abs_sum)\n",
    "\n",
    "    # Create line plot with x values specified\n",
    "    line_plot = go.Scatter(x=list(range(len(dog_hidden_states))), y=abs_sum, mode='lines', name=f'Dog {i+1}')\n",
    "    fig.add_trace(line_plot)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Absolute Sum of Hidden States Over Time\",\n",
    "    xaxis_title=\"Time Step\",\n",
    "    yaxis_title=\"Absolute Sum of Hidden States\",\n",
    "    legend_title=\"Dogs\",\n",
    ")\n",
    "\n",
    "# Display the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.layouts import column\n",
    "from bokeh.models import Slider, CustomJS\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.io import curdoc\n",
    "import numpy as np\n",
    "\n",
    "# Assuming hidden_states is a list of 2D numpy arrays\n",
    "hidden_states = [[hidden_state[0:256].cpu().numpy().reshape(16, 16) for hidden_state in dog[0:min_len]] for dog in hiddens]\n",
    "\n",
    "# Create a ColumnDataSource to hold the data for the current frame\n",
    "source = ColumnDataSource(data={'image': [hidden_states[0][0]]})\n",
    "\n",
    "# Create a figure and add an image renderer\n",
    "p = figure(x_range=(0, 1), y_range=(0, 1), width=400, height=400, match_aspect=True)\n",
    "p.image(image='image', x=0, y=0, dw=1, dh=1, source=source)\n",
    "\n",
    "# Create a slider that goes from the first to the last frame\n",
    "slider = Slider(start=0, end=len(hidden_states[0])-1, value=0, step=1)\n",
    "\n",
    "# Define a callback function to update the ColumnDataSource's data when the slider is changed\n",
    "callback = CustomJS(args=dict(source=source, states=hidden_states[0]), code=\"\"\"\n",
    "    var i = cb_obj.value;\n",
    "    source.data = {'image': [states[i]]};\n",
    "\"\"\")\n",
    "\n",
    "# Attach the callback to the slider\n",
    "slider.js_on_change('value', callback)\n",
    "\n",
    "# Display the figure and slider\n",
    "layout = column(p, slider)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in raceDB.hidden_state_inits:\n",
    "    print(i[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state_init = model.h0\n",
    "raceDB.reset_hidden_w_param(hidden_state_init,num_layers=2, hidden_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raceDB.dogsDict['408750377'].hidden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTORCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a48ca33c5a1168302a4f8eae355aad1c03b1396f568d40bc174a6e6aabe725d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
