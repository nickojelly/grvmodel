{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from topaz import TopazAPI\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import concurrent.futures\n",
    "import math\n",
    "\n",
    "import mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ft_sec_key import SECKEY,TOPAZ, PW\n",
    "api_key = TOPAZ #Insert your API key \n",
    "topaz_api = TopazAPI(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 131 markets.\n",
      "<bound method NDFrame.head of            marketStart        track raceNumber raceType  winMarketId  \\\n",
      "0  2024-05-19 12:13:00     Capalaba          2     Heat  1.229082532   \n",
      "1  2024-05-19 12:13:00     Capalaba          2     Heat  1.229082532   \n",
      "2  2024-05-19 12:13:00     Capalaba          2     Heat  1.229082532   \n",
      "3  2024-05-19 12:13:00     Capalaba          2     Heat  1.229082532   \n",
      "4  2024-05-19 12:13:00     Capalaba          2     Heat  1.229082532   \n",
      "..                 ...          ...        ...      ...          ...   \n",
      "3  2024-05-19 22:02:00  Broken Hill         10      Gr5  1.229080620   \n",
      "4  2024-05-19 22:02:00  Broken Hill         10      Gr5  1.229080620   \n",
      "5  2024-05-19 22:02:00  Broken Hill         10      Gr5  1.229080620   \n",
      "6  2024-05-19 22:02:00  Broken Hill         10      Gr5  1.229080620   \n",
      "7  2024-05-19 22:02:00  Broken Hill         10      Gr5  1.229080620   \n",
      "\n",
      "   selectionId rugNumber boxNumber         dogName  \n",
      "0     69503288         1         1       This Time  \n",
      "1     69503289         2         2  Charge On Blue  \n",
      "2     69503290         3         3    Naval Choice  \n",
      "3     69503291         4         4         Red Sun  \n",
      "4     68752486         5         5      Solar Rose  \n",
      "..         ...       ...       ...             ...  \n",
      "3     64264354         4         4      Its Potter  \n",
      "4     69503358         5         5      Its Ashtyn  \n",
      "5     58407557         6         6   Far West Lass  \n",
      "6     60078248         7         7    Group Effort  \n",
      "7     69503359         8         8       Edgy Girl  \n",
      "\n",
      "[980 rows x 9 columns]>\n"
     ]
    }
   ],
   "source": [
    "import betfairlightweight\n",
    "from betfairlightweight import filters\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import warnings\n",
    "import json\n",
    "# data_tools.ft_sec_key import SECKEY\n",
    "\n",
    "warnings.filterwarnings('ignore', message='The behavior of DataFrame concatenation with empty or all-NA entries is deprecated.*')\n",
    "\n",
    "def bflw_trading():\n",
    "    '''\n",
    "    This function loads the credentials file, and passes the credentials into the betfairlightweight instance\n",
    "    '''\n",
    "\n",
    "    username = 'nickbarlow@live.com.au'\n",
    "    password = 'un6/chxe!N!?adsp'\n",
    "    app_key = 'JFWqJHqB4Akfi5hK'\n",
    "\n",
    "    # Define the betfairlightweight client\n",
    "    trading = betfairlightweight.APIClient(username, password, app_key=app_key)\n",
    "\n",
    "    return trading\n",
    "\n",
    "def login(trading):\n",
    "    # login to the API\n",
    "    trading.login_interactive()\n",
    "\n",
    "def greyhound_market_filter():\n",
    "    # Define the greyhound market filter\n",
    "    market_filter = filters.market_filter(\n",
    "        event_type_ids=[4339],  # For horse racing\n",
    "        market_countries=['AU'],  # For Australia\n",
    "        market_type_codes=['WIN']  # For win markets\n",
    "    )\n",
    "\n",
    "    return market_filter\n",
    "\n",
    "def process_runner_books(runner_books):\n",
    "    # Define the fields required from the runner book\n",
    "    selection_ids = [runner_book.selection_id for runner_book in runner_books]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'selectionId': selection_ids,\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def generate_greyhound_catalogue(trading,market_filter):\n",
    "    # Load the greyhound market catalogues from the Betfair API\n",
    "    greyhound_market_catalogues = trading.betting.list_market_catalogue(\n",
    "    filter=market_filter,\n",
    "    market_projection=['RUNNER_DESCRIPTION', 'EVENT', 'MARKET_DESCRIPTION'],\n",
    "    max_results='200')\n",
    "\n",
    "    print(f\"Found {len(greyhound_market_catalogues)} markets.\")\n",
    "\n",
    "    return greyhound_market_catalogues\n",
    "\n",
    "RUNNER_DATA_COLUMNS = [\n",
    "            'marketStart',\n",
    "            'track',\n",
    "            'raceNumber',\n",
    "            'raceType',\n",
    "            'winMarketId',\n",
    "            'selectionId',\n",
    "            'rugNumber',\n",
    "            'boxNumber',\n",
    "            'dogName'\n",
    "            ]\n",
    "\n",
    "def initilise_dataframe():\n",
    "    # Create the empty dataframe\n",
    "    data = pd.DataFrame(columns=RUNNER_DATA_COLUMNS)\n",
    "\n",
    "    return data\n",
    "\n",
    "PATTERN1 = r'(?<=<br>Dog ).+?(?= starts)'\n",
    "\n",
    "PATTERN2 = r\"(?<=\\bbox no. )(\\w+)\"\n",
    "\n",
    "def process_market_clarifications(runners_df,clarifications):\n",
    "    '''\n",
    "    This function accesses the market clarifications field which explains which box the reserve runner will be starting from (if any) and parses the information using regex\n",
    "    We utilise this information rather than the Topaz API data because Betfair markets only use final field information\n",
    "\n",
    "    A clarification will look like: \"<br>Box changes:<br>Dog 9. Tralee Blaze starts from box no. 8<br><br>Dog 6. That Other One starts from box no. 2<br><br>\"\n",
    "    '''\n",
    "    # Define the clarifications dataframe\n",
    "    market_clarifications = pd.DataFrame(regexp_tokenize(clarifications, PATTERN1), columns = ['dogName'])\n",
    "\n",
    "    # Remove dog name from runner_number\n",
    "    market_clarifications['rugNumber'] = market_clarifications['dogName'].str.split(r'. ').str[0]\n",
    "\n",
    "    # Extract box number from clarifications\n",
    "    market_clarifications['boxNumber'] = regexp_tokenize(clarifications, PATTERN2)\n",
    "\n",
    "    # Keep only boxNumber and rugNumber\n",
    "    market_clarifications=market_clarifications[['rugNumber','boxNumber']]\n",
    "\n",
    "    # Merge the clarifications with the original dataframe\n",
    "    runners_df = pd.merge(runners_df,market_clarifications,how='left',on=['rugNumber'])\n",
    "\n",
    "    # Any runners with no clarifications will start in the box that matches the rugNumber\n",
    "    runners_df['boxNumber'].fillna(runners_df['rugNumber'],inplace=True)\n",
    "\n",
    "    return runners_df\n",
    "\n",
    "\n",
    "def collect_greyhound_market_data(trading,greyhound_market_catalogues,data):\n",
    "    '''\n",
    "    This function will process the greyhound market catalogue to access information about the market including:\n",
    "     - Market ID\n",
    "     - Market Name\n",
    "     - Event Name\n",
    "     - Start Time\n",
    "     - Clarifications\n",
    "\n",
    "    It will then process each individual market book to gather the runner information, following by some operations to put market information into the dataframe columns including adjusting the timezone from UTC to AEST\n",
    "    Finally it will then perform some string splitting operations to generate more useful market/runner information:\n",
    "     - Track\n",
    "     - Race Number\n",
    "     - Race Type\n",
    "     - Rug Number\n",
    "     - Dog Name\n",
    "\n",
    "    These operations may be useful depending on whether the betting intention is for a specific subset of races. It is also possible to split out race distance from the market name\n",
    "    '''\n",
    "    # Initiate the for loop\n",
    "    for market_catalogue in greyhound_market_catalogues:\n",
    "\n",
    "        # Name variables for market parameters\n",
    "        market_id = market_catalogue.market_id\n",
    "        market_name = market_catalogue.market_name\n",
    "        event_name = market_catalogue.event.name\n",
    "        market_start_time = market_catalogue.description.market_time\n",
    "\n",
    "        # Try to access clarifications and replace a known string replacement to prepare it for our regex functuon\n",
    "        try:\n",
    "            clarifications = market_catalogue.description.clarifications.replace(\"<br> Dog\",\"<br>Dog\")\n",
    "        except AttributeError:\n",
    "            clarifications = None\n",
    "\n",
    "        # Generate our market_books list\n",
    "        market_books = trading.betting.list_market_book(market_ids=[market_id])\n",
    "\n",
    "        # Generate our runner_catalogues list\n",
    "        runner_catalogues = market_catalogue.runners\n",
    "\n",
    "        # Initiate the market_books for loop\n",
    "        for market_book in market_books:\n",
    "\n",
    "            # Call the process_runner_books function\n",
    "            runners_df = process_runner_books(market_book.runners)\n",
    "\n",
    "            # Get the runner catalogue\n",
    "            for runner in market_book.runners:\n",
    "\n",
    "                # define the runner catalogue\n",
    "                runner_catalogue = next((rd for rd in runner_catalogues if rd.selection_id == runner.selection_id), None)\n",
    "\n",
    "                # define the runner name for non-empty runner_catalogues\n",
    "                if runner_catalogue is not None:\n",
    "                    runner_name = runner_catalogue.runner_name\n",
    "                    runners_df.loc[runners_df['selectionId'] == runner.selection_id, 'dogName'] = runner_name\n",
    "\n",
    "            # Assign market variables to the dataframe\n",
    "            runners_df['winMarketId'] = market_id\n",
    "            runners_df['marketName'] = market_name\n",
    "            runners_df['eventName'] = event_name\n",
    "            runners_df['marketStart'] = market_start_time\n",
    "\n",
    "            # Adjust the timezone from UTC to AEST\n",
    "            runners_df['marketStart'] = runners_df['marketStart'] + timedelta(hours=10)\n",
    "\n",
    "            # Perform string split operations \n",
    "            runners_df['track']=runners_df['eventName'].str.split(' \\(').str[0]\n",
    "            runners_df['raceNumber']=runners_df['marketName'].str.split(r' ').str[0]\n",
    "            runners_df['raceNumber']=runners_df['raceNumber'].str.split('R').str[1]\n",
    "            runners_df['raceType']=runners_df['marketName'].str.split(r'm ').str[1]\n",
    "            runners_df['rugNumber']=runners_df['dogName'].str.split(r'. ').str[0]\n",
    "            runners_df['dogName']=runners_df['dogName'].str.split('\\. ').str[1]\n",
    "\n",
    "            # Call the process_market_clarifications function. If there no reserve runners running then the boxNumber = rugNumber\n",
    "            try:\n",
    "                runners_df = process_market_clarifications(runners_df,clarifications)\n",
    "            except TypeError:\n",
    "                runners_df['boxNumber'] = runners_df['rugNumber']\n",
    "\n",
    "            # concatenate the dataframes together\n",
    "            data=pd.concat([data,runners_df], sort=False)\n",
    "\n",
    "    # Keep only required columns\n",
    "    data = data[RUNNER_DATA_COLUMNS]\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "    print(data.head)\n",
    "\n",
    "    return data\n",
    "\n",
    "def download_betfair_market_data():\n",
    "    '''\n",
    "    This function combines all our previously defined functions to generate our market csv from the Betfair API\n",
    "    '''\n",
    "    trading = bflw_trading()\n",
    "\n",
    "    login(trading)\n",
    "\n",
    "    market_filter = greyhound_market_filter()\n",
    "\n",
    "    greyhound_market_catalogues = generate_greyhound_catalogue(trading,market_filter)\n",
    "\n",
    "    data = initilise_dataframe()\n",
    "\n",
    "    data = collect_greyhound_market_data(trading,greyhound_market_catalogues,data)\n",
    "\n",
    "    return data\n",
    "\n",
    "betfair_data = download_betfair_market_data()\n",
    "\n",
    "# def upcoming_topaz_data(codes,datatype,betfair_data):\n",
    "#     '''\n",
    "#     This function loads our upcoming races, discards the Topaz API boxNumber and adds the boxNumber information retrieved from the Betfair API\n",
    "#     '''\n",
    "#     # Load today's race information\n",
    "#     TodaysTopazData = load_topaz_data(codes,datatype)\n",
    "\n",
    "#     # Keep only required Betfair information\n",
    "#     betfair_fields = betfair_data[['track','raceNumber','rugNumber','boxNumber']]\n",
    "\n",
    "#     # Discard the Topaz API boxNumber information\n",
    "#     TodaysTopazData.drop(columns=['boxNumber'], inplace=True)\n",
    "\n",
    "#     # Merge the Betfair boxNumber information\n",
    "#     TodaysTopazData = pd.merge(TodaysTopazData,betfair_fields,how='left',on=['track','raceNumber','rugNumber'])\n",
    "\n",
    "#     return TodaysTopazData\n",
    "\n",
    "# TodaysTopazData = (JURISDICTION_CODES,'UPCOMING',betfair_data)\n",
    "\n",
    "# def concatenate_data(TopazDataHistorical,TodaysTopazData):\n",
    "\n",
    "#     # Concatenate the last 12 months of Topaz Data with today's races\n",
    "#     TopazDataPreProcessing = pd.concat([TopazDataHistorical,TodaysTopazData])\n",
    "\n",
    "#     return TopazDataPreProcessing\n",
    "\n",
    "# TopazDataPreProcessing = concatenate_data(TopazDataHistorical,TodaysTopazData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_race_runs['dogName'] = new_race_runs['dogName'].str.upper()\n",
    "betfair_data['dogName'] = betfair_data['dogName'].str.replace('.','').str.replace(\"'\",'').str.replace('\"','').str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"silvery-resonance-793\"\n",
    "pred_df = pd.read_feather(f'../model_all_price/predsVIC-{model_name} - val_all_price_df.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         SORRY IM LATE\n",
       "1            SODA DOBBY\n",
       "2                RIBERO\n",
       "3            BOURNE SID\n",
       "4        JOHNSON STREET\n",
       "             ...       \n",
       "275           FAST LEGS\n",
       "276           LITERALLY\n",
       "277    PYRENEES CHATEAU\n",
       "278              DEMURE\n",
       "279           REPAYMENT\n",
       "Name: dog_name, Length: 280, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df['dog_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "track\n",
       "Sale           96\n",
       "Ballarat       96\n",
       "Healesville    88\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.track.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'raw_margins', 'correct', 'simple', 'win_price', 'relu',\n",
       "       'bet_amount_model', 'output_price', 'pred_prob', 'pred_prob2', 'prices',\n",
       "       'imp_prob', 'betfair_log_loss', 'log_loss', 'label_loss', 'pred_price',\n",
       "       'pred_price2', 'classes', 'track', 'onehot_win', 'dogID', 'dog_name',\n",
       "       'dog_box', 'raceID', 'date', 'entropy', 'mutual_info', 'race_num',\n",
       "       'loss', 'loss_bfsp', 'favorite_correct', 'one_hot_win'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_simple = pred_df[['dog_name','pred_prob','pred_prob2']].rename(columns={'dog_name':'dogName'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2024, 5, 19)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2024, 5, 19)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = datetime.today().date()\n",
    "pred_df['date'] = pd.to_datetime(pred_df.date).dt.date\n",
    "pred_df=pred_df[pred_df['date']  == date]\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2024, 5, 19)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df['date'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_fin = pred_df_simple.merge(betfair_data, on = 'dogName')\n",
    "pred_df_fin.to_csv('betfair_model_predictions_nbarlow_18_05_2024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dogName</th>\n",
       "      <th>pred_prob</th>\n",
       "      <th>pred_prob2</th>\n",
       "      <th>marketStart</th>\n",
       "      <th>track</th>\n",
       "      <th>raceNumber</th>\n",
       "      <th>raceType</th>\n",
       "      <th>winMarketId</th>\n",
       "      <th>selectionId</th>\n",
       "      <th>rugNumber</th>\n",
       "      <th>boxNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SORRY IM LATE</td>\n",
       "      <td>0.209668</td>\n",
       "      <td>0.217282</td>\n",
       "      <td>2024-05-19 16:57:00</td>\n",
       "      <td>Sale</td>\n",
       "      <td>2</td>\n",
       "      <td>Gr6/7</td>\n",
       "      <td>1.229082472</td>\n",
       "      <td>69503247</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SODA DOBBY</td>\n",
       "      <td>0.019669</td>\n",
       "      <td>0.009261</td>\n",
       "      <td>2024-05-19 16:57:00</td>\n",
       "      <td>Sale</td>\n",
       "      <td>2</td>\n",
       "      <td>Gr6/7</td>\n",
       "      <td>1.229082472</td>\n",
       "      <td>69503248</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RIBERO</td>\n",
       "      <td>0.301506</td>\n",
       "      <td>0.352675</td>\n",
       "      <td>2024-05-19 16:57:00</td>\n",
       "      <td>Sale</td>\n",
       "      <td>2</td>\n",
       "      <td>Gr6/7</td>\n",
       "      <td>1.229082472</td>\n",
       "      <td>69241965</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BOURNE SID</td>\n",
       "      <td>0.033659</td>\n",
       "      <td>0.018958</td>\n",
       "      <td>2024-05-19 16:57:00</td>\n",
       "      <td>Sale</td>\n",
       "      <td>2</td>\n",
       "      <td>Gr6/7</td>\n",
       "      <td>1.229082472</td>\n",
       "      <td>63471457</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOHNSON STREET</td>\n",
       "      <td>0.239234</td>\n",
       "      <td>0.259066</td>\n",
       "      <td>2024-05-19 16:57:00</td>\n",
       "      <td>Sale</td>\n",
       "      <td>2</td>\n",
       "      <td>Gr6/7</td>\n",
       "      <td>1.229082472</td>\n",
       "      <td>69503249</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>FAST LEGS</td>\n",
       "      <td>0.415598</td>\n",
       "      <td>0.512106</td>\n",
       "      <td>2024-05-19 14:21:00</td>\n",
       "      <td>Ballarat</td>\n",
       "      <td>3</td>\n",
       "      <td>Mdn</td>\n",
       "      <td>1.229083568</td>\n",
       "      <td>69503333</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>LITERALLY</td>\n",
       "      <td>0.176426</td>\n",
       "      <td>0.163385</td>\n",
       "      <td>2024-05-19 14:21:00</td>\n",
       "      <td>Ballarat</td>\n",
       "      <td>3</td>\n",
       "      <td>Mdn</td>\n",
       "      <td>1.229083568</td>\n",
       "      <td>69242104</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>PYRENEES CHATEAU</td>\n",
       "      <td>0.015256</td>\n",
       "      <td>0.006247</td>\n",
       "      <td>2024-05-19 14:21:00</td>\n",
       "      <td>Ballarat</td>\n",
       "      <td>3</td>\n",
       "      <td>Mdn</td>\n",
       "      <td>1.229083568</td>\n",
       "      <td>52148496</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>DEMURE</td>\n",
       "      <td>0.142068</td>\n",
       "      <td>0.122403</td>\n",
       "      <td>2024-05-19 14:21:00</td>\n",
       "      <td>Ballarat</td>\n",
       "      <td>3</td>\n",
       "      <td>Mdn</td>\n",
       "      <td>1.229083568</td>\n",
       "      <td>55782568</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>REPAYMENT</td>\n",
       "      <td>0.148166</td>\n",
       "      <td>0.129457</td>\n",
       "      <td>2024-05-19 14:21:00</td>\n",
       "      <td>Ballarat</td>\n",
       "      <td>3</td>\n",
       "      <td>Mdn</td>\n",
       "      <td>1.229083568</td>\n",
       "      <td>69503334</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dogName  pred_prob  pred_prob2         marketStart     track  \\\n",
       "0       SORRY IM LATE   0.209668    0.217282 2024-05-19 16:57:00      Sale   \n",
       "1          SODA DOBBY   0.019669    0.009261 2024-05-19 16:57:00      Sale   \n",
       "2              RIBERO   0.301506    0.352675 2024-05-19 16:57:00      Sale   \n",
       "3          BOURNE SID   0.033659    0.018958 2024-05-19 16:57:00      Sale   \n",
       "4      JOHNSON STREET   0.239234    0.259066 2024-05-19 16:57:00      Sale   \n",
       "..                ...        ...         ...                 ...       ...   \n",
       "234         FAST LEGS   0.415598    0.512106 2024-05-19 14:21:00  Ballarat   \n",
       "235         LITERALLY   0.176426    0.163385 2024-05-19 14:21:00  Ballarat   \n",
       "236  PYRENEES CHATEAU   0.015256    0.006247 2024-05-19 14:21:00  Ballarat   \n",
       "237            DEMURE   0.142068    0.122403 2024-05-19 14:21:00  Ballarat   \n",
       "238         REPAYMENT   0.148166    0.129457 2024-05-19 14:21:00  Ballarat   \n",
       "\n",
       "    raceNumber raceType  winMarketId selectionId rugNumber boxNumber  \n",
       "0            2    Gr6/7  1.229082472    69503247         1         1  \n",
       "1            2    Gr6/7  1.229082472    69503248         2         2  \n",
       "2            2    Gr6/7  1.229082472    69241965         3         3  \n",
       "3            2    Gr6/7  1.229082472    63471457         4         4  \n",
       "4            2    Gr6/7  1.229082472    69503249         5         5  \n",
       "..         ...      ...          ...         ...       ...       ...  \n",
       "234          3      Mdn  1.229083568    69503333         4         4  \n",
       "235          3      Mdn  1.229083568    69242104         5         5  \n",
       "236          3      Mdn  1.229083568    52148496         6         6  \n",
       "237          3      Mdn  1.229083568    55782568         7         7  \n",
       "238          3      Mdn  1.229083568    69503334         8         8  \n",
       "\n",
       "[239 rows x 11 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../submissions/Greyhound_Racing_Datathon_2024_Submission_Form_Model_Name_20240519.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['market_start', 'venue', 'race_no', 'win_market_id', 'selection_id',\n",
       "       'tab_number', 'runner_name', 'probability'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_merged = submission.merge(pred_df_fin[['selectionId','pred_prob','pred_prob2']].rename(columns={'selectionId':'selection_id'}), on='selection_id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_merged.to_csv('../submissions/Greyhound_Racing_Datathon_2024_Submission_InLimbo_20240519.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../submissions/Greyhound_Racing_Datathon_2024_Submission_InLimbo_20240518.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sub \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../submissions/Greyhound_Racing_Datathon_2024_Submission_InLimbo_20240518.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:504\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    503\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 504\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1563\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1562\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1563\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1566\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1567\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1568\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1570\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1419\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1417\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1419\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1421\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1422\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1423\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Nick\\.conda\\envs\\python311\\Lib\\site-packages\\pandas\\io\\common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../submissions/Greyhound_Racing_Datathon_2024_Submission_InLimbo_20240518.xlsx'"
     ]
    }
   ],
   "source": [
    "sub = pd.read_excel('../submissions/Greyhound_Racing_Datathon_2024_Submission_InLimbo_20240518.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_start</th>\n",
       "      <th>venue</th>\n",
       "      <th>race_no</th>\n",
       "      <th>win_market_id</th>\n",
       "      <th>selection_id</th>\n",
       "      <th>tab_number</th>\n",
       "      <th>runner_name</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-17 15:02:00</td>\n",
       "      <td>Bendigo</td>\n",
       "      <td>1</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>69421826</td>\n",
       "      <td>1</td>\n",
       "      <td>Evies Promise</td>\n",
       "      <td>0.402649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-17 15:02:00</td>\n",
       "      <td>Bendigo</td>\n",
       "      <td>1</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>68235127</td>\n",
       "      <td>2</td>\n",
       "      <td>Calamity Kait</td>\n",
       "      <td>0.034972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-17 15:02:00</td>\n",
       "      <td>Bendigo</td>\n",
       "      <td>1</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>69421827</td>\n",
       "      <td>3</td>\n",
       "      <td>Merls Lad</td>\n",
       "      <td>0.223170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-05-17 15:02:00</td>\n",
       "      <td>Bendigo</td>\n",
       "      <td>1</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>69421828</td>\n",
       "      <td>4</td>\n",
       "      <td>Borough Bound</td>\n",
       "      <td>0.049710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-05-17 15:02:00</td>\n",
       "      <td>Bendigo</td>\n",
       "      <td>1</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>64458909</td>\n",
       "      <td>5</td>\n",
       "      <td>Little Tic</td>\n",
       "      <td>0.135942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2024-05-17 22:48:00</td>\n",
       "      <td>Geelong</td>\n",
       "      <td>12</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>61028130</td>\n",
       "      <td>4</td>\n",
       "      <td>Ventura Bale</td>\n",
       "      <td>0.173609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2024-05-17 22:48:00</td>\n",
       "      <td>Geelong</td>\n",
       "      <td>12</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>48821690</td>\n",
       "      <td>5</td>\n",
       "      <td>Talk Out Loud</td>\n",
       "      <td>0.036829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2024-05-17 22:48:00</td>\n",
       "      <td>Geelong</td>\n",
       "      <td>12</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>43256898</td>\n",
       "      <td>6</td>\n",
       "      <td>Who Told Mel</td>\n",
       "      <td>0.058196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2024-05-17 22:48:00</td>\n",
       "      <td>Geelong</td>\n",
       "      <td>12</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>49514111</td>\n",
       "      <td>7</td>\n",
       "      <td>Crackerjack Pot</td>\n",
       "      <td>0.081224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2024-05-17 22:48:00</td>\n",
       "      <td>Geelong</td>\n",
       "      <td>12</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>66204405</td>\n",
       "      <td>8</td>\n",
       "      <td>Tinker Dot</td>\n",
       "      <td>0.516399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           market_start    venue  race_no  win_market_id  selection_id  \\\n",
       "0   2024-05-17 15:02:00  Bendigo        1       1.228987      69421826   \n",
       "1   2024-05-17 15:02:00  Bendigo        1       1.228987      68235127   \n",
       "2   2024-05-17 15:02:00  Bendigo        1       1.228987      69421827   \n",
       "3   2024-05-17 15:02:00  Bendigo        1       1.228987      69421828   \n",
       "4   2024-05-17 15:02:00  Bendigo        1       1.228987      64458909   \n",
       "..                  ...      ...      ...            ...           ...   \n",
       "234 2024-05-17 22:48:00  Geelong       12       1.228987      61028130   \n",
       "235 2024-05-17 22:48:00  Geelong       12       1.228987      48821690   \n",
       "236 2024-05-17 22:48:00  Geelong       12       1.228987      43256898   \n",
       "237 2024-05-17 22:48:00  Geelong       12       1.228987      49514111   \n",
       "238 2024-05-17 22:48:00  Geelong       12       1.228987      66204405   \n",
       "\n",
       "     tab_number      runner_name  probability  \n",
       "0             1    Evies Promise     0.402649  \n",
       "1             2    Calamity Kait     0.034972  \n",
       "2             3        Merls Lad     0.223170  \n",
       "3             4    Borough Bound     0.049710  \n",
       "4             5       Little Tic     0.135942  \n",
       "..          ...              ...          ...  \n",
       "234           4     Ventura Bale     0.173609  \n",
       "235           5    Talk Out Loud     0.036829  \n",
       "236           6     Who Told Mel     0.058196  \n",
       "237           7  Crackerjack Pot     0.081224  \n",
       "238           8       Tinker Dot     0.516399  \n",
       "\n",
       "[239 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      DESTINI PALADIN\n",
       "1       DUSTY OLD ROAD\n",
       "2              no_name\n",
       "3              no_name\n",
       "4              no_name\n",
       "            ...       \n",
       "275     JAYVILLE MOLLY\n",
       "276     WREAKING HAVOC\n",
       "277      DARNUM DIESEL\n",
       "278     CHEAP RED WINE\n",
       "279            no_name\n",
       "Name: dog_name, Length: 280, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.dog_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_start</th>\n",
       "      <th>venue</th>\n",
       "      <th>race_no</th>\n",
       "      <th>win_market_id</th>\n",
       "      <th>selection_id</th>\n",
       "      <th>tab_number</th>\n",
       "      <th>runner_name</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-17 15:02:00</td>\n",
       "      <td>Bendigo</td>\n",
       "      <td>1</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>69421826</td>\n",
       "      <td>1</td>\n",
       "      <td>Evies Promise</td>\n",
       "      <td>0.402649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-17 15:02:00</td>\n",
       "      <td>Bendigo</td>\n",
       "      <td>1</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>68235127</td>\n",
       "      <td>2</td>\n",
       "      <td>Calamity Kait</td>\n",
       "      <td>0.034972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-17 15:02:00</td>\n",
       "      <td>Bendigo</td>\n",
       "      <td>1</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>69421827</td>\n",
       "      <td>3</td>\n",
       "      <td>Merls Lad</td>\n",
       "      <td>0.223170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-05-17 15:02:00</td>\n",
       "      <td>Bendigo</td>\n",
       "      <td>1</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>69421828</td>\n",
       "      <td>4</td>\n",
       "      <td>Borough Bound</td>\n",
       "      <td>0.049710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-05-17 15:02:00</td>\n",
       "      <td>Bendigo</td>\n",
       "      <td>1</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>64458909</td>\n",
       "      <td>5</td>\n",
       "      <td>Little Tic</td>\n",
       "      <td>0.135942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2024-05-17 22:48:00</td>\n",
       "      <td>Geelong</td>\n",
       "      <td>12</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>61028130</td>\n",
       "      <td>4</td>\n",
       "      <td>Ventura Bale</td>\n",
       "      <td>0.173609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2024-05-17 22:48:00</td>\n",
       "      <td>Geelong</td>\n",
       "      <td>12</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>48821690</td>\n",
       "      <td>5</td>\n",
       "      <td>Talk Out Loud</td>\n",
       "      <td>0.036829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2024-05-17 22:48:00</td>\n",
       "      <td>Geelong</td>\n",
       "      <td>12</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>43256898</td>\n",
       "      <td>6</td>\n",
       "      <td>Who Told Mel</td>\n",
       "      <td>0.058196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2024-05-17 22:48:00</td>\n",
       "      <td>Geelong</td>\n",
       "      <td>12</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>49514111</td>\n",
       "      <td>7</td>\n",
       "      <td>Crackerjack Pot</td>\n",
       "      <td>0.081224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2024-05-17 22:48:00</td>\n",
       "      <td>Geelong</td>\n",
       "      <td>12</td>\n",
       "      <td>1.228987</td>\n",
       "      <td>66204405</td>\n",
       "      <td>8</td>\n",
       "      <td>Tinker Dot</td>\n",
       "      <td>0.516399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           market_start    venue  race_no  win_market_id  selection_id  \\\n",
       "0   2024-05-17 15:02:00  Bendigo        1       1.228987      69421826   \n",
       "1   2024-05-17 15:02:00  Bendigo        1       1.228987      68235127   \n",
       "2   2024-05-17 15:02:00  Bendigo        1       1.228987      69421827   \n",
       "3   2024-05-17 15:02:00  Bendigo        1       1.228987      69421828   \n",
       "4   2024-05-17 15:02:00  Bendigo        1       1.228987      64458909   \n",
       "..                  ...      ...      ...            ...           ...   \n",
       "234 2024-05-17 22:48:00  Geelong       12       1.228987      61028130   \n",
       "235 2024-05-17 22:48:00  Geelong       12       1.228987      48821690   \n",
       "236 2024-05-17 22:48:00  Geelong       12       1.228987      43256898   \n",
       "237 2024-05-17 22:48:00  Geelong       12       1.228987      49514111   \n",
       "238 2024-05-17 22:48:00  Geelong       12       1.228987      66204405   \n",
       "\n",
       "     tab_number      runner_name  probability  \n",
       "0             1    Evies Promise     0.402649  \n",
       "1             2    Calamity Kait     0.034972  \n",
       "2             3        Merls Lad     0.223170  \n",
       "3             4    Borough Bound     0.049710  \n",
       "4             5       Little Tic     0.135942  \n",
       "..          ...              ...          ...  \n",
       "234           4     Ventura Bale     0.173609  \n",
       "235           5    Talk Out Loud     0.036829  \n",
       "236           6     Who Told Mel     0.058196  \n",
       "237           7  Crackerjack Pot     0.081224  \n",
       "238           8       Tinker Dot     0.516399  \n",
       "\n",
       "[239 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['dog_name'] = sub['runner_name'].str.replace('.','').str.replace(\"'\",'').str.replace('\"','').str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.merge(pred_df, left_on='dog_name', right_on='dog_name', how='left').to_excel('../submissions/test.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_race_runs_merged = new_race_runs.merge(betfair_df_dogs_only ,how='inner', on='dogName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "def pd_softmax(x):\n",
    "    x = torch.tensor(list(x))\n",
    "    x = F.softmax(x, dim=0)\n",
    "    return x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.18849   , 0.17420626, 0.17957641, 0.21921717, 0.23851018],\n",
       "       dtype=float32),\n",
       " array([0.15281205, 0.17257346, 0.14641863, 0.19442493, 0.14678824,\n",
       "        0.18698272], dtype=float32),\n",
       " array([0.11840022, 0.11177149, 0.10957627, 0.16575077, 0.1304922 ,\n",
       "        0.11106802, 0.1260849 , 0.1268561 ], dtype=float32),\n",
       " array([0.14836417, 0.11660397, 0.12521355, 0.14293487, 0.11786979,\n",
       "        0.11987338, 0.11591206, 0.11322825], dtype=float32),\n",
       " array([0.15806764, 0.1806787 , 0.14583397, 0.142339  , 0.17156258,\n",
       "        0.20151813], dtype=float32),\n",
       " array([0.1119795 , 0.1421925 , 0.13150015, 0.12330826, 0.11311845,\n",
       "        0.11136733, 0.11974572, 0.14678818], dtype=float32),\n",
       " array([0.11356873, 0.11434497, 0.13475898, 0.11062954, 0.15119211,\n",
       "        0.12775455, 0.13510752, 0.11264369], dtype=float32),\n",
       " array([0.14117089, 0.15423873, 0.1428199 , 0.14148337, 0.13182694,\n",
       "        0.14364566, 0.14481442], dtype=float32),\n",
       " array([0.11192398, 0.11129102, 0.12452698, 0.13055874, 0.11095163,\n",
       "        0.15212406, 0.14179537, 0.11682815], dtype=float32),\n",
       " array([0.13165729, 0.1670689 , 0.1232777 , 0.16975342, 0.13295339,\n",
       "        0.14491001, 0.13037927], dtype=float32),\n",
       " array([0.149216  , 0.14234458, 0.18702671, 0.1570609 , 0.1633241 ,\n",
       "        0.20102783], dtype=float32),\n",
       " array([0.14267093, 0.204001  , 0.14827627, 0.20077747, 0.14644457,\n",
       "        0.15782978], dtype=float32),\n",
       " array([0.12973338, 0.13720556, 0.13708925, 0.20060803, 0.12570657,\n",
       "        0.13132864, 0.1383285 ], dtype=float32),\n",
       " array([0.17386769, 0.15361498, 0.13001776, 0.14296964, 0.1356704 ,\n",
       "        0.13034494, 0.13351458], dtype=float32),\n",
       " array([0.16857259, 0.14967561, 0.15607312, 0.15620168, 0.15824011,\n",
       "        0.21123692], dtype=float32),\n",
       " array([0.12614262, 0.1245538 , 0.17553692, 0.17614134, 0.12355047,\n",
       "        0.1457544 , 0.12832044], dtype=float32),\n",
       " array([0.17090233, 0.21717852, 0.14232376, 0.15864831, 0.14731665,\n",
       "        0.16363046], dtype=float32),\n",
       " array([0.19607137, 0.16641654, 0.24364342, 0.17053072, 0.22333796],\n",
       "       dtype=float32),\n",
       " array([0.16820061, 0.180751  , 0.15996556, 0.16039507, 0.16271584,\n",
       "        0.16797194], dtype=float32),\n",
       " array([0.15084153, 0.18476452, 0.14279765, 0.15002392, 0.21163912,\n",
       "        0.15993322], dtype=float32),\n",
       " array([0.14677705, 0.11412623, 0.1350473 , 0.12154289, 0.11733629,\n",
       "        0.11925667, 0.11128011, 0.13463356], dtype=float32),\n",
       " array([0.13145928, 0.15468346, 0.15472727, 0.13728751, 0.15855578,\n",
       "        0.12845607, 0.13483052], dtype=float32),\n",
       " array([0.24575032, 0.22051807, 0.2825798 , 0.2511518 ], dtype=float32),\n",
       " array([0.14254315, 0.14698589, 0.12652841, 0.16591944, 0.14218129,\n",
       "        0.14749032, 0.12835145], dtype=float32)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_merged.groupby('win_market_id',sort=False).apply(lambda x: pd_softmax(x['pred_prob'])).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df = pd.concat([pd.read_feather('server_data/'+x) for x in os.listdir('server_data/')])\n",
    "runs_df = runs_df.dropna(subset=['place'], how='all')    \n",
    "runs_df.drop_duplicates(inplace=True)\n",
    "runs_df['date'] = pd.to_datetime(runs_df['meetingDate']).dt.date\n",
    "runs_df['year-month'] = pd.to_datetime(runs_df['meetingDate']).dt.to_period('M')\n",
    "runs_df['year'] = pd.to_datetime(runs_df['meetingDate']).dt.to_period('Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrackDict = {\n",
    "    'Auckland (NZ)':'Manukau',\n",
    "    'Christchurch (NZ)':'Addington',\n",
    "    'Dport @ HOB':'Hobart',\n",
    "    'Dport @ LCN':'Launceston',\n",
    "    'Meadows (MEP)':'The Meadows',\n",
    "    'Otago (NZ)':'Forbury Park',\n",
    "    'Palmerston Nth (NZ)':'Manawatu',\n",
    "    'Sandown (SAP)':'Sandown Park',\n",
    "    'Southland (NZ)':'Ascot Park',\n",
    "    'Tokoroa (NZ)':'Tokoroa',\n",
    "    'Waikato (NZ)':'Cambridge',\n",
    "    'Wanganui (NZ)':'Hatrick',\n",
    "    'Taranaki (NZ)':'Taranaki',\n",
    "    'Ashburton (NZ)':'Ashburton',\n",
    "    'Richmond (RIS)':'Richmond Straight',\n",
    "    'Murray Bridge (MBR)':'Murray Bridge',\n",
    "    'Murray Bridge (MBS)':'Murray Bridge Straight'\n",
    "}\n",
    "TopazData = runs_df.copy()\n",
    "TopazData['track'] = TopazData['track'].replace(TrackDict)\n",
    "TopazData['meetingDate'] = pd.to_datetime(TopazData['meetingDate'])\n",
    "TopazData['dateWhelped'] = pd.to_datetime(TopazData['dateWhelped'])\n",
    "\n",
    "TopazData['dogName']=TopazData['dogName'].str.replace(\"'\",\"\")\n",
    "TopazData['sireName']=TopazData['sireName'].str.replace(\"'\",\"\")\n",
    "TopazData['damName']=TopazData['damName'].str.replace(\"'\",\"\")\n",
    "state_map = {x['trackName']:x['State'] for x in mapping.trackCodes}\n",
    "TopazData['state'] = TopazData['track'].map(state_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopazData = TopazData.query('state == \"VIC\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopazData.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2024,4,27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_races = topaz_api.get_races(start_date)\n",
    "new_races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_races.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_race_ids = new_races['raceId'].unique()\n",
    "new_race_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_run = topaz_api.get_race_runs(race_id = 1029602600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_run['meetingDate'] = pd.to_datetime(race_run['meetingDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopazData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w_form = pd.concat([TopazData,race_run])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopazData = Topazbackup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topazbackup = TopazData.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopazData = new_w_form.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopazData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TopazData['last5'] = TopazData['last5'].astype(str)\n",
    "scaler = MinMaxScaler()\n",
    "TopazData['track'] = TopazData['track'].replace(TrackDict)\n",
    "TopazData['meetingDate'] = pd.to_datetime(TopazData['meetingDate'])\n",
    "TopazData['dateWhelped'] = pd.to_datetime(TopazData['dateWhelped'])\n",
    "\n",
    "TopazData['dogName']=TopazData['dogName'].str.replace(\"'\",\"\")\n",
    "TopazData['sireName']=TopazData['sireName'].str.replace(\"'\",\"\")\n",
    "TopazData['damName']=TopazData['damName'].str.replace(\"'\",\"\")\n",
    "# # Convert the 'pir' column to string\n",
    "try:\n",
    "    TopazData['pir'] = TopazData['pir'].fillna(0)\n",
    "    TopazData['pir'] = TopazData['pir'].astype(int).astype(str)\n",
    "except ValueError:\n",
    "    print('Error converting pir to string')\n",
    "    TopazData['pir'] = '000'\n",
    "\n",
    "# # Extract the second last letter and create a new column '2ndLastPIR'\n",
    "TopazData['2ndLastPIR'] = TopazData['pir'].apply(lambda x: x[-2] if len(x) >= 2 else None)\n",
    "TopazData['2ndLastPIR'] = TopazData['2ndLastPIR'].fillna(TopazData['place']).fillna(0)\n",
    "TopazData['2ndLastPIR'] = TopazData['2ndLastPIR'].astype(int)\n",
    "\n",
    "# # Create a feature that calculates places gained/conceded in the home straight\n",
    "TopazData['finishingPlaceMovement'] = TopazData['2ndLastPIR'] - TopazData['place']\n",
    "\n",
    "TopazData['weightInKgScaled'] = TopazData.groupby('raceId')['weightInKg'].transform(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)).flatten() if x.nunique() > 1 else 0)\n",
    "# TopazData['weightInKgScaled'] = TopazData.groupby('raceId')['weightInKg'].transform(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)).flatten() if x.nunique() > 1 else 0)\n",
    "#Scale values as required\n",
    "TopazData['prizemoneyLog'] = np.log10(TopazData['prizeMoney'] + 1)\n",
    "TopazData['placeLog'] = np.log10(TopazData['place'] + 1)\n",
    "TopazData['marginLog'] = np.log10(TopazData['resultMargin'] + 1)\n",
    "\n",
    "# Calculate median winner time per track/distance\n",
    "win_results = TopazData[TopazData['place'] == 1]\n",
    "\n",
    "grouped_data = win_results.groupby(['track', 'distance', 'meetingDate'])['resultTime'].median().reset_index()\n",
    "\n",
    "median_win_time = pd.DataFrame(grouped_data.groupby(['track', 'distance']).apply(lambda x: x.sort_values('meetingDate').set_index('meetingDate')['resultTime'].shift(1).rolling('365D', min_periods=1).median())).reset_index()\n",
    "median_win_time.rename(columns={\"resultTime\": \"runTimeMedian\"},inplace=True)\n",
    "\n",
    "median_win_time['speedIndex'] = (median_win_time['runTimeMedian'] / median_win_time['distance'])\n",
    "median_win_time['speedIndex'] = MinMaxScaler().fit_transform(median_win_time[['speedIndex']])\n",
    "\n",
    "TopazData = TopazData.merge(median_win_time, how='left', on=['track', 'distance','meetingDate'])\n",
    "\n",
    "TopazData['runTimeNorm'] = (TopazData['runTimeMedian'] / TopazData['resultTime']).clip(0.8, 1.2)\n",
    "TopazData['runTimeNorm'] = MinMaxScaler().fit_transform(TopazData[['runTimeNorm']])\n",
    "\n",
    "#Same for split time\n",
    "split_win_results = TopazData[TopazData['position_1'] == 1]\n",
    "grouped_data = split_win_results.groupby(['track', 'distance', 'meetingDate'])['time_1'].median().reset_index()\n",
    "\n",
    "median_split_win_time = pd.DataFrame(grouped_data.groupby(['track', 'distance']).apply(lambda x: x.sort_values('meetingDate').set_index('meetingDate')['time_1'].shift(1).rolling('365D', min_periods=1).median())).reset_index()\n",
    "median_split_win_time.rename(columns={'time_1': 'split_time_median'},inplace=True)\n",
    "\n",
    "median_split_win_time['split_speedIndex'] = median_split_win_time['split_time_median']\n",
    "median_split_win_time['split_speedIndex'] = MinMaxScaler().fit_transform(median_split_win_time[['split_time_median']])\n",
    "\n",
    "# Merge with median winner time\n",
    "TopazData = TopazData.merge(median_split_win_time, how='left', on=['track', 'distance','meetingDate'])\n",
    "\n",
    "# Normalise time comparison\n",
    "TopazData['split_runTimeNorm'] = (TopazData['split_time_median'] / TopazData['time_1']).clip(0.5, 1.5)\n",
    "TopazData['split_runTimeNorm'] = MinMaxScaler().fit_transform(TopazData[['split_runTimeNorm']])\n",
    "\n",
    "min_run_time = TopazData.groupby('raceId')[['time_1']].min().reset_index().rename(columns={'time_1':'min_run_time'})\n",
    "TopazData = TopazData.merge(min_run_time, on='raceId')\n",
    "TopazData['split_time_margin'] = TopazData['time_1']-TopazData['min_run_time']\n",
    "# Same for runhome time\n",
    "TopazData['run_home_time'] = TopazData['resultTime'] - TopazData['time_1']\n",
    "win_results = TopazData[TopazData['place'] == 1]\n",
    "\n",
    "\n",
    "grouped_data = win_results.groupby(['track', 'distance', 'meetingDate'])['run_home_time'].median().reset_index()\n",
    "\n",
    "median_win_time = pd.DataFrame(grouped_data.groupby(['track', 'distance']).apply(lambda x: x.sort_values('meetingDate').set_index('meetingDate')['run_home_time'].shift(1).rolling('365D', min_periods=1).median())).reset_index()\n",
    "median_win_time.rename(columns={'run_home_time': \"run_home_TimeMedian\"},inplace=True)\n",
    "\n",
    "median_win_time['run_home_speedIndex'] = (median_win_time['run_home_TimeMedian'] / median_win_time['distance'])\n",
    "median_win_time['run_home_speedIndex'] = MinMaxScaler().fit_transform(median_win_time[['run_home_speedIndex']])\n",
    "\n",
    "TopazData = TopazData.merge(median_win_time, how='left', on=['track', 'distance','meetingDate'])\n",
    "\n",
    "TopazData['run_home_TimeNorm'] = (TopazData['run_home_TimeMedian'] / TopazData['run_home_time']).clip(0.8, 1.2)\n",
    "TopazData['run_home_TimeNorm'] = MinMaxScaler().fit_transform(TopazData[['runTimeNorm']])\n",
    "\n",
    "# Sort the DataFrame by 'RaceId' and 'Box'\n",
    "TopazData = TopazData.sort_values(by=['raceId', 'boxNumber'])\n",
    "\n",
    "# Check if there is an entry equal to boxNumber + 1\n",
    "TopazData['hasEntryBoxNumberPlus1'] = (TopazData.groupby('raceId')['boxNumber'].shift(1) == TopazData['boxNumber'] + 1) | (TopazData['boxNumber'] == 8)\n",
    "TopazData['hasEntryBoxNumberMinus1'] = (TopazData.groupby('raceId')['boxNumber'].shift(-1) == TopazData['boxNumber'] - 1)\n",
    "# Convert boolean values to 1\n",
    "TopazData['hasEntryBoxNumberPlus1'] = TopazData['hasEntryBoxNumberPlus1'].astype(int)\n",
    "TopazData['hasEntryBoxNumberMinus1'] = TopazData['hasEntryBoxNumberMinus1'].astype(int)\n",
    "# Display the resulting DataFrame which shows adjacent Vacant Boxes\n",
    "# Box 1 is treated as having a vacant box to the left always as we are looking how much space the dog has to move.\n",
    "TopazData['adjacentVacantBoxes'] = 2 - TopazData['hasEntryBoxNumberPlus1'] - TopazData['hasEntryBoxNumberMinus1']\n",
    "# Calculate 'hasAtLeast1VacantBox'\n",
    "TopazData['hasAtLeast1VacantBox'] = (TopazData['adjacentVacantBoxes'] > 0).astype(int)\n",
    "\n",
    "TopazData['win'] = TopazData['place'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "grouped_data = TopazData.groupby(['track', 'distance', 'boxNumber', 'hasAtLeast1VacantBox', 'meetingDate'])['win'].mean().reset_index()\n",
    "grouped_data.set_index('meetingDate', inplace=True)\n",
    "\n",
    "# Apply rolling mean calculation to the aggregated data\n",
    "box_win_percent = grouped_data.groupby(['track', 'distance', 'boxNumber', 'hasAtLeast1VacantBox']).apply(lambda x: x.sort_values('meetingDate')['win'].shift(1).rolling('365D', min_periods=1).mean()).reset_index()\n",
    "\n",
    "# Reset index and rename columns\n",
    "box_win_percent.columns = ['track', 'distance', 'boxNumber', 'hasAtLeast1VacantBox', 'meetingDate', 'rolling_box_win_percentage']\n",
    "\n",
    "# Add to dog results dataframe\n",
    "TopazData = TopazData.merge(box_win_percent, on=['track', 'distance', 'meetingDate','boxNumber','hasAtLeast1VacantBox'], how='left')\n",
    "\n",
    "# resultMargin has the same value for 1st and 2nd placed dogs, but should be 0 for the 1st placed dog.\n",
    "TopazData.loc[TopazData['place'] == 1, ['resultMargin']] = 0\n",
    "\n",
    "TopazData['dogAge'] = (TopazData['meetingDate'] - TopazData['dateWhelped']).dt.days\n",
    "scaler = MinMaxScaler()\n",
    "TopazData['dogAgeScaled'] = TopazData.groupby('raceId')['dogAge'].transform(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)).flatten())\n",
    "TopazData['averageSpeed'] = TopazData['distance'] / TopazData['resultTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopazData['split_time_margin'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,c in enumerate(TopazData.columns):\n",
    "    print(i,c,TopazData[c].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopazData.speedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopazData['averageSpeed'] = TopazData['distance'] / TopazData['resultTime']\n",
    "TopazData['splitMargin_1'] = np.where(TopazData['position_1'] == 1, 0, TopazData['splitMargin_1'])\n",
    "TopazData['margin_from_lengths'] = pd.to_numeric(TopazData['resultMarginLengths'].str.replace('L',''))\n",
    "# TopazData['win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopazData['resultMargin'] = TopazData['margin_from_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopazData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopazData.margin_from_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopazData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_last(x):\n",
    "    return x.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "dataset = TopazData.copy()\n",
    "print(f\"dataset shape: {dataset.shape}\")\n",
    "dataset['meetingDate'] = pd.to_datetime(dataset['meetingDate'])\n",
    "\n",
    "# Calculate values for dog, trainer, dam and sire\n",
    "subsets = ['dog', 'trainer', 'dam', 'sire']\n",
    "# subsets = ['dog']\n",
    "\n",
    "# Use rolling window of 28, 91 and 365 days\n",
    "# rolling_windows = ['28D','91D', '365D']\n",
    "rolling_windows = [1,'28D','91D', '365D']\n",
    "rolling_windows = [1,'365D']\n",
    "# rolling_windows = [1]\n",
    "\n",
    "# Features to use for rolling windows calculation\n",
    "features = ['distance','boxNumber','runTimeNorm', 'placeLog', 'prizemoneyLog', \n",
    "            'marginLog','finishingPlaceMovement','splitMargin_1','split_runTimeNorm','run_home_TimeNorm','finishingPlaceMovement',\n",
    "            'time_1','averageSpeed']\n",
    "\n",
    "features = ['distance','boxNumber','runTimeNorm', 'place', 'resultMargin',\n",
    "            'split_time_margin','split_runTimeNorm','time_1','run_home_TimeNorm','finishingPlaceMovement',\n",
    "            'averageSpeed', 'win']\n",
    "\n",
    "\n",
    "dam_features = ['distance','boxNumber','runTimeNorm', 'place', 'resultMargin','split_time_margin','split_runTimeNorm',\n",
    "            'time_1','averageSpeed', 'win']\n",
    "\n",
    "\n",
    "# Aggregation functions to apply\n",
    "\n",
    "aggregates = ['mean']\n",
    "\n",
    "# Keep track of generated feature names\n",
    "feature_cols = []\n",
    "\n",
    "for i in subsets:\n",
    "    # Generate rolling window features\n",
    "    idnumber = i + 'Id'\n",
    "\n",
    "    subset_dataframe = dataset[['meetingDate',idnumber] + features]\n",
    "    average_df = pd.DataFrame()\n",
    "\n",
    "    for feature in features:\n",
    "        # Group by 'damId' and 'meetingDate' and calculate the average of the current feature\n",
    "        feature_average_df = subset_dataframe.groupby([idnumber, 'meetingDate'])[feature].mean().reset_index()\n",
    "        # Rename the feature column to indicate it's the average of that feature\n",
    "        feature_average_df.rename(columns={feature: f'{feature}{i}DayAverage'}, inplace=True)\n",
    "\n",
    "        # If average_df is empty, assign the feature_average_df to it\n",
    "        if average_df.empty:\n",
    "            average_df = feature_average_df\n",
    "        else:\n",
    "            # Otherwise, merge feature_average_df with average_df\n",
    "            average_df = pd.merge(average_df, feature_average_df, on=[idnumber, 'meetingDate'],how='left')\n",
    "\n",
    "        # Assuming df is your DataFrame\n",
    "    column_names = average_df.columns.tolist()\n",
    "    # Columns to exclude\n",
    "    columns_to_exclude = [idnumber,'meetingDate']\n",
    "    # Exclude specified columns from the list\n",
    "    column_names_filtered = [col for col in column_names if col not in columns_to_exclude]\n",
    "\n",
    "    average_df.drop_duplicates(inplace=True)\n",
    "    average_df['meetingDate'] = pd.to_datetime(average_df['meetingDate'])\n",
    "    average_df = average_df.set_index([idnumber, 'meetingDate']).sort_index() \n",
    "\n",
    "\n",
    "    #Process Dog Stats\n",
    "    for rolling_window in rolling_windows:\n",
    "        print(f\"dataset shape: {dataset.shape}\")\n",
    "        print(f'Processing {i} rolling window {rolling_window} days')\n",
    "\n",
    "        rolling_result = (\n",
    "            average_df\n",
    "            .reset_index(level=0)\n",
    "            .groupby(idnumber)[column_names_filtered]\n",
    "            .rolling(rolling_window)  # Use timedelta for rolling window\n",
    "            .agg(aggregates)\n",
    "            .groupby(level=0)\n",
    "            .shift(1)\n",
    "        )\n",
    "\n",
    "        # Generate list of rolling window feature names (eg: RunTime_norm_min_365D)\n",
    "        agg_features_cols = [f'{i}_{f}_{a}_{rolling_window}' for f, a in itertools.product(features, aggregates)]\n",
    "        # Add features to dataset\n",
    "        average_df[agg_features_cols] = rolling_result\n",
    "        # Keep track of generated feature names\n",
    "        feature_cols.extend(agg_features_cols)\n",
    "        average_df.fillna(0, inplace=True)\n",
    "\n",
    "    \n",
    "\n",
    "    average_df.reset_index(inplace=True)\n",
    "    dataset = pd.merge(dataset,average_df,on=[idnumber, 'meetingDate'],how='left')\n",
    "\n",
    "# Only keep data 12 months after the start date of your dataset since we've used a 365D rolling timeframe for some features\n",
    "# feature_cols = np.unique(feature_cols).tolist()\n",
    "# dataset = dataset[dataset['meetingDate'] >= '2021-01-01']\n",
    "\n",
    "dataset = dataset[[\n",
    "                'meetingDate',\n",
    "                'state',\n",
    "                'track',\n",
    "                'distance',\n",
    "                'raceId',\n",
    "                'raceTypeCode',\n",
    "                'raceNumber',\n",
    "                'boxNumber',\n",
    "                'rugNumber',\n",
    "                'runId',\n",
    "                'dogId',\n",
    "                'dogName',\n",
    "                'weightInKg',\n",
    "                'sex',\n",
    "                'trainerId',\n",
    "                'trainerState',\n",
    "                'damId',\n",
    "                'damName',\n",
    "                'sireId',\n",
    "                'sireName',\n",
    "                'win',\n",
    "                'place',\n",
    "                'resultTime',\n",
    "                'resultMargin',\n",
    "                'resultMarginLengths',\n",
    "                'dogAgeScaled',\n",
    "                'startPrice',\n",
    "                # 'lastFiveWinPercentage',\n",
    "                # 'lastFivePlacePercentage',\n",
    "                'weightInKgScaled',\n",
    "                'rolling_box_win_percentage',\n",
    "                'hasEntryBoxNumberPlus1', \n",
    "                'hasEntryBoxNumberMinus1',]\n",
    "                 + feature_cols\n",
    "                ]\n",
    "\n",
    "feature_cols.extend(['dogAgeScaled',\n",
    "                     'boxNumber',\n",
    "                # 'lastFiveWinPercentage',\n",
    "                # 'lastFivePlacePercentage',\n",
    "                'weightInKgScaled',\n",
    "                'hasEntryBoxNumberPlus1', 'hasEntryBoxNumberMinus1',\n",
    "                'rolling_box_win_percentage'])\n",
    "\n",
    "#The below line will output your dataframe to a csv but may be too large to open in Excel.\n",
    "#dataset.to_csv('testing.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['dogAgeScaled',\n",
    "                     'boxNumber',\n",
    "                # 'lastFiveWinPercentage',\n",
    "                # 'lastFivePlacePercentage',\n",
    "                'weightInKgScaled',\n",
    "                'hasEntryBoxNumberPlus1', 'hasEntryBoxNumberMinus1',\n",
    "                'rolling_box_win_percentage'] + [x for x in feature_cols if (('dog' in x) and ('_1' in x)) or (('dog' not in x) and ('_365' in x))]\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['dogAgeScaled',\n",
    " 'boxNumber',\n",
    " 'weightInKgScaled',\n",
    " 'hasEntryBoxNumberPlus1',\n",
    " 'hasEntryBoxNumberMinus1',\n",
    " 'rolling_box_win_percentage',\n",
    " 'dog_distance_mean_1',\n",
    " 'dog_boxNumber_mean_1',\n",
    " 'dog_runTimeNorm_mean_1',\n",
    " 'dog_place_mean_1',\n",
    " 'dog_resultMargin_mean_1',\n",
    " 'dog_split_time_margin_mean_1',\n",
    " 'dog_split_runTimeNorm_mean_1',\n",
    " 'dog_time_1_mean_1',\n",
    " 'dog_run_home_TimeNorm_mean_1',\n",
    " 'dog_finishingPlaceMovement_mean_1',\n",
    " 'dog_averageSpeed_mean_1',\n",
    " 'dog_win_mean_1',\n",
    " 'trainer_distance_mean_365D',\n",
    " 'trainer_boxNumber_mean_365D',\n",
    " 'trainer_runTimeNorm_mean_365D',\n",
    " 'trainer_place_mean_365D',\n",
    " 'trainer_resultMargin_mean_365D',\n",
    " 'trainer_split_time_margin_mean_365D',\n",
    " 'trainer_split_runTimeNorm_mean_365D',\n",
    " 'trainer_time_1_mean_365D',\n",
    " 'trainer_run_home_TimeNorm_mean_365D',\n",
    " 'trainer_finishingPlaceMovement_mean_365D',\n",
    " 'trainer_averageSpeed_mean_365D',\n",
    " 'trainer_win_mean_365D',\n",
    " 'dam_distance_mean_365D',\n",
    " 'dam_boxNumber_mean_365D',\n",
    " 'dam_runTimeNorm_mean_365D',\n",
    " 'dam_place_mean_365D',\n",
    " 'dam_resultMargin_mean_365D',\n",
    " 'dam_split_time_margin_mean_365D',\n",
    " 'dam_split_runTimeNorm_mean_365D',\n",
    " 'dam_time_1_mean_365D',\n",
    " 'dam_run_home_TimeNorm_mean_365D',\n",
    " 'dam_finishingPlaceMovement_mean_365D',\n",
    " 'dam_averageSpeed_mean_365D',\n",
    " 'dam_win_mean_365D',\n",
    " 'sire_distance_mean_365D',\n",
    " 'sire_boxNumber_mean_365D',\n",
    " 'sire_runTimeNorm_mean_365D',\n",
    " 'sire_place_mean_365D',\n",
    " 'sire_resultMargin_mean_365D',\n",
    " 'sire_split_time_margin_mean_365D',\n",
    " 'sire_split_runTimeNorm_mean_365D',\n",
    " 'sire_time_1_mean_365D',\n",
    " 'sire_run_home_TimeNorm_mean_365D',\n",
    " 'sire_finishingPlaceMovement_mean_365D',\n",
    " 'sire_averageSpeed_mean_365D',\n",
    " 'sire_win_mean_365D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pred = dataset.query('raceId == 1029602600')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dog_split_time_margin_mean_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dog = dataset.query('dogId == 713187529')\n",
    "single_dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume df is your DataFrame and 'date' is your date column\n",
    "df = TopazData.copy()\n",
    "df['date'] = pd.to_datetime(df['date']).dt.to_period('M')\n",
    "\n",
    "# List of relevant columns\n",
    "columns = ['distance','boxNumber','runTimeNorm', 'place', 'resultMargin','splitMargin_1','split_runTimeNorm',\n",
    "            'time_1','averageSpeed', 'win']  # replace with your actual column names\n",
    "\n",
    "# Create a separate plot for each column\n",
    "for col in columns:\n",
    "    # Group by 'date' and calculate the percentage of missing values in each month for the column\n",
    "    missing_data = df.groupby('date')[col].apply(lambda x: x.isnull().mean() * 100)\n",
    "\n",
    "    # Plot the results\n",
    "    missing_data.plot(kind='bar', figsize=(12, 6))\n",
    "    plt.title(f'Percentage of missing values per month for {col}')\n",
    "    plt.ylabel('Percentage of missing values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_feather('topaz_data_dog.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dogName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betfair_df = pd.read_feather('../data_tools/DATA/df-betfairSP.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betfair_df['date'] =  (pd.to_datetime(betfair_df['EVENT_DT'],dayfirst=True) + pd.Timedelta(hours=7)).dt.date\n",
    "betfair_df['dogName'] = betfair_df.dog.str.strip().str.upper().str.replace('.','').replace(\"'\",'')\n",
    "# betfair_df.to_csv('betfair_bsp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['date'] = pd.to_datetime(dataset['meetingDate']).dt.date\n",
    "dataset['dogName'] = dataset['dogName'].str.upper().str.replace('.','').str.replace(\"'\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bsp = dataset.merge(betfair_df,on=['date','dogName'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prev_race(df_in, df_g, rolling_window=10, factor=''):\n",
    "    df = df_in.copy()\n",
    "    original_cols = df_in.columns\n",
    "    df[f'prev_race'] = df_g['raceId'].shift(1).fillna('-1').astype('string')\n",
    "    df[f'prev_race_date'] = df_g['date'].shift(1).fillna('-1').astype('string')\n",
    "    df[f'prev_race_track'] = df_g['track'].shift(1).fillna('-1').astype('string')\n",
    "    df[f'prev_race_state'] = df_g['state'].shift(1).fillna('-1').astype('string')\n",
    "    df[f'next_race'] = df_g['raceId'].shift(-1).fillna('-1').astype('string')\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bsp = generate_prev_race(dataset_bsp,dataset_bsp.groupby('dogId'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['dog_distance_min_1',\n",
    " 'dog_boxNumber_min_1',\n",
    " 'dog_runTimeNorm_min_1',\n",
    " 'dog_place_min_1',\n",
    " 'dog_resultMargin_min_1',\n",
    " 'dog_split_time_margin_min_1',\n",
    " 'dog_split_runTimeNorm_min_1',\n",
    " 'dog_time_1_min_1',\n",
    " 'dog_averageSpeed_min_1',\n",
    " 'dog_win_min_1',\n",
    " 'dogAgeScaled',\n",
    " 'boxNumber',\n",
    " 'weightInKgScaled',\n",
    " 'hasEntryBoxNumberPlus1',\n",
    " 'hasEntryBoxNumberMinus1',\n",
    " 'rolling_box_win_percentage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols[0:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_values =  pd.Series( dataset_bsp[feature_cols].fillna(-1.0).astype('float32').values.tolist())\n",
    "dataset_bsp['stats_topaz'] = stat_values\n",
    "dataset_bsp['dogid'] = dataset_bsp['dogId'].astype('str')\n",
    "dataset_bsp['raceid'] = dataset_bsp['raceId'].astype('str')\n",
    "dataset_bsp['stats_cols'] = str(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_bsp.to_feather('topaz_data_w_bsp.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_bsp = pd.read_feather('topaz_data_w_bsp.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_trackname(trackname, hash_size=1024):\n",
    "    hashes = [hash(x) % hash_size for x in trackname]\n",
    "    return hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = {x: hash(x)%1024 for x in dataset_bsp.track.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bsp['track_hash'] = dataset_bsp['track'].map(hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bsp.to_feather('topaz_data_w_bsp.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bsp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.query('raceId == 618363326').place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_bsp['stats_topaz'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset_bsp.copy()\n",
    "col = 'BSP'\n",
    "df = df[df['state']!='NZ']\n",
    "df['date'] = pd.to_datetime(df['date']).dt.to_period('Y')\n",
    "missing_data = df.groupby(['date','state'])[col].apply(lambda x: x.isnull().mean() * 100)\n",
    "\n",
    "# Plot the results\n",
    "missing_data.plot(kind='bar', figsize=(40, 6))\n",
    "plt.title(f'Percentage of missing values per month for {col}')\n",
    "plt.ylabel('Percentage of missing values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_feather('../data/topaz_data_w_bsp.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,c in enumerate(dataset.columns):\n",
    "    print(i,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dog_split_runTimeNorm_min_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('../data/topaz_data_w_bsp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dataset.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.raceId.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing function to generate date range\n",
    "def generate_date_range(start_date, end_date):\n",
    "    start_date = start_date\n",
    "    end_date = end_date\n",
    "\n",
    "    date_list = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        date_list.append(current_date.strftime(\"%Y-%m-%d\"))\n",
    "        current_date += timedelta(days=31)\n",
    "\n",
    "    return date_list\n",
    "\n",
    "# Example usage:\n",
    "start_date = datetime(2024,1,1)\n",
    "end_date = (datetime.today() + timedelta(days=31))\n",
    "\n",
    "# Generate the date range\n",
    "date_range = generate_date_range(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def topaz_races_threaded(buckets, topaz_api, progress):\n",
    "    all_races = []\n",
    "    # print(f\"{buckets=}\")\n",
    "    errors = []\n",
    "    for bucket in buckets:\n",
    "        start_date, end_date, state = bucket\n",
    "        # print(bucket)\n",
    "        try:\n",
    "            races = topaz_api.get_races(from_date=start_date, to_date=end_date, owning_authority_code=state)\n",
    "            races['state'] = state\n",
    "            all_races.append(races)\n",
    "        except requests.HTTPError as http_err:\n",
    "            print(f'HTTP error occurred: {http_err}')\n",
    "            errors.append(bucket)\n",
    "            pass\n",
    "        # time.sleep(2)\n",
    "        progress.update()\n",
    "    return all_races,errors\n",
    "\n",
    "def get_topaz_races(start_date, end_date, states, topaz_api:TopazAPI):\n",
    "    date_range = generate_date_range(start_date, end_date)\n",
    "    starts = date_range[:-1]\n",
    "    ends = date_range[1:]\n",
    "    date_range_states = [(start, end, state) for start, end in zip(starts, ends) for state in states]\n",
    "\n",
    "    print(f\"Created {len(date_range_states)} date ranges for {len(states)} states\")\n",
    "\n",
    "    num_workers = min(6, len(date_range_states))  # Adjust this value based on your system's capabilities\n",
    "    chunk_size = math.ceil(len(date_range_states) / num_workers)\n",
    "\n",
    "    chunks = [date_range_states[i:i + chunk_size] for i in range(0, len(date_range_states), chunk_size)]\n",
    "    \n",
    "    print(chunks)\n",
    "    print(len(chunks))\n",
    "    _process_jobs = []\n",
    "    bars = []\n",
    "    results = []\n",
    "    errors = []\n",
    "    for i in range(num_workers):\n",
    "        bars.append(tqdm(total=len(chunks[i]), position=i)) \n",
    "        # time.sleep(2)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "\n",
    "\n",
    "        for i,chunk in enumerate(chunks):\n",
    "            _process_jobs.append(executor.submit(topaz_races_threaded, chunk, topaz_api, bars[i]))\n",
    "\n",
    "        # results = []\n",
    "        for job in concurrent.futures.as_completed(_process_jobs):\n",
    "            result,error = job.result()\n",
    "            errors.extend(error)\n",
    "            results.append(result)\n",
    "\n",
    "    \n",
    "\n",
    "    # results = []\n",
    "    print(errors)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2015,1,1)\n",
    "end_date = (datetime.today() + timedelta(days=31))\n",
    "states = states = ['NSW', 'VIC', 'NZ', 'QLD', 'SA', 'WA', 'TAS', 'NT', 'ACT']\n",
    "states = ['NZ']\n",
    "states = ['SA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_topaz_races(start_date, end_date, states, topaz_api)\n",
    "output_flat = [item for sublist in output for item in sublist]\n",
    "all_races_df = pd.concat(output_flat,ignore_index=True).reset_index(drop=True)\n",
    "all_races_df.to_csv('all_races_topas_SA.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Assume you have 1000 unique track names and you want to create an embedding of size 50 for each track name\n",
    "num_tracknames = 1000\n",
    "embedding_dim = 50\n",
    "\n",
    "# Create an embedding layer\n",
    "embedding = nn.Embedding(num_tracknames, embedding_dim)\n",
    "\n",
    "# Assume trackname_indices is a tensor of integers, where each integer is the index of a track name in the dictionary\n",
    "# For example, you can create it by replacing each track name in your data with its index in the dictionary of track names\n",
    "trackname_indices = torch.tensor([0, 1, 2, 3, 4])  # replace with your actual data\n",
    "\n",
    "# Get the embeddings of the track names\n",
    "trackname_embeddings = embedding(trackname_indices)\n",
    "\n",
    "print(trackname_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_races_df['date'] = pd.to_datetime(all_races_df['raceStart']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting_ids = list(all_races_df['meetingId'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topaz_meeting_runs_threaded(chunk,topaz_api:TopazAPI,progress):\n",
    "    race_runs = []\n",
    "    race_results = []\n",
    "    errors = []\n",
    "    for race_id in chunk:\n",
    "        try:\n",
    "            # race_run = topaz_api.get_race_runs(race_id=race_id)\n",
    "            # race_runs.append(race_run)\n",
    "            time.sleep(0.3)\n",
    "            race_result_json = topaz_api.get_race_result(race_id = race_id)\n",
    "            try:\n",
    "                race_run.to_feather(f\"race_runs/{race_id}_run.fth\")\n",
    "                race_result_df = pd.DataFrame.from_dict([race_result_json])\n",
    "                race_result_df.to_feather(f\"results/{race_id}_results.fth\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        except requests.HTTPError as http_err:\n",
    "            print(f'HTTP error occurred: {http_err}')\n",
    "            if http_err.response.status_code == 429:\n",
    "\n",
    "                time.sleep(120)\n",
    "            errors.append(race_id)\n",
    "            pass\n",
    "        progress.update()\n",
    "\n",
    "    return race_runs,race_results,errors\n",
    "\n",
    "def topaz_meeting_run_getter(race_id_list,topaz_api:TopazAPI):\n",
    "\n",
    "    print(f\"Fetching data for  {len(race_id_list)}\")\n",
    "\n",
    "    num_workers = 6\n",
    "    chunk_size = math.ceil(len(race_id_list) / num_workers)\n",
    "\n",
    "    chunks = [race_id_list[i:i + chunk_size] for i in range(0, len(race_id_list), chunk_size)]\n",
    "    \n",
    "    print(chunks)\n",
    "    print(len(chunks))\n",
    "    _process_jobs = []\n",
    "    bars = []\n",
    "    race_runs = []\n",
    "    results = []\n",
    "    errors = []\n",
    "    for i in range(num_workers):\n",
    "        bars.append(tqdm(total=len(chunks[i]), position=i)) \n",
    "        # time.sleep(2)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "\n",
    "\n",
    "        for i,chunk in enumerate(chunks):\n",
    "            _process_jobs.append(executor.submit(topaz_meeting_runs_threaded, chunk, topaz_api, bars[i]))\n",
    "\n",
    "        # results = []\n",
    "        for job in concurrent.futures.as_completed(_process_jobs):\n",
    "            race_run,result_json,error = job.result()\n",
    "            race_runs.extend(race_run)\n",
    "            errors.extend(error)\n",
    "            results.extend(result_json)\n",
    "\n",
    "    \n",
    "\n",
    "    # results = []\n",
    "    print(errors)\n",
    "    return race_runs,results,errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_ids = list(all_races_df['raceId'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_id = 837931333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_run = topaz_api.get_race_runs(race_id=race_id)\n",
    "race_result_json,response = topaz_api.get_race_result(race_id=race_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_lim_left = int(response.headers['ratelimit-remaining'])\n",
    "reset_time = int(response.headers['ratelimit-reset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_run_from_json = pd.DataFrame(race_result_json['runs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_run_from_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_times = pd.DataFrame(race_result_json['splitTimes'])\n",
    "split_times_1 = split_times[split_times['splitTimeMarker'] == 1][['runId','time','position','splitMargin']]\n",
    "split_time_2 = split_times[split_times['splitTimeMarker'] == 2][['runId','time','position','splitMargin']]\n",
    "split_times = split_times_1.merge(split_time_2, on='runId',suffixes=('_1','_2'),how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_run_from_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_result_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_result_json_meeting = topaz_api.get_meeting_details(meeting_id = 809592457)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topaz_race_runs_threaded(chunk,topaz_api:TopazAPI,progress):\n",
    "    race_runs = []\n",
    "    race_results = []\n",
    "    errors = []\n",
    "    for race_id in chunk:\n",
    "        try:\n",
    "            race_run = topaz_api.get_race_runs(race_id=race_id)\n",
    "            race_runs.append(race_run)\n",
    "            time.sleep(0.3)\n",
    "            race_result_json = topaz_api.get_race_result(race_id = race_id)\n",
    "            try:\n",
    "                race_run.to_feather(f\"race_runs/{race_id}_run.fth\")\n",
    "                race_result_df = pd.DataFrame.from_dict([race_result_json])\n",
    "                race_result_df.to_feather(f\"results/{race_id}_results.fth\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        except requests.HTTPError as http_err:\n",
    "            print(f'HTTP error occurred: {http_err}')\n",
    "            if http_err.response.status_code == 429:\n",
    "\n",
    "                time.sleep(120)\n",
    "            errors.append(race_id)\n",
    "            pass\n",
    "        progress.update()\n",
    "\n",
    "    return race_runs,race_results,errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topaz_race_run_getter(race_id_list,topaz_api:TopazAPI):\n",
    "\n",
    "    print(f\"Fetching data for  {len(race_id_list)}\")\n",
    "\n",
    "    num_workers = 6\n",
    "    chunk_size = math.ceil(len(race_id_list) / num_workers)\n",
    "\n",
    "    chunks = [race_id_list[i:i + chunk_size] for i in range(0, len(race_id_list), chunk_size)]\n",
    "    \n",
    "    print(chunks)\n",
    "    print(len(chunks))\n",
    "    _process_jobs = []\n",
    "    bars = []\n",
    "    race_runs = []\n",
    "    results = []\n",
    "    errors = []\n",
    "    for i in range(num_workers):\n",
    "        bars.append(tqdm(total=len(chunks[i]), position=i)) \n",
    "        # time.sleep(2)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "\n",
    "\n",
    "        for i,chunk in enumerate(chunks):\n",
    "            _process_jobs.append(executor.submit(topaz_race_runs_threaded, chunk, topaz_api, bars[i]))\n",
    "\n",
    "        # results = []\n",
    "        for job in concurrent.futures.as_completed(_process_jobs):\n",
    "            race_run,result_json,error = job.result()\n",
    "            race_runs.extend(race_run)\n",
    "            errors.extend(error)\n",
    "            results.extend(result_json)\n",
    "\n",
    "    \n",
    "\n",
    "    # results = []\n",
    "    print(errors)\n",
    "    return race_runs,results,errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_races_df = pd.read_csv('all_races_topas.csv', header=0)\n",
    "i = 0\n",
    "race_ids = list(all_races_df['raceId'].unique())\n",
    "# subset_ids = race_ids[i:min(len(race_ids),i+100)]\n",
    "# race_runs,results,errors = topaz_race_run_getter(subset_ids,topaz_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_races_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_race_run_df = pd.read_feather('race_runs/837931333_run.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_race_results_json_df = pd.read_feather('results/837931333_results.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_race_results_json_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(race_ids),100):\n",
    "    subset_ids = race_ids[i:min(len(race_ids),i+1000)]\n",
    "    race_runs,results,errors = topaz_race_run_getter(subset_ids,topaz_api)\n",
    "\n",
    "    results_df = pd.DataFrame.from_dict(results)\n",
    "    all_race_runs = pd.concat(race_runs,ignore_index=True).reset_index(drop=True)\n",
    "    all_race_runs.to_feather(f'race_runs/{i}_topaz_race_runs.fth')\n",
    "    results_df.to_feather(f\"results/{i}_topaz_results.fth\")\n",
    "    # with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_ids = list(all_races_df['raceId'].unique())\n",
    "code = \"NSW-VIC\"\n",
    "for race_id in tqdm(race_ids, desc=\"Processing races\", unit=\"race\"):\n",
    "    result_retries = 10\n",
    "\n",
    "    while result_retries > 0:\n",
    "        # Use tqdm to create a progress bar\n",
    "        # Get race run data\n",
    "        try:\n",
    "            race_run = topaz_api.get_race_runs(race_id=race_id)\n",
    "            race_result_json = topaz_api.get_race_result(race_id=race_id)\n",
    "            file_path = code + '_DATA.csv'\n",
    "            file_exists = os.path.isfile(file_path)\n",
    "            header_param = not file_exists\n",
    "\n",
    "            race_result = pd.DataFrame.from_dict([race_result_json])\n",
    "            split_times_df = pd.DataFrame(race_result['splitTimes'].tolist(),index=race_result.index)\n",
    "\n",
    "            splits_dict = split_times_df.T.stack().to_frame()\n",
    "            splits_dict.reset_index(drop=True, inplace= True)\n",
    "            splits_normalised = pd.json_normalize(splits_dict[0])\n",
    "\n",
    "            if len(splits_normalised) == 0:\n",
    "                race_run.to_csv(code + '_DATA.csv', mode='a', header=header_param, index=False)\n",
    "                break\n",
    "\n",
    "            first_split = splits_normalised[splits_normalised['splitTimeMarker'] == 1]\n",
    "            first_split = first_split[['runId','position','time']]\n",
    "            first_split = first_split.rename(columns={'position':'firstSplitPosition','time':'firstSplitTime'})\n",
    "            second_split = splits_normalised[splits_normalised['splitTimeMarker'] == 2]\n",
    "            second_split = second_split[['runId','position','time']]\n",
    "            second_split = second_split.rename(columns={'position':'secondSplitPosition','time':'secondSplitTime'})\n",
    "\n",
    "            split_times = splits_normalised[['runId']]\n",
    "            split_times = pd.merge(split_times,first_split,how='left',on=['runId'])\n",
    "            split_times = pd.merge(split_times,second_split,how='left',on=['runId'])\n",
    "\n",
    "            race_run = pd.merge(race_run,split_times,how='left',on=['runId'])\n",
    "            race_run.drop_duplicates(inplace=True)\n",
    "            race_run.to_csv(code + '_DATA.csv', mode='a', header=header_param, index=False)\n",
    "            break\n",
    "        except requests.HTTPError as http_err:\n",
    "            if http_err.response.status_code == 404:\n",
    "                file_path = code + '_DATA.csv'\n",
    "                file_exists = os.path.isfile(file_path)\n",
    "                header_param = not file_exists\n",
    "                race_run.to_csv(code + '_DATA.csv', mode='a', header=header_param, index=False)\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(race_id)\n",
    "            result_retries -= 1\n",
    "            if result_retries > 0:\n",
    "                time.sleep(15)\n",
    "            else:\n",
    "                time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over 7-day blocks\n",
    "for i in range(0, len(date_range), 10):\n",
    "    start_block_date = date_range[i]\n",
    "    print(start_block_date)\n",
    "    end_block_date = date_range[min(i + 9, len(date_range) - 1)]  # Ensure the end date is within the range\n",
    "\n",
    "    codes = ['NT','VIC','NSW','SA','WA','QLD','TAS','NZ']\n",
    "    codes = ['VIC', \"NSW\"]\n",
    "    all_races = []\n",
    "    for code in codes:\n",
    "        \n",
    "        print(code)\n",
    "        retries = 10  # Number of retries\n",
    "        while retries > 0:\n",
    "            try:\n",
    "                races = topaz_api.get_races(from_date=start_block_date, to_date=end_block_date, owning_authority_code=code)\n",
    "                all_races.append(races)\n",
    "                break  # Break out of the loop if successful\n",
    "            except requests.HTTPError as http_err:\n",
    "                if http_err.response.status_code == 429:\n",
    "                    retries -= 1\n",
    "                    if retries > 0:\n",
    "                        print(f\"Rate limited. Retrying in 121 seconds...\")\n",
    "                        time.sleep(121)\n",
    "                    else:\n",
    "                        print(\"Max retries reached. Moving to the next block.\")\n",
    "                else:\n",
    "                    print(f\"Error fetching races for {code}: {http_err.response.status_code}\")\n",
    "                    retries -= 1\n",
    "                    if retries > 0:\n",
    "                        print(f\"Retrying in 30 seconds...\")\n",
    "                        time.sleep(30)\n",
    "                    else:\n",
    "                        print(\"Max retries reached. Moving to the next block.\")\n",
    "\n",
    "    try:\n",
    "        all_races_df = pd.concat(all_races,ignore_index=True).reset_index(drop=True)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # Extract unique race IDs\n",
    "    race_ids = list(all_races_df['raceId'].unique())\n",
    "\n",
    "    for race_id in tqdm(race_ids, desc=\"Processing races\", unit=\"race\"):\n",
    "        result_retries = 10\n",
    "\n",
    "        while result_retries > 0:\n",
    "            # Use tqdm to create a progress bar\n",
    "            # Get race run data\n",
    "            try:\n",
    "                race_run = topaz_api.get_race_runs(race_id=race_id)\n",
    "                race_result_json = topaz_api.get_race_result(race_id=race_id)\n",
    "                file_path = code + '_DATA.csv'\n",
    "                file_exists = os.path.isfile(file_path)\n",
    "                header_param = not file_exists\n",
    "\n",
    "                race_result = pd.DataFrame.from_dict([race_result_json])\n",
    "                split_times_df = pd.DataFrame(race_result['splitTimes'].tolist(),index=race_result.index)\n",
    "\n",
    "                splits_dict = split_times_df.T.stack().to_frame()\n",
    "                splits_dict.reset_index(drop=True, inplace= True)\n",
    "                splits_normalised = pd.json_normalize(splits_dict[0])\n",
    "\n",
    "                if len(splits_normalised) == 0:\n",
    "                    race_run.to_csv(code + '_DATA.csv', mode='a', header=header_param, index=False)\n",
    "                    break\n",
    "\n",
    "                first_split = splits_normalised[splits_normalised['splitTimeMarker'] == 1]\n",
    "                first_split = first_split[['runId','position','time']]\n",
    "                first_split = first_split.rename(columns={'position':'firstSplitPosition','time':'firstSplitTime'})\n",
    "                second_split = splits_normalised[splits_normalised['splitTimeMarker'] == 2]\n",
    "                second_split = second_split[['runId','position','time']]\n",
    "                second_split = second_split.rename(columns={'position':'secondSplitPosition','time':'secondSplitTime'})\n",
    "\n",
    "                split_times = splits_normalised[['runId']]\n",
    "                split_times = pd.merge(split_times,first_split,how='left',on=['runId'])\n",
    "                split_times = pd.merge(split_times,second_split,how='left',on=['runId'])\n",
    "\n",
    "                race_run = pd.merge(race_run,split_times,how='left',on=['runId'])\n",
    "                race_run.drop_duplicates(inplace=True)\n",
    "                race_run.to_csv(code + '_DATA.csv', mode='a', header=header_param, index=False)\n",
    "                break\n",
    "            except requests.HTTPError as http_err:\n",
    "                if http_err.response.status_code == 404:\n",
    "                    file_path = code + '_DATA.csv'\n",
    "                    file_exists = os.path.isfile(file_path)\n",
    "                    header_param = not file_exists\n",
    "                    race_run.to_csv(code + '_DATA.csv', mode='a', header=header_param, index=False)\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(race_id)\n",
    "                result_retries -= 1\n",
    "                if result_retries > 0:\n",
    "                    time.sleep(15)\n",
    "                else:\n",
    "                    time.sleep(120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
